{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ClimData Tutorial\n",
    "This notebook demonstrates usage of the `ClimData` class for climate data extraction, extreme index computation, and workflow management.\n",
    "Includes examples for point-based and box-based extraction, variable exploration, and error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab7a58a",
   "metadata": {},
   "source": [
    "# 1️⃣ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climdata import ClimData\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(levelname)s | %(message)s\",\n",
    "    force=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2️⃣ Explore available datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dwd', 'mswx', 'hyras', 'cmip', 'power', 'w5e5', 'cmip_w5e5', 'nexgddp']\n"
     ]
    }
   ],
   "source": [
    "extractor = ClimData()\n",
    "datasets = extractor.get_datasets()\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3️⃣ Explore variables for a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n",
      "⚠️  Warning: Requested time range 1989-2020 extends beyond\n",
      "   the typical Historical period (1850-2014).\n",
      "   Data availability may be limited.\n",
      "['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "INFO | 46 models found for experiment 'ssp245'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n",
      "['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n"
     ]
    }
   ],
   "source": [
    "variables = extractor.get_variables('w5e5')\n",
    "print(variables)\n",
    "\n",
    "# for CMIP\n",
    "import climdata\n",
    "extractor_CMIP = climdata.CMIP(extractor.cfg)\n",
    "print(extractor_CMIP.get_experiment_ids())\n",
    "print(extractor_CMIP.get_source_ids('ssp245'))\n",
    "print(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4️⃣ Explore metadata for a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n",
      "**********************************************************************\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable 'rlds' not found in varinfo",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(variables)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m varinfo \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_varinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrlds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(varinfo)\n",
      "File \u001b[0;32m/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:921\u001b[0m, in \u001b[0;36mClimateExtractor.get_varinfo\u001b[0;34m(self, var)\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration or varinfo not loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m var \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mvarinfo:\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in varinfo\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mvarinfo[var]\n",
      "\u001b[0;31mValueError\u001b[0m: Variable 'rlds' not found in varinfo"
     ]
    }
   ],
   "source": [
    "variables = extractor.get_variables('w5e5')\n",
    "print(variables)\n",
    "print(\"*\"*70)\n",
    "varinfo = extractor.get_varinfo('rlds')\n",
    "print(varinfo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5️⃣ Explore available workflow actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['extract', 'calc_index', 'impute', 'to_nc', 'to_csv', 'upload_netcdf', 'upload_csv'])\n"
     ]
    }
   ],
   "source": [
    "actions = extractor.get_actions()\n",
    "print(actions.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6fe84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['heat_wave_index', 'heat_wave_frequency', 'heat_wave_max_length', 'heat_wave_total_length', 'hot_spell_frequency', 'hot_spell_max_length', 'hot_spell_total_length', 'hot_spell_max_magnitude', 'ice_days', 'isothermality', 'maximum_consecutive_frost_days', 'maximum_consecutive_frost_free_days', 'maximum_consecutive_tx_days'])\n",
      "dict_keys(['BRITS', 'XGBOOST', 'CDRec', 'SoftImpute'])\n"
     ]
    }
   ],
   "source": [
    "indices = extractor.get_indices(['tasmin', 'tasmax'])\n",
    "print(indices.keys())\n",
    "\n",
    "impute_methods = extractor.get_impute_methods()\n",
    "print(impute_methods.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6️⃣ Point extraction workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n",
      "INFO | Completed action: extract\n",
      "INFO | Starting action: impute\n",
      "INFO | \u001b[93mNo missing data found. Imputation not required.\u001b[0m\n",
      "INFO | Completed action: impute\n",
      "INFO | Starting action: calc_index\n",
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires ≥30 years, got 11\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | Completed action: calc_index\n",
      "INFO | Starting action: to_nc\n",
      "<frozen importlib._bootstrap>:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n",
      "INFO | Dataset saved to NetCDF file: cmip_tn10p_LAT12.891982026993958_LON24.246667038198012_2004-01-01_2014-12-31.nc\n",
      "INFO | Completed action: to_nc\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Define the area of interest (AOI)\n",
    "# -----------------------------\n",
    "# The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude].\n",
    "geojson = {\n",
    "  \"type\": \"FeatureCollection\",\n",
    "  \"features\": [\n",
    "    {\n",
    "      \"type\": \"Feature\",\n",
    "      \"properties\": {},\n",
    "      \"geometry\": {\n",
    "        \"coordinates\": [\n",
    "          24.246667038198012,  # longitude\n",
    "          12.891982026993958   # latitude\n",
    "        ],\n",
    "        \"type\": \"Point\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Define configuration overrides\n",
    "# -----------------------------\n",
    "# Overrides are strings used by Hydra to modify default configurations at runtime.\n",
    "overrides = [\n",
    "    \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n",
    "    f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above\n",
    "    f\"time_range.start_date=2004-01-01\",  # Start date for data extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n",
    "    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n",
    "    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n",
    "    \"index=tn10p\",  # Climate extreme index to calculate\n",
    "    \"impute=BRITS\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Define the workflow sequence\n",
    "# -----------------------------\n",
    "seq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Initialize the ClimData extractor\n",
    "# -----------------------------\n",
    "extractor = ClimData(overrides=overrides)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Run the Multi-Step workflow\n",
    "# -----------------------------\n",
    "result = extractor.run_workflow(\n",
    "    actions=seq,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ca2bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n",
      "✅ All 31 pr files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires ≥30 years, got 1\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | DataFrame saved to CSV file: index.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'index.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# -----------------------------\n",
    "# Define the area of interest (AOI)\n",
    "# -----------------------------\n",
    "# This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012\n",
    "geojson = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": [\n",
    "        {\n",
    "            \"type\": \"Feature\",\n",
    "            \"properties\": {},\n",
    "            \"geometry\": {\n",
    "                \"coordinates\": [24.246667038198012, 12.891982026993958],\n",
    "                \"type\": \"Point\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Define configuration overrides\n",
    "# -----------------------------\n",
    "# These strings override the default hydra config at runtime\n",
    "overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature & precipitation\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "    # Optional Google service account if needed for MSWX access\n",
    "    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n",
    "    \"index=tn10p\",  # Extreme climate index to calculate\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize the ClimData extractor\n",
    "# -----------------------------\n",
    "# This loads the configuration with overrides and prepares the object\n",
    "extractor = ClimData(overrides=overrides)\n",
    "\n",
    "# -----------------------------\n",
    "# Extract climate data\n",
    "# -----------------------------\n",
    "# Returns an xarray.Dataset for the selected variables, AOI, and time range\n",
    "ds = extractor.extract()\n",
    "\n",
    "# -----------------------------\n",
    "# Compute the climate index\n",
    "# -----------------------------\n",
    "# Takes the extracted dataset and calculates the extreme index \"tn10p\"\n",
    "# Returns a new xarray.Dataset containing only the index\n",
    "ds_index = extractor.calc_index(ds)\n",
    "\n",
    "# -----------------------------\n",
    "# Convert the index dataset to a long-form pandas DataFrame\n",
    "# -----------------------------\n",
    "# Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\")\n",
    "df_index = extractor.to_dataframe(ds_index)\n",
    "\n",
    "# -----------------------------\n",
    "# Save the DataFrame to CSV\n",
    "# -----------------------------\n",
    "# This will write the index values to \"index.csv\" in the current working directory\n",
    "extractor.to_csv(df_index, filename=\"index.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index.csv\n"
     ]
    }
   ],
   "source": [
    "print(extractor.current_filename)\n",
    "# print(extractor_point.filename_nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7️⃣ Box extraction workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Completed action: extract\n",
      "INFO | Starting action: to_csv\n",
      "INFO | DataFrame saved to CSV file: mswx_tasmin_tasmax_LAT_34.0_71.0_LON_-25.0_45.0_2014-12-01_2014-12-31.csv\n",
      "INFO | Completed action: to_csv\n"
     ]
    }
   ],
   "source": [
    "box_overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    \"region=europe\", # Select the region\n",
    "    \"variables=[tasmin,tasmax]\",\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "]\n",
    "\n",
    "extractor_box = ClimData(overrides=box_overrides)\n",
    "result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_csv\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8️⃣ Compute extreme index only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: extract\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All 31 tasmin files already exist locally.\n",
      "✅ All 31 tasmax files already exist locally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Completed action: extract\n",
      "INFO | Starting action: calc_index\n",
      "/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index heat_wave_max_length usually requires ≥30 years, got 1\n",
      "  warnings.warn(f\"Index {cfg.index} usually requires ≥30 years, got {n_years}\", UserWarning)\n",
      "INFO | Completed action: calc_index\n",
      "INFO | Starting action: to_dataframe\n",
      "ERROR | Action 'to_dataframe' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 862, in run_workflow\n",
      "    raise ValueError(f\"Unknown action '{action}'\")\n",
      "ValueError: Unknown action 'to_dataframe'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown action 'to_dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 15\u001b[0m\n\u001b[1;32m      2\u001b[0m idx_overrides \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset=mswx\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Select the MSWX dataset for extraction\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlat=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlat_berlin\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Select the region\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex=heat_wave_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m ]\n\u001b[1;32m     14\u001b[0m extractor_idx \u001b[38;5;241m=\u001b[39m ClimData(overrides\u001b[38;5;241m=\u001b[39midx_overrides)\n\u001b[0;32m---> 15\u001b[0m result_idx \u001b[38;5;241m=\u001b[39m \u001b[43mextractor_idx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mextract\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcalc_index\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_dataframe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m result_idx\u001b[38;5;241m.\u001b[39mdataframe\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:862\u001b[0m, in \u001b[0;36mClimateExtractor.run_workflow\u001b[0;34m(self, overrides, actions, file)\u001b[0m\n\u001b[1;32m    859\u001b[0m         result\u001b[38;5;241m.\u001b[39mimpute_ds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimpute_ds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 862\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown action \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted action: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, action)\n\u001b[1;32m    864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown action 'to_dataframe'"
     ]
    }
   ],
   "source": [
    "lat_berlin, lon_berlin = [52.5,13.4]\n",
    "idx_overrides = [\n",
    "    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n",
    "    f\"lat={lat_berlin}\", # Select the region\n",
    "    f\"lon={lon_berlin}\",\n",
    "    \"variables=[tasmin,tasmax]\",\n",
    "    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n",
    "    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n",
    "    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n",
    "    \"index=heat_wave_max_length\"\n",
    "]\n",
    "\n",
    "\n",
    "extractor_idx = ClimData(overrides=idx_overrides)\n",
    "result_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_csv\"])\n",
    "result_idx.dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9️⃣ Error examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: calc_index\n",
      "ERROR | Action 'calc_index' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 760, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: to_csv\n",
      "ERROR | Action 'to_csv' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 778, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'to_csv' requires a DataFrame, but no DataFrame is available. Use 'to_dataframe' or upload a CSV before saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'to_csv' requires a DataFrame, but no DataFrame is available. Use 'to_dataframe' or upload a CSV before saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Starting action: upload_netcdf\n",
      "ERROR | Action 'upload_netcdf' failed\n",
      "Traceback (most recent call last):\n",
      "  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 717, in run_workflow\n",
      "    raise ValueError(\n",
      "ValueError: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"calc_index\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"to_csv\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "\n",
    "try:\n",
    "    bad_ex = ClimData()\n",
    "    bad_ex.run_workflow(actions=[\"upload_netcdf\"])\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452cf67d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
