{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to climdata","text":""},{"location":"#climdata-quickstart-overview","title":"ClimData \u2014 Quickstart &amp; Overview","text":"<p>ClimData provides a unified interface for extracting climate data from multiple providers (MSWX, CMIP, POWER, DWD, HYRAS), computing extreme indices, and converting results to tabular form. The ClimData (or ClimateExtractor) class is central: it manages configuration, extraction, index computation, and common I/O.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Provider-agnostic extraction (point / region / shapefile)</li> <li>Unit normalization via xclim</li> <li>Compute extreme indices using package indices</li> <li>Convert xarray Datasets \u2192 long-form pandas DataFrames</li> <li>Simple workflow runner for chained actions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>1) Create and activate a conda environment: <pre><code># create\nconda create -n climdata python=3.11 -y\n\n# activate\nconda activate climdata\n</code></pre></p> <p>2) Install via pip (PyPI, if available) or from source: <pre><code># from PyPI\npip install climdata\n\n# or from local source (editable)\ngit clone &lt;repo-url&gt;\ncd climdata\npip install -e .\n</code></pre></p> <p>Install optional extras as needed (e.g., xclim, shapely, hydra, dask): <pre><code>pip install xarray xclim shapely hydra-core dask \"pandas&gt;=1.5\"\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from climdata import ClimData  # or from climdata.utils.wrapper_workflow import ClimateExtractor\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n    \"data_dir=/path/to/data\",\n    \"index=tn10p\",\n]\n\n# initialize\nextractor = ClimData(overrides=overrides)\n\n# extract data (returns xarray.Dataset and updates internal state)\nds = extractor.extract()\n\n# compute index (uses cfg.index)\nds_index = extractor.calc_index(ds)\n\n# convert to long-form dataframe and save\ndf = extractor.to_dataframe(ds_index)\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"#workflow-runner","title":"Workflow runner","text":"<p>Use <code>run_workflow</code> for multi-step sequences: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n</code></pre> <code>WorkflowResult</code> contains produced dataset(s), dataframe(s), and filenames.</p>"},{"location":"#documentation-api","title":"Documentation &amp; API","text":"<ul> <li>See API docs under <code>docs/api/</code> for detailed descriptions of ClimData/ClimateExtractor methods.</li> <li>Examples and notebooks are under <code>examples/</code>.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Run tests and lint locally.</li> <li>Follow project coding and documentation conventions; submit PRs with tests.</li> </ul>"},{"location":"#license","title":"License","text":"<p>Refer to the repository LICENSE file for terms.</p>"},{"location":"#tip","title":"\u26a1\ufe0f Tip","text":"<ul> <li> <p>Make sure <code>yq</code> is installed:   <pre><code>brew install yq   # macOS\n# OR\npip install yq\n</code></pre></p> </li> <li> <p>To see available variables for a specific dataset (for example <code>mswx</code>), run:   <pre><code>python download_location.py --cfg job | yq '.mappings.mswx.variables | keys'\n</code></pre></p> </li> </ul>"},{"location":"#key-features_1","title":"\u2699\ufe0f Key Features","text":"<ul> <li>Supports multiple weather data providers</li> <li>Uses <code>xarray</code> for robust gridded data extraction</li> <li>Handles curvilinear and rectilinear grids</li> <li>Uses a Google Drive Service Account for secure downloads</li> <li>Easily reproducible runs using Hydra</li> </ul>"},{"location":"#google-drive-api-setup","title":"\ud83d\udce1 Google Drive API Setup","text":"<p>This project uses the Google Drive API with a Service Account to securely download weather data files from a shared Google Drive folder.</p> <p>Follow these steps to set it up correctly:</p>"},{"location":"#1-create-a-google-cloud-project","title":"\u2705 1. Create a Google Cloud Project","text":"<ul> <li>Go to Google Cloud Console.</li> <li>Click \u201cSelect Project\u201d \u2192 \u201cNew Project\u201d.</li> <li>Enter a project name (e.g. <code>WeatherDataDownloader</code>).</li> <li>Click \u201cCreate\u201d.</li> </ul>"},{"location":"#2-enable-the-google-drive-api","title":"\u2705 2. Enable the Google Drive API","text":"<ul> <li>In the left sidebar, go to APIs &amp; Services \u2192 Library.</li> <li>Search for \u201cGoogle Drive API\u201d.</li> <li>Click it, then click \u201cEnable\u201d.</li> </ul>"},{"location":"#3-create-a-service-account","title":"\u2705 3. Create a Service Account","text":"<ul> <li>Go to IAM &amp; Admin \u2192 Service Accounts.</li> <li>Click \u201cCreate Service Account\u201d.</li> <li>Enter a name (e.g. <code>weather-downloader-sa</code>).</li> <li>Click \u201cCreate and Continue\u201d. You can skip assigning roles for read-only Drive access.</li> <li>Click \u201cDone\u201d to finish.</li> </ul>"},{"location":"#4-create-and-download-a-json-key","title":"\u2705 4. Create and Download a JSON Key","text":"<ul> <li>After creating the Service Account, click on its email address to open its details.</li> <li>Go to the \u201cKeys\u201d tab.</li> <li>Click \u201cAdd Key\u201d \u2192 \u201cCreate new key\u201d \u2192 choose <code>JSON</code> \u2192 click \u201cCreate\u201d.</li> <li>A <code>.json</code> key file will download automatically. Store it securely!</li> </ul>"},{"location":"#5-store-the-json-key-securely","title":"\u2705 5. Store the JSON Key Securely","text":"<ul> <li>Place the downloaded <code>.json</code> key in the conf folder with the name service.json. </li> </ul>"},{"location":"#setup-instructions-from-era5-api","title":"Setup Instructions from ERA5 api","text":""},{"location":"#1-cds-api-key-setup","title":"1. CDS API Key Setup","text":"<ol> <li>Create a free account on the Copernicus Climate Data Store</li> <li>Once logged in, go to your user profile</li> <li>Click on the \"Show API key\" button</li> <li>Create the file <code>~/.cdsapirc</code> with the following content:</li> </ol> <pre><code>url: https://cds.climate.copernicus.eu/api/v2\nkey: &lt;your-api-key-here&gt;\n</code></pre> <ol> <li>Make sure the file has the correct permissions: <code>chmod 600 ~/.cdsapirc</code></li> </ol>"},{"location":"INTEGRATION_GUIDE/","title":"W5E5 Dataset Integration Guide","text":""},{"location":"INTEGRATION_GUIDE/#quick-start","title":"Quick Start","text":"<p>The W5E5 dataset from ISIMIP has been successfully integrated into the climdata package. Here's everything you need to know:</p>"},{"location":"INTEGRATION_GUIDE/#what-was-added","title":"What Was Added","text":""},{"location":"INTEGRATION_GUIDE/#1-new-dataset-module","title":"1. New Dataset Module","text":"<p>File: <code>climdata/datasets/W5E5.py</code></p> <p>A complete implementation following the same pattern as existing datasets (ERA5, MSWX, POWER) with: - <code>fetch()</code> - Download data from ISIMIP repository - <code>load()</code> - Load NetCDF files into xarray - <code>extract()</code> - Spatial subsetting (point, box, shapefile) - <code>save_csv()</code> / <code>save_netcdf()</code> - Save results</p>"},{"location":"INTEGRATION_GUIDE/#2-configuration","title":"2. Configuration","text":"<p>File: <code>climdata/conf/mappings/parameters.yaml</code></p> <p>Added W5E5 configuration section with: - Dataset parameters (simulation_round, product, forcing, scenario) - Variable mappings (tas, tasmax, tasmin, pr, rsds, hurs, sfcWind, etc.) - Metadata (units, long names)</p>"},{"location":"INTEGRATION_GUIDE/#3-dependency","title":"3. Dependency","text":"<p>File: <code>requirements.txt</code></p> <p>Added <code>isimip-client</code> package for accessing ISIMIP data repository</p>"},{"location":"INTEGRATION_GUIDE/#4-documentation-examples","title":"4. Documentation &amp; Examples","text":"<ul> <li>Example Notebook: <code>docs/examples/w5e5_example.ipynb</code></li> <li>Test Script: <code>tests/test_w5e5.py</code></li> <li>Full Documentation: <code>docs/W5E5_README.md</code></li> </ul>"},{"location":"INTEGRATION_GUIDE/#usage-comparison","title":"Usage Comparison","text":""},{"location":"INTEGRATION_GUIDE/#w5e5-vs-other-datasets","title":"W5E5 vs Other Datasets","text":"<pre><code># ========== W5E5 (NEW) ==========\nfrom climdata.datasets.W5E5 import W5E5\nw5e5 = W5E5(cfg)\nw5e5.fetch()  # Downloads from ISIMIP\nw5e5.load()\nw5e5.extract(point=(lon, lat))\nw5e5.save_csv('output.csv')\n\n# ========== ERA5 (Existing) ==========\nfrom climdata.datasets.ERA5 import ERA5Mirror\nera5 = ERA5Mirror(base_path, fs)\nera5.download_chunk(variable, year, month)\n# ... different API\n\n# ========== MSWX (Existing) ==========\nfrom climdata.datasets.MSWX import MSWXmirror\nmswx = MSWXmirror(cfg)\nmswx.fetch(folder_id, variable)\nmswx.load()\nmswx.extract(point=(lon, lat))\n\n# ========== NASA POWER (Existing) ==========\nfrom climdata.datasets.NASAPOWER import POWER\npower = POWER(cfg)\npower.fetch()\npower.load()\npower.extract(start, end)\n</code></pre> <p>W5E5 follows the MSWX/POWER pattern - simpler and more consistent!</p>"},{"location":"INTEGRATION_GUIDE/#configuration-examples","title":"Configuration Examples","text":""},{"location":"INTEGRATION_GUIDE/#using-config-file","title":"Using Config File","text":"<pre><code># In config.yaml or as overrides\ndataset: w5e5\nlat: 52.52\nlon: 13.405\nvariables: [tas, tasmax, tasmin, pr, rsds]\ntime_range:\n  start_date: \"2010-01-01\"\n  end_date: \"2010-12-31\"\ndata_dir: ./data\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#using-python-overrides","title":"Using Python Overrides","text":"<pre><code>from climdata.utils.config import load_config\n\ncfg = load_config(\n    config_name='config',\n    overrides=[\n        'dataset=w5e5',\n        'lat=40.7128',\n        'lon=-74.0060',\n        'variables=[tas,pr,rsds]',\n        'time_range.start_date=2015-01-01',\n        'time_range.end_date=2015-12-31'\n    ]\n)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#data-access-flow","title":"Data Access Flow","text":"<pre><code>User Request\n    \u2193\nW5E5(cfg) - Initialize with config\n    \u2193\nfetch() - Search ISIMIP API \u2192 Download files \u2192 Cache locally\n    \u2193\nload() - Open NetCDF \u2192 Merge variables \u2192 Subset time range\n    \u2193\nextract() - Spatial subset (point/box/shape)\n    \u2193\nsave_csv() / save_netcdf() - Export results\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":""},{"location":"INTEGRATION_GUIDE/#works-with-wrapperpy","title":"Works with wrapper.py","text":"<pre><code># If you're using the wrapper workflow, W5E5 can be integrated:\nfrom climdata.utils.wrapper import DatasetWrapper\n\nwrapper = DatasetWrapper(\n    dataset='w5e5',\n    lat=52.52,\n    lon=13.405,\n    variables=['tas', 'pr'],\n    start_date='2010-01-01',\n    end_date='2010-12-31'\n)\n\n# The wrapper would need to be updated to recognize 'w5e5' dataset\n# and instantiate the W5E5 class accordingly\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#works-with-cli","title":"Works with CLI","text":"<pre><code># After updating the CLI to recognize w5e5:\nclimdata fetch --dataset w5e5 --lat 52.52 --lon 13.405 \\\n               --variables tas,pr --start 2010-01-01 --end 2010-12-31\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#testing-the-implementation","title":"Testing the Implementation","text":""},{"location":"INTEGRATION_GUIDE/#1-quick-test-python-script","title":"1. Quick Test (Python Script)","text":"<pre><code>cd /beegfs/muduchuru/pkgs_fnl/climdata\npython tests/test_w5e5.py\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#2-notebook-test","title":"2. Notebook Test","text":"<pre><code>jupyter notebook docs/examples/w5e5_example.ipynb\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#3-manual-test","title":"3. Manual Test","text":"<pre><code>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\n\ncfg = load_config(overrides=['dataset=w5e5', 'lat=52.52', 'lon=13.405'])\nw5e5 = W5E5(cfg)\nprint(\"\u2713 W5E5 initialized successfully!\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#next-steps-optional-enhancements","title":"Next Steps (Optional Enhancements)","text":""},{"location":"INTEGRATION_GUIDE/#1-add-to-wrappercli","title":"1. Add to Wrapper/CLI","text":"<p>Update <code>climdata/utils/wrapper.py</code> or CLI to recognize 'w5e5' dataset:</p> <pre><code>def get_dataset_instance(dataset_name, cfg):\n    if dataset_name == 'w5e5':\n        from climdata.datasets.W5E5 import W5E5\n        return W5E5(cfg)\n    elif dataset_name == 'mswx':\n        from climdata.datasets.MSWX import MSWXmirror\n        return MSWXmirror(cfg)\n    # ... etc\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#2-add-unit-tests","title":"2. Add Unit Tests","text":"<p>Create proper unit tests in <code>tests/test_climdata.py</code>:</p> <pre><code>def test_w5e5_init():\n    cfg = create_test_config('w5e5')\n    w5e5 = W5E5(cfg)\n    assert w5e5.client is not None\n\ndef test_w5e5_variable_mapping():\n    w5e5 = W5E5(test_cfg)\n    assert w5e5._map_variable_name('tas') == 'tas'\n    assert w5e5._map_variable_name('sfcWind') == 'sfcwind'\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#3-add-to-documentation","title":"3. Add to Documentation","text":"<p>Update main documentation files: - <code>README.md</code> - Add W5E5 to list of supported datasets - <code>docs/usage.md</code> - Add W5E5 usage examples - <code>docs/api.md</code> - Add W5E5 API documentation</p>"},{"location":"INTEGRATION_GUIDE/#dataset-comparison-summary","title":"Dataset Comparison Summary","text":"Feature W5E5 ERA5 MSWX POWER Resolution 0.5\u00b0 0.25\u00b0 0.1\u00b0 0.5\u00b0 Period 1979+ 1950+ 1979-2020 1981+ Variables 10+ 100+ 7 6+ Source ISIMIP CDS Google Drive NASA API Access isimip-client cdsapi Google API REST API Speed Medium Slow Fast Very Fast Quality High Very High High Good"},{"location":"INTEGRATION_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INTEGRATION_GUIDE/#import-error","title":"Import Error","text":"<p><pre><code>ImportError: isimip-client is required\n</code></pre> Solution: <code>pip install isimip-client</code></p>"},{"location":"INTEGRATION_GUIDE/#download-error","title":"Download Error","text":"<p><pre><code>Error fetching tas: ...\n</code></pre> Solution: Check internet connection and ISIMIP API status</p>"},{"location":"INTEGRATION_GUIDE/#memory-error","title":"Memory Error","text":"<p><pre><code>MemoryError during load\n</code></pre> Solution: Extract smaller region or shorter time period</p>"},{"location":"INTEGRATION_GUIDE/#files-createdmodified","title":"Files Created/Modified","text":""},{"location":"INTEGRATION_GUIDE/#created","title":"Created","text":"<ul> <li>\u2705 <code>climdata/datasets/W5E5.py</code> - Main implementation</li> <li>\u2705 <code>docs/W5E5_README.md</code> - Full documentation</li> <li>\u2705 <code>docs/examples/w5e5_example.ipynb</code> - Example notebook</li> <li>\u2705 <code>tests/test_w5e5.py</code> - Test script</li> <li>\u2705 <code>docs/INTEGRATION_GUIDE.md</code> - This file</li> </ul>"},{"location":"INTEGRATION_GUIDE/#modified","title":"Modified","text":"<ul> <li>\u2705 <code>requirements.txt</code> - Added isimip-client</li> <li>\u2705 <code>climdata/conf/mappings/parameters.yaml</code> - Added w5e5 config</li> </ul>"},{"location":"INTEGRATION_GUIDE/#summary","title":"Summary","text":"<p>\u2705 W5E5 dataset is fully integrated and ready to use!</p> <p>The implementation: - \u2705 Follows existing dataset patterns - \u2705 Uses ISIMIP API via isimip-client - \u2705 Supports all standard operations (fetch, load, extract, save) - \u2705 Includes comprehensive documentation and examples - \u2705 Is configurable via YAML/Python - \u2705 Works with existing climdata infrastructure</p> <p>You can start using W5E5 immediately with the examples provided!</p>"},{"location":"W5E5_README/","title":"W5E5 Dataset Support","text":"<p>The climdata package now includes support for the W5E5 (WFDE5 over land merged with ERA5 over ocean) global meteorological forcing dataset from ISIMIP.</p>"},{"location":"W5E5_README/#about-w5e5","title":"About W5E5","text":"<p>W5E5 is a high-quality global meteorological forcing dataset that combines: - WFDE5 (WATCH Forcing Data ERA5) over land - ERA5 reanalysis over ocean - Available through the ISIMIP (Inter-Sectoral Impact Model Intercomparison Project) data repository</p>"},{"location":"W5E5_README/#dataset-characteristics","title":"Dataset Characteristics","text":"<ul> <li>Spatial Resolution: 0.5\u00b0 \u00d7 0.5\u00b0 (approximately 50 km)</li> <li>Temporal Resolution: Daily</li> <li>Temporal Coverage: 1979 - present</li> <li>Global Coverage: Complete global coverage</li> <li>Data Source: ISIMIP3a (observational/historical climate)</li> </ul>"},{"location":"W5E5_README/#available-variables","title":"Available Variables","text":"Variable Name Description Units <code>tas</code> Near-Surface Air Temperature Mean daily temperature at 2m height K <code>tasmax</code> Maximum Temperature Daily maximum temperature at 2m height K <code>tasmin</code> Minimum Temperature Daily minimum temperature at 2m height K <code>pr</code> Precipitation Total daily precipitation kg m\u207b\u00b2 s\u207b\u00b9 <code>rsds</code> Shortwave Radiation Surface downwelling shortwave radiation W m\u207b\u00b2 <code>rlds</code> Longwave Radiation Surface downwelling longwave radiation W m\u207b\u00b2 <code>hurs</code> Relative Humidity Near-surface relative humidity % <code>sfcWind</code> Wind Speed Near-surface wind speed m s\u207b\u00b9 <code>ps</code> Surface Pressure Surface air pressure Pa <code>huss</code> Specific Humidity Near-surface specific humidity 1"},{"location":"W5E5_README/#installation","title":"Installation","text":"<p>To use the W5E5 dataset, you need to install the <code>isimip-client</code> package:</p> <pre><code>pip install isimip-client\n</code></pre> <p>Or if installing climdata from the updated requirements:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"W5E5_README/#basic-usage","title":"Basic Usage","text":""},{"location":"W5E5_README/#1-using-the-configuration-system","title":"1. Using the Configuration System","text":"<pre><code>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\n\n# Load configuration\ncfg = load_config(\n    config_name='config',\n    overrides=[\n        'dataset=w5e5',\n        'lat=52.52',  # Berlin, Germany\n        'lon=13.405',\n        'variables=[tas,tasmax,tasmin,pr,rsds]',\n        'time_range.start_date=2010-01-01',\n        'time_range.end_date=2010-12-31',\n        'data_dir=./data'\n    ]\n)\n\n# Initialize and fetch data\nw5e5 = W5E5(cfg)\nw5e5.fetch()  # Download from ISIMIP\nw5e5.load()   # Load into xarray\nw5e5.extract(point=(cfg.lon, cfg.lat))  # Extract for location\n\n# Save results\nw5e5.save_csv('output.csv')\nw5e5.save_netcdf('output.nc')\n</code></pre>"},{"location":"W5E5_README/#2-direct-instantiation","title":"2. Direct Instantiation","text":"<pre><code>from omegaconf import OmegaConf\nfrom climdata.datasets.W5E5 import W5E5\n\n# Create configuration manually\ncfg = OmegaConf.create({\n    'dataset': 'w5e5',\n    'lat': 40.7128,  # New York\n    'lon': -74.0060,\n    'variables': ['tas', 'pr'],\n    'time_range': {\n        'start_date': '2015-01-01',\n        'end_date': '2015-12-31'\n    },\n    'data_dir': './w5e5_data'\n})\n\nw5e5 = W5E5(cfg)\nw5e5.fetch()\nw5e5.load()\nw5e5.extract(point=(cfg.lon, cfg.lat))\n</code></pre>"},{"location":"W5E5_README/#advanced-usage","title":"Advanced Usage","text":""},{"location":"W5E5_README/#spatial-extraction-options","title":"Spatial Extraction Options","text":""},{"location":"W5E5_README/#point-extraction","title":"Point Extraction","text":"<pre><code># Extract for a single point\nw5e5.extract(point=(lon, lat))\n\n# Extract with buffer (average over surrounding area)\nw5e5.extract(point=(lon, lat), buffer_km=50)\n</code></pre>"},{"location":"W5E5_README/#bounding-box-extraction","title":"Bounding Box Extraction","text":"<pre><code># Extract for a rectangular region\nw5e5.extract(box={\n    'lon_min': 10.0,\n    'lon_max': 15.0,\n    'lat_min': 50.0,\n    'lat_max': 55.0\n})\n</code></pre>"},{"location":"W5E5_README/#shapefile-extraction","title":"Shapefile Extraction","text":"<pre><code>import geopandas as gpd\n\n# Extract for a shapefile region\ngdf = gpd.read_file('region.shp')\nw5e5.extract(shapefile=gdf)\n</code></pre>"},{"location":"W5E5_README/#working-with-the-xarray-dataset","title":"Working with the xarray Dataset","text":"<pre><code># Access the loaded dataset\nds = w5e5.ds\n\n# Convert to pandas DataFrame\ndf = ds.to_dataframe()\n\n# Unit conversions\ndf['tas_celsius'] = df['tas'] - 273.15  # K to \u00b0C\ndf['pr_mm_day'] = df['pr'] * 86400      # kg/m\u00b2/s to mm/day\n\n# Basic statistics\nprint(df.describe())\n\n# Plotting\nimport matplotlib.pyplot as plt\ndf['tas_celsius'].plot(title='Temperature Time Series')\nplt.show()\n</code></pre>"},{"location":"W5E5_README/#data-access-details","title":"Data Access Details","text":""},{"location":"W5E5_README/#isimip-repository-structure","title":"ISIMIP Repository Structure","text":"<p>W5E5 data is organized in the ISIMIP repository as: <pre><code>ISIMIP3a/InputData/climate/atmosphere/obsclim/global/daily/historical/w5e5v2.0/\n</code></pre></p> <p>The W5E5 class automatically: 1. Searches for datasets matching your criteria 2. Downloads relevant files covering your time range 3. Caches files locally to avoid re-downloading 4. Loads and merges multiple files as needed</p>"},{"location":"W5E5_README/#file-naming-convention","title":"File Naming Convention","text":"<p>W5E5 files follow this pattern: <pre><code>w5e5v2.0_obsclim_{variable}_global_daily_{start_year}_{end_year}.nc\n</code></pre></p> <p>For example: <pre><code>w5e5v2.0_obsclim_tas_global_daily_2010_2019.nc\n</code></pre></p>"},{"location":"W5E5_README/#performance-tips","title":"Performance Tips","text":"<ol> <li>Download once: Downloaded files are cached in <code>data_dir/w5e5/</code></li> <li>Subset early: Extract for your region immediately after loading to reduce memory usage</li> <li>Use appropriate time ranges: W5E5 files cover multi-year periods, so choose ranges that minimize file downloads</li> <li>Parallel processing: The <code>load()</code> method uses dask for parallel file reading</li> </ol>"},{"location":"W5E5_README/#comparison-with-other-datasets","title":"Comparison with Other Datasets","text":"Feature W5E5 ERA5 MSWX NASA POWER Resolution 0.5\u00b0 0.25\u00b0 0.1\u00b0 0.5\u00b0 Start Year 1979 1950 1979 1981 Coverage Global Global Global Global Variables 10+ 100+ 7 6+ Update Freq Annual Monthly Annual Near real-time Quality High Very High High Good"},{"location":"W5E5_README/#when-to-use-w5e5","title":"When to Use W5E5","text":"<ul> <li>\u2705 Need bias-corrected ERA5 data</li> <li>\u2705 Working with ISIMIP climate impact models</li> <li>\u2705 Require consistent land-ocean dataset</li> <li>\u2705 0.5\u00b0 resolution is sufficient</li> <li>\u2705 Post-1979 period is adequate</li> </ul>"},{"location":"W5E5_README/#when-to-use-alternatives","title":"When to Use Alternatives","text":"<ul> <li>Use ERA5 if you need higher resolution (0.25\u00b0) or more variables</li> <li>Use MSWX if you need higher resolution (0.1\u00b0) and focus on precipitation</li> <li>Use NASA POWER if you need near real-time data or solar energy applications</li> </ul>"},{"location":"W5E5_README/#troubleshooting","title":"Troubleshooting","text":""},{"location":"W5E5_README/#installation-issues","title":"Installation Issues","text":"<pre><code># If isimip-client installation fails\npip install --upgrade pip\npip install isimip-client\n\n# Or use conda\nconda install -c conda-forge isimip-client\n</code></pre>"},{"location":"W5E5_README/#download-issues","title":"Download Issues","text":"<pre><code># Check ISIMIP API status\nfrom isimip_client.client import ISIMIPClient\nclient = ISIMIPClient()\nresponse = client.datasets(query='w5e5')\nprint(f\"Found {len(response.get('results', []))} datasets\")\n</code></pre>"},{"location":"W5E5_README/#memory-issues","title":"Memory Issues","text":"<pre><code># Process data in chunks for large regions\nw5e5.load()\n# Extract smaller region first\nw5e5.extract(box={'lon_min': 10, 'lon_max': 11, \n                   'lat_min': 50, 'lat_max': 51})\n</code></pre>"},{"location":"W5E5_README/#references","title":"References","text":"<ol> <li> <p>W5E5 Dataset: Lange, S. (2019). WFDE5 over land merged with ERA5 over the ocean (W5E5). V. 1.0. DOI: 10.5880/pik.2019.023</p> </li> <li> <p>ISIMIP Project: https://www.isimip.org/</p> </li> <li> <p>ISIMIP Data Repository: https://data.isimip.org/</p> </li> <li> <p>ISIMIP Client: https://github.com/ISI-MIP/isimip-client</p> </li> </ol>"},{"location":"W5E5_README/#example-notebooks","title":"Example Notebooks","text":"<p>See the following example notebooks: - <code>docs/examples/w5e5_example.ipynb</code> - Complete usage example</p>"},{"location":"W5E5_README/#support","title":"Support","text":"<p>For issues related to: - W5E5 implementation: Open an issue in the climdata repository - ISIMIP data access: Visit ISIMIP support - Data quality/methodology: See W5E5 documentation at PIK</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#climdata.utils.wrapper_workflow.ClimData","title":"<code> climdata.utils.wrapper_workflow.ClimData        </code>","text":"<p>Climate data extraction and extreme-index workflow manager.</p> <p>Provides a high-level API for:     - loading/configuring dataset providers via Hydra config     - uploading NetCDF/CSV content into xarray Datasets     - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER)     - computing extreme indices using configured xclim indices     - converting datasets to long-form DataFrames and saving results</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object describing dataset, region/time/variables, outputs.</p> <code>current_ds</code> <code>xr.Dataset</code> <p>The most recently loaded or extracted dataset.</p> <code>current_df</code> <code>pd.DataFrame</code> <p>The most recently produced long-form DataFrame.</p> <code>filename_csv/filename_nc/filename_zarr</code> <code>str</code> <p>Generated output filename templates/paths.</p> <p>Examples:</p> <p>extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe']) extractor.extract() idx_ds = extractor.calc_index() df = extractor.to_dataframe(idx_ds) extractor.to_csv(df)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>class ClimateExtractor:\n    \"\"\"Climate data extraction and extreme-index workflow manager.\n\n    Provides a high-level API for:\n        - loading/configuring dataset providers via Hydra config\n        - uploading NetCDF/CSV content into xarray Datasets\n        - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER)\n        - computing extreme indices using configured xclim indices\n        - converting datasets to long-form DataFrames and saving results\n\n    Attributes:\n        cfg (DictConfig): Hydra configuration object describing dataset, region/time/variables, outputs.\n        current_ds (xr.Dataset): The most recently loaded or extracted dataset.\n        current_df (pd.DataFrame): The most recently produced long-form DataFrame.\n        filename_csv/filename_nc/filename_zarr (str): Generated output filename templates/paths.\n\n    Example:\n        extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe'])\n        extractor.extract()\n        idx_ds = extractor.calc_index()\n        df = extractor.to_dataframe(idx_ds)\n        extractor.to_csv(df)\n    \"\"\"\n\n    def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n        \"\"\"Initialize the workflow manager and load configuration.\n\n        Args:\n            cfg_name (str): Name of the Hydra configuration (default: \"config\").\n            conf_path (str, optional): Optional config path override.\n            overrides (list[str], optional): Hydra overrides to apply to the configuration.\n        \"\"\"\n        self.cfg_name = cfg_name\n        self.conf_path = conf_path\n        self.cfg: Optional[DictConfig] = None\n\n        # Stage datasets\n        self.ds = None\n        self.current_ds = None\n        self.index_ds = None\n        self.impute_ds = None\n        self.bias_corrected_ds = None\n\n        # Stage DataFrames\n        self.raw_df = None\n        self.current_df = None\n        self.index_df = None\n        self.impute_df = None\n        self.bias_corrected_df = None\n        self.df = None  # alias for current_df\n\n        # filenames\n        self.filename = None\n        self.filetype = None\n\n        # Automatically load config on init\n        self.load_config(overrides)\n        self.cfg = self.preprocess_aoi(self.cfg)\n\n        # instance logger for this extractor\n        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n    def _gen_fn(self, *, ds: xr.Dataset = None, df: pd.DataFrame = None):\n        \"\"\"Create filenames (csv, nc, zarr) using config templates and dataset metadata.\n\n        Accepts either:\n        - ds : xarray.Dataset\n        - df : pandas.DataFrame (long form with columns: lat, lon, time/date, variable, value, optional source)\n\n        Exactly one must be provided (keyword-only).\n        \"\"\"\n\n        # ------------------------\n        # Validate inputs\n        # ------------------------\n        if (ds is None) == (df is None):\n            raise ValueError(\"Provide exactly one of `ds` or `df` as a keyword argument.\")\n\n        # ------------------------\n        # Helper: find coord alias (xarray)\n        # ------------------------\n        def find_coord(ds, names):\n            for name in names:\n                if name in ds.coords:\n                    return ds[name]\n            return None\n\n        # ------------------------\n        # Case 1: xarray.Dataset\n        # ------------------------\n        if ds is not None:\n            lat = find_coord(ds, [\"lat\", \"latitude\"])\n            lon = find_coord(ds, [\"lon\", \"longitude\"])\n            time = find_coord(ds, [\"time\", \"date\"])\n\n            provider = ds.attrs.get(\"source\", \"unknown\")\n            vars_list = list(ds.data_vars)\n            parameter = vars_list[0] if len(vars_list) == 1 else \"_\".join(vars_list)\n\n            # Latitude range\n            if lat is not None:\n                lat_vals = lat.values.reshape(-1)\n                lat_min, lat_max = float(lat_vals.min()), float(lat_vals.max())\n            else:\n                lat_min = lat_max = None\n\n            # Longitude range\n            if lon is not None:\n                lon_vals = lon.values.reshape(-1)\n                lon_min, lon_max = float(lon_vals.min()), float(lon_vals.max())\n            else:\n                lon_min = lon_max = None\n\n            # Time range\n            if time is not None:\n                tvals = pd.to_datetime(time.values)\n                start, end = tvals.min().strftime(\"%Y-%m-%d\"), tvals.max().strftime(\"%Y-%m-%d\")\n            else:\n                start = end = \"unknown\"\n\n        # ------------------------\n        # Case 2: pandas.DataFrame (long form)\n        # ------------------------\n        else:\n            cols = df.columns.astype(str)\n\n            # Identify coordinate columns\n            lat_cols = [c for c in cols if \"lat\" in c.lower()]\n            lon_cols = [c for c in cols if \"lon\" in c.lower()]\n            time_cols = [c for c in cols if \"time\" in c.lower() or \"date\" in c.lower()]\n\n            # Provider from 'source' column\n            if \"source\" in df.columns:\n                unique_sources = df[\"source\"].dropna().unique()\n                provider = unique_sources[0] if len(unique_sources) == 1 else \"_\".join(map(str, unique_sources))\n            else:\n                provider = \"unknown\"\n\n            # Unique parameters from 'variable' column\n            if \"variable\" in df.columns:\n                unique_parameters = sorted(df[\"variable\"].dropna().unique())\n                parameter = unique_parameters[0] if len(unique_parameters) == 1 else \"_\".join(unique_parameters)\n            else:\n                parameter = \"unknown\"\n\n            # Latitude range\n            if lat_cols:\n                lat_vals = pd.to_numeric(df[lat_cols[0]], errors=\"coerce\")\n                lat_min, lat_max = float(lat_vals.min()), float(lat_vals.max())\n            else:\n                lat_min = lat_max = None\n\n            # Longitude range\n            if lon_cols:\n                lon_vals = pd.to_numeric(df[lon_cols[0]], errors=\"coerce\")\n                lon_min, lon_max = float(lon_vals.min()), float(lon_vals.max())\n            else:\n                lon_min = lon_max = None\n\n            # Time range\n            if time_cols:\n                tvals = pd.to_datetime(df[time_cols[0]], errors=\"coerce\")\n                start = tvals.min().strftime(\"%Y-%m-%d\")\n                end = tvals.max().strftime(\"%Y-%m-%d\")\n            else:\n                start = end = \"unknown\"\n\n        # ------------------------\n        # Format lat/lon strings\n        # ------------------------\n        if lat_min is None:\n            lat_str = lat_range = \"unknown\"\n        elif lat_min == lat_max:\n            lat_str = lat_range = f\"{lat_min}\"\n        else:\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n\n        if lon_min is None:\n            lon_str = lon_range = \"unknown\"\n        elif lon_min == lon_max:\n            lon_str = lon_range = f\"{lon_min}\"\n        else:\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # ------------------------\n        # Build filenames\n        # ------------------------\n        outdir = Path(self.cfg.output.out_dir)\n        outdir.mkdir(parents=True, exist_ok=True)\n        def build(fn_template):\n            return fn_template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range,\n                lon_range=lon_range,\n            )\n\n        self.filename_csv = str(outdir / build(self.cfg.output.filename_csv))\n        self.filename_nc = str(outdir / build(self.cfg.output.filename_nc))\n        self.filename_zarr = str(outdir / build(self.cfg.output.filename_zarr))\n        return self\n    def _gen_fn_cfg(self):\n        \"\"\"Generate output filenames using configuration and extracted dataset metadata.\n\n        Uses settings from ``self.cfg`` and ``self.current_ds`` (if available) to build filename templates.\n        \"\"\"\n\n        cfg = self.cfg\n        out = cfg.output\n        provider = cfg.dataset.lower()\n        if self.current_ds:\n            if len(self.current_ds.data_vars) == 0:\n                parameter = \"unknown\"\n            elif len(self.current_ds.data_vars) == 1:\n                parameter = next(iter(self.current_ds.data_vars))\n            else:\n                parameter = \"_\".join(self.current_ds.data_vars)\n        else:\n            parameter = \"_\".join(self.cfg.variables)\n        # --------------------------------\n        # Determine lat/lon values\n        # --------------------------------\n        if cfg.lat is not None and cfg.lon is not None:\n            lat_range = lon_range = None   # single point\n            lat_str = str(cfg.lat)\n            lon_str = str(cfg.lon)\n        else:\n            b = cfg.bounds[cfg.region]\n            lat_min, lat_max = b[\"lat_min\"], b[\"lat_max\"]\n            lon_min, lon_max = b[\"lon_min\"], b[\"lon_max\"]\n\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # --------------------------------\n        # Time range from cfg\n        # --------------------------------\n        start = pd.to_datetime(cfg.time_range.start_date).strftime(\"%Y-%m-%d\")\n        end = pd.to_datetime(cfg.time_range.end_date).strftime(\"%Y-%m-%d\")\n\n        # --------------------------------\n        # Format filenames\n        # --------------------------------\n        def format_template(template):\n            return template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range or lat_str,\n                lon_range=lon_range or lon_str,\n            )\n\n        out_dir = Path(self.cfg.output.out_dir)\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        self.filename_csv = str(out_dir / format_template(out.filename_csv))\n        self.filename_nc = str(out_dir / format_template(out.filename_nc))\n        self.filename_zarr = str(out_dir / format_template(out.filename_zarr))\n\n    # ----------------------------\n    # Hydra config\n    # ----------------------------\n    def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n        \"\"\"Load and compose the Hydra configuration.\n\n        Args:\n            overrides (list[str], optional): Hydra overrides to apply when composing the configuration.\n\n        Returns:\n            DictConfig: Composed Hydra configuration object stored on ``self.cfg``.\n        \"\"\"\n        overrides = overrides or []\n        conf_dir = _ensure_local_conf()\n        rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n        if not GlobalHydra.instance().is_initialized():\n            hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n        else:\n            hydra_ctx = None\n\n        if hydra_ctx:\n            with hydra_ctx:\n                self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        else:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        return self.cfg\n\n    # ----------------------------\n    # AOI preprocessing\n    # ----------------------------\n    def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n        \"\"\"Process an 'aoi' specification in the configuration.\n\n        Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).\n\n        Args:\n            cfg (DictConfig): Configuration object with optional ``aoi`` entry.\n\n        Returns:\n            DictConfig: The modified configuration. When a Point is provided, ``cfg.lat`` and ``cfg.lon`` are set; when a Polygon is provided, ``cfg.bounds`` is set and ``cfg.region`` is set to \"custom\".\n        \"\"\"\n        if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n            return cfg\n\n        if isinstance(cfg.aoi, str):\n            try:\n                cfg.aoi = json.loads(cfg.aoi)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid AOI JSON string\")\n\n        aoi = cfg.aoi\n\n        if aoi.get(\"type\") == \"FeatureCollection\":\n            geom = shape(aoi[\"features\"][0][\"geometry\"])\n        elif aoi.get(\"type\") == \"Feature\":\n            geom = shape(aoi[\"geometry\"])\n        elif \"type\" in aoi:\n            geom = shape(aoi)\n        else:\n            raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n        if isinstance(geom, Point):\n            cfg.lat = geom.y\n            cfg.lon = geom.x\n            cfg.bounds = None\n        elif isinstance(geom, Polygon):\n            minx, miny, maxx, maxy = geom.bounds\n            cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                     \"lon_min\": minx, \"lon_max\": maxx}}\n            cfg.region = \"custom\"\n            cfg.lat = None\n            cfg.lon = None\n        else:\n            raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n        return cfg\n\n    # ----------------------------\n    # Upload NetCDF\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n        \"\"\"Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n        Args:\n            nc_file (str): Path to the NetCDF file to open.\n\n        Returns:\n            xr.Dataset: The loaded dataset (also sets ``self.current_ds``).\n        \"\"\"\n        if not os.path.exists(nc_file):\n            raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n        ds = xr.open_dataset(nc_file)\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                                for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Upload CSV \u2192 xarray.Dataset\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n        \"\"\"Load a long-form CSV into an xarray.Dataset.\n\n        The CSV must contain ``time`` and ``lat``/``latitude``, ``lon``/``longitude``, ``variable``, ``value``. Units may be supplied in a ``units`` column and an optional ``source`` column is recognized.\n\n        Args:\n            csv_file (str): Path to the CSV file to load.\n\n        Returns:\n            xr.Dataset: The converted dataset (also sets ``self.current_ds``).\n        \"\"\"\n        if not os.path.exists(csv_file):\n            raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n        df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n        lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n        lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n        if lat_col is None or lon_col is None:\n            raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n        id_vars = [\"time\", lat_col, lon_col]\n        df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n        ds = df_wide.set_index(id_vars).to_xarray()\n\n        # Attach units from CSV\n        for var in ds.data_vars:\n            units_series = df[df[\"variable\"] == var][\"units\"]\n            ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n        # Global source attribute\n        if \"source\" in df.columns:\n            source_series = df[\"source\"].dropna().unique()\n            if len(source_series) &gt; 0:\n                ds.attrs[\"source\"] = source_series[0]\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Extract data from datasets like CMIP, DWD, etc.\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def extract(self) -&gt; xr.Dataset:\n        \"\"\"Extract data from the configured provider using ``self.cfg``.\n\n        Uses provider-specific classes (e.g., ``CMIP``, ``DWD``, ``MSWX``, ``HYRAS``, ``POWER``)\n        to fetch, load and extract datasets. When extraction completes, units are converted to those declared in ``cfg.varinfo``, the dataset is computed, and filenames are generated from the configuration.\n\n        Returns:\n            xr.Dataset: The extracted and computed dataset (also sets ``self.current_ds``).\n        \"\"\"\n        cfg = self.cfg\n        extract_kwargs = {}\n\n        if cfg.lat is not None and cfg.lon is not None:\n            extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n            if cfg.dataset == \"dwd\":\n                extract_kwargs[\"buffer_km\"] = 30\n        elif cfg.region is not None:\n            extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n        elif cfg.shapefile is not None:\n            extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n        ds = None\n        dataset_upper = cfg.dataset.upper()\n\n        if dataset_upper == \"MSWX\":\n            ds_vars = []\n            for var in cfg.variables:\n                mswx = climdata.MSWX(cfg)\n                mswx.extract(**extract_kwargs)\n                mswx.load(var)\n                ds_vars.append(mswx.dataset)\n            ds = xr.merge(ds_vars)\n            self.dataset_class = mswx\n        elif dataset_upper == \"CMIP\":\n            cmip = climdata.CMIP(cfg)\n            cmip.fetch()\n            cmip.load()\n            cmip.extract(**extract_kwargs)\n            ds = cmip.ds\n            self.dataset_class = cmip\n        elif dataset_upper == \"POWER\":\n            power = climdata.POWER(cfg)\n            power.fetch()\n            power.load()\n            ds = power.ds\n            self.dataset_class = power\n        elif dataset_upper == \"DWD\":\n            ds_vars = []\n            for var in cfg.variables:\n                dwd = climdata.DWD(cfg)\n                ds_var = dwd.extract(variable=var, **extract_kwargs)\n                ds_vars.append(ds_var)\n            ds = xr.merge(ds_vars)\n            self.dataset_class = dwd\n        elif dataset_upper == \"HYRAS\":\n            hyras = climdata.HYRAS(cfg)\n            ds_vars = []\n            for var in cfg.variables:\n                hyras.extract(**extract_kwargs)\n                ds_vars.append(hyras.load(var, chunking={'time':\"auto\"})[[var]])\n            ds = xr.merge(ds_vars, compat=\"override\")\n            self.dataset_class = hyras\n        elif dataset_upper == \"W5E5\":\n            w5e5 = climdata.W5E5(cfg)\n            w5e5.fetch()  # Download from ISIMIP\n            w5e5.load()   # Load into xarray\n            w5e5.extract(**extract_kwargs)\n            ds = w5e5.ds\n            self.dataset_class = w5e5\n        for var in ds.data_vars:\n            ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n        # ds = ds.compute()\n\n        return ds\n    # ----------------------------\n    # Compute extreme index\n    # ----------------------------\n    @update_ds(attr_name='index_ds')\n    def calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"Calculate the configured extreme index using xclim indices.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to operate on. If ``None``, ``self.current_ds`` is used.\n\n        Returns:\n            xr.Dataset: The computed index as an xarray Dataset (also sets ``self.index_ds``).\n        \"\"\"\n        cfg = self.cfg\n\n        # Use provided ds or fallback\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.index is None:\n            self.logger.info(\"No index selected.\")\n            return None\n\n        if \"time\" in ds.coords:\n            years = pd.to_datetime(ds.time.values).year\n            n_years = len(pd.unique(years))\n            if n_years &lt; 30:\n                warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n        indices = extreme_index(cfg, ds)\n        index_ds = indices.calculate(cfg.index).compute()\n        index_ds = index_ds.to_dataset(name=cfg.index)\n\n        return index_ds\n    # ----------------------------\n    # Dataset \u2192 Long-form DataFrame\n    # ----------------------------\n    @update_df()\n    def to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n        \"\"\"Convert a dataset to a long-form pandas DataFrame.\n\n        The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to convert. If ``None``, uses ``self.current_ds``.\n\n        Returns:\n            pd.DataFrame: Long-form DataFrame (also sets ``self.current_df``).\n        \"\"\"\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        df = ds.to_dataframe().reset_index()\n\n        id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n        value_vars = [v for v in ds.data_vars if v in df.columns]\n\n        if not value_vars:\n            raise ValueError(\"No variables in dataset available to melt into long format\")\n\n        df_long = df.melt(\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"variable\",\n            value_name=\"value\"\n        )\n\n        df_long[\"units\"] = df_long[\"variable\"].apply(\n            lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n        )\n        if getattr(self.cfg, \"dataset\") == 'cmip':\n            df_long[\"source_id\"] = getattr(self.cfg, \"source_id\")\n        df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n        df_long = df_long.drop_duplicates()\n        self._gen_fn_cfg()\n        return df_long\n\n    # ----------------------------\n    # Save CSV\n    # ----------------------------\n    def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"Save a DataFrame to CSV.\n\n        Args:\n            df (pd.DataFrame, optional): DataFrame to save. Defaults to ``self.current_df``.\n            filename (str, optional): Output filename. Defaults to ``self.filename_csv``.\n\n        Returns:\n            str: The path of the written CSV file.\n        \"\"\"\n        df = df if df is not None else self.current_df\n\n        filename = filename or getattr(self, \"filename_csv\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_csv is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        df.to_csv(filename, index=False)\n        self.filename_csv = str(path)\n        self.current_filename = str(path)\n\n        # print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n        self.logger.info(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n        return filename\n\n    def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"Save an xarray Dataset to NetCDF.\n\n        Notes:\n            - If ``ds`` is ``None``: save ``current_ds``.\n            - If ``filename`` is ``None``: use ``self.filename_nc``.\n            - Creates directories if needed and updates ``self.filename_nc`` and ``self.current_filename``.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to save. If ``None``, uses ``self.current_ds``.\n            filename (str, optional): Output filename. Defaults to ``self.filename_nc``.\n\n        Returns:\n            str: The path of the written NetCDF file.\n        \"\"\"\n\n        # -------------------------------\n        # 1. Determine dataset to save\n        # -------------------------------\n        ds = ds or getattr(self, \"current_ds\", None)\n        if ds is None:\n            raise ValueError(\"No dataset available to save\")\n\n        # -------------------------------\n        # 2. Determine filename\n        # -------------------------------\n        filename = filename or getattr(self, \"filename_nc\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_nc is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # -------------------------------\n        # 3. Save to NetCDF\n        # -------------------------------\n        ds.to_netcdf(path)\n\n        # -------------------------------\n        # 4. Track filenames\n        # -------------------------------\n        self.filename_nc = str(path)\n        self.current_filename = str(path)\n\n        # print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n        self.logger.info(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n        return str(path)\n\n    # ----------------------------\n    # Unified workflow\n    # ----------------------------\n    def run_workflow(self, overrides: Optional[List[str]] = None,\n                     actions: Optional[List[str]] = None,\n                     file: Optional[str] = None) -&gt; WorkflowResult:\n        \"\"\"Execute a sequence of workflow actions.\n\n        Args:\n            overrides (list[str], optional): Hydra overrides to apply (not all actions will use these).\n            actions (list[str], optional): Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.\n            file (str, optional): File path used for upload actions when required.\n\n        Returns:\n            WorkflowResult: Named result container with populated fields for dataframe/dataset/filenames.\n        \"\"\"\n        actions = actions or [\"extract\", \"calc_index\", \"to_csv\", \"to_nc\"]\n        result = WorkflowResult(cfg=self.cfg)\n        for action in actions:\n            self.logger.info(\"Starting action: %s\", action)\n            try:\n                if action == \"upload_netcdf\":\n                    if file is None:\n                        raise ValueError(\n                            \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                            \"but none was provided.\"\n                        )\n                        # Validate extension\n                    valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                    if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                        raise ValueError(\n                            f\"Invalid file format for upload_netcdf: '{file}'. \"\n                            f\"Expected one of: {valid_nc_ext}\"\n                        )\n                    self.upload_netcdf(file)\n                    result.dataset = self.current_ds\n\n                elif action == \"upload_csv\":\n                    if file is None:\n                        raise ValueError(\n                            \"Action 'upload_csv' requires argument 'csv_file', \"\n                            \"but none was provided.\"\n                        )\n\n                    # Validate CSV extension\n                    valid_csv_ext = (\".csv\", \".csv.gz\")\n                    if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                        raise ValueError(\n                            f\"Invalid file format for upload_csv: '{file}'. \"\n                            f\"Expected one of: {valid_csv_ext}\"\n                        )\n\n                    self.upload_csv(file)\n                    result.dataset = self.current_ds\n\n                elif action == \"extract\":\n                    if self.cfg.dataset is None:\n                        raise ValueError(\n                            \"Action 'extract' cannot run because no dataset provider is set \"\n                            \"(cfg.dataset is None).\"\n                        )\n                    self.extract()\n                    result.dataset = self.current_ds\n\n                elif action == \"calc_index\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before computing an index.\"\n                        )\n                    self.calc_index()\n                    result.index_ds = self.current_ds\n\n                elif action == \"to_csv\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before converting to a DataFrame.\"\n                        )\n                    self.to_dataframe()\n                    result.dataframe = self.current_df\n                    result.filename = self.to_csv()\n\n                elif action == \"to_nc\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before saving to NetCDF.\"\n                        )\n                    result.filename = self.to_nc()\n\n                elif action == \"impute\":\n                    if self.current_ds is None:\n                        raise ValueError(\"Action 'impute' requires a dataset, but no dataset is available.\")\n                    self.impute()\n                    result.dataset = self.current_ds\n                    result.impute_ds = getattr(self, \"impute_ds\", None)\n\n                else:\n                    raise ValueError(f\"Unknown action '{action}'\")\n                self.logger.info(\"Completed action: %s\", action)\n            except Exception:\n                self.logger.exception(\"Action '%s' failed\", action)\n                raise\n\n        return result\n\n    # ----------------------------\n    # Exploration helpers using cfg.dsinfo\n    # ----------------------------\n    def get_datasets(self) -&gt; List[str]:\n        \"\"\"Return the list of dataset provider names available in configuration.\n\n        Returns:\n            List[str]: Names of available dataset providers from ``cfg.dsinfo``.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n        return list(self.cfg.dsinfo.keys())\n\n    def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n        \"\"\"Return the list of variables available for a dataset.\n\n        Args:\n            dataset (str, optional): Dataset name to query. Defaults to ``cfg.dataset``.\n\n        Returns:\n            List[str]: List of variable names.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n\n        dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n        if dataset_name is None:\n            raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n        dsinfo = self.cfg.dsinfo.get(dataset_name)\n        if not dsinfo or \"variables\" not in dsinfo:\n            raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n        return list(dsinfo[\"variables\"].keys())\n\n    def get_varinfo(self, var: str) -&gt; dict:\n        \"\"\"Get metadata for a variable from varinfo.\n\n        Args:\n            var (str): Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n        Returns:\n            dict: Metadata dictionary containing cf_name, long_name, units, etc.\n\n        Raises:\n            ValueError: If varinfo is not loaded or variable not found.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            raise ValueError(\"Configuration or varinfo not loaded\")\n\n        if var not in self.cfg.varinfo:\n            raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n        return self.cfg.varinfo[var]\n\n\n    def get_actions(self) -&gt; dict:\n        \"\"\"Return a dictionary of workflow actions with their outputs and descriptions.\n\n        Supports ``actionsinfo`` in mapping style or list style and returns a consistent mapping of action name to description/output.\n\n        Returns:\n            dict: Mapping action name -&gt; {'output': ..., 'description': ...}\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n            raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n        actions_map = getattr(self.cfg, \"actionsinfo\")\n\n        # If 'actions' key exists, fallback to list style\n        if \"actions\" in actions_map:\n            actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                        for a in actions_map[\"actions\"]}\n\n        return actions_map\n    def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n        \"\"\"Fetch climate extreme indices from ``cfg.extinfo`` that involve the given variables.\n\n        Args:\n            variables (list[str]): Variables to filter indices by (if ``None``, uses ``cfg.variables``).\n            require_all (bool): If True, return indices that require all provided variables; otherwise return indices if any variable matches.\n\n        Returns:\n            dict: Mapping index_name -&gt; index_definition.\n        \"\"\"\n        cfg = self.cfg\n        variables = variables or cfg.variables \n        if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n            raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n        indices_def = cfg.extinfo.get(\"indices\", {})\n        if not indices_def:\n            return {}\n\n        matched_indices = {}\n        for idx_name, idx_info in indices_def.items():\n            idx_vars = idx_info.get(\"variables\", [])\n            if require_all:\n                if all(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n            else:\n                if any(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n\n        return matched_indices\n\n    # ----------------------------\n    # Imputation\n    # ----------------------------\n    @update_ds(attr_name='impute_ds')\n    def impute(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"Impute missing values using the configured imputation method.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to impute. If None, uses\n                ``self.current_ds``.\n\n        Returns:\n            xr.Dataset | None: The imputed dataset (also sets\n                ``self.current_ds`` and ``self.impute_ds``). Returns ``None``\n                if no imputation method is configured.\n\n        Raises:\n            ValueError: If ``ds`` is ``None`` and ``self.current_ds`` is not set.\n        \"\"\"\n        cfg = self.cfg\n        impute_cfg = cfg.imputeinfo\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.impute is None:\n            self.logger.warning(\"No imputation method selected.\")\n            return None\n        # select variables (optional)\n        # variables = cfg.get(\"variables\", None)\n        # if variables:\n        #     missing = [v for v in variables if v not in self.current_ds.data_vars]\n        #     if missing:\n        #         raise ValueError(f\"Variables not present in dataset: {missing}\")\n        #     ds_in = self.current_ds[variables]\n        # else:\n        #     ds_in = self.current_ds\n\n        method = cfg.impute\n        normalize = impute_cfg[method].get(\"normalize\", True)\n        time_dim = cfg.dsinfo[cfg.dataset].get(\"time_dim\", \"time\")\n        lat_dim = cfg.dsinfo[cfg.dataset].get(\"lat_dim\", \"lat\")\n        lon_dim = cfg.dsinfo[cfg.dataset].get(\"lon_dim\", \"lon\")\n        # epochs = impute_cfg[method].get(\"epochs\", 300)\n\n        # run imputer (Imputer expects dims (time, lat, lon))\n        imputer = Imputer(\n            ds,\n            time_dim=time_dim,\n            lat_dim=lat_dim,\n            lon_dim=lon_dim,\n            method=method,\n            normalize=normalize,\n        )\n        recovered = imputer.impute()\n\n        # merge imputed variables back into original dataset if we operated on a subset\n\n        ds_out = recovered\n\n        # Return dataset (decorator will set current_ds and impute_ds and generate filenames)\n        return ds_out\n\n    def get_impute_methods(self) -&gt; Dict[str, dict]:\n        \"\"\"Return mapping of available imputation methods from config.\n\n        Returns:\n            Dict[str, dict]: Mapping of method name -&gt; config (empty dict if none configured).\n        \"\"\"\n        if not hasattr(self.cfg, \"imputeinfo\") or not self.cfg.imputeinfo:\n            return {}\n        return dict(self.cfg.imputeinfo)\n\n    def configure_logging(self, level=logging.INFO, handler: logging.Handler = None):\n        \"\"\"Configure logging for this extractor instance.\n\n        Args:\n            level (int, optional): Logging level (default: ``logging.INFO``).\n            handler (logging.Handler, optional): Handler to add; if ``None``, a default StreamHandler is created.\n        \"\"\"\n        if handler is None:\n            handler = logging.StreamHandler()\n            handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n        # Avoid adding duplicate handlers\n        if not any(isinstance(h, handler.__class__) for h in self.logger.handlers):\n            self.logger.addHandler(handler)\n        self.logger.setLevel(level)\n        # also set module logger level\n        logger.setLevel(level)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.__init__","title":"<code>__init__(self, cfg_name='config', conf_path=None, overrides=None)</code>  <code>special</code>","text":"<p>Initialize the workflow manager and load configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_name</code> <code>str</code> <p>Name of the Hydra configuration (default: \"config\").</p> <code>'config'</code> <code>conf_path</code> <code>str</code> <p>Optional config path override.</p> <code>None</code> <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply to the configuration.</p> <code>None</code> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n    \"\"\"Initialize the workflow manager and load configuration.\n\n    Args:\n        cfg_name (str): Name of the Hydra configuration (default: \"config\").\n        conf_path (str, optional): Optional config path override.\n        overrides (list[str], optional): Hydra overrides to apply to the configuration.\n    \"\"\"\n    self.cfg_name = cfg_name\n    self.conf_path = conf_path\n    self.cfg: Optional[DictConfig] = None\n\n    # Stage datasets\n    self.ds = None\n    self.current_ds = None\n    self.index_ds = None\n    self.impute_ds = None\n    self.bias_corrected_ds = None\n\n    # Stage DataFrames\n    self.raw_df = None\n    self.current_df = None\n    self.index_df = None\n    self.impute_df = None\n    self.bias_corrected_df = None\n    self.df = None  # alias for current_df\n\n    # filenames\n    self.filename = None\n    self.filetype = None\n\n    # Automatically load config on init\n    self.load_config(overrides)\n    self.cfg = self.preprocess_aoi(self.cfg)\n\n    # instance logger for this extractor\n    self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.calc_index","title":"<code>calc_index(self, ds=None)</code>","text":"<p>Calculate the configured extreme index using xclim indices.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to operate on. If <code>None</code>, <code>self.current_ds</code> is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The computed index as an xarray Dataset (also sets <code>self.index_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='index_ds')\ndef calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n    \"\"\"Calculate the configured extreme index using xclim indices.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to operate on. If ``None``, ``self.current_ds`` is used.\n\n    Returns:\n        xr.Dataset: The computed index as an xarray Dataset (also sets ``self.index_ds``).\n    \"\"\"\n    cfg = self.cfg\n\n    # Use provided ds or fallback\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    if cfg.index is None:\n        self.logger.info(\"No index selected.\")\n        return None\n\n    if \"time\" in ds.coords:\n        years = pd.to_datetime(ds.time.values).year\n        n_years = len(pd.unique(years))\n        if n_years &lt; 30:\n            warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n    indices = extreme_index(cfg, ds)\n    index_ds = indices.calculate(cfg.index).compute()\n    index_ds = index_ds.to_dataset(name=cfg.index)\n\n    return index_ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.configure_logging","title":"<code>configure_logging(self, level=20, handler=None)</code>","text":"<p>Configure logging for this extractor instance.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (default: <code>logging.INFO</code>).</p> <code>20</code> <code>handler</code> <code>logging.Handler</code> <p>Handler to add; if <code>None</code>, a default StreamHandler is created.</p> <code>None</code> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def configure_logging(self, level=logging.INFO, handler: logging.Handler = None):\n    \"\"\"Configure logging for this extractor instance.\n\n    Args:\n        level (int, optional): Logging level (default: ``logging.INFO``).\n        handler (logging.Handler, optional): Handler to add; if ``None``, a default StreamHandler is created.\n    \"\"\"\n    if handler is None:\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n    # Avoid adding duplicate handlers\n    if not any(isinstance(h, handler.__class__) for h in self.logger.handlers):\n        self.logger.addHandler(handler)\n    self.logger.setLevel(level)\n    # also set module logger level\n    logger.setLevel(level)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.extract","title":"<code>extract(self)</code>","text":"<p>Extract data from the configured provider using <code>self.cfg</code>.</p> <p>Uses provider-specific classes (e.g., <code>CMIP</code>, <code>DWD</code>, <code>MSWX</code>, <code>HYRAS</code>, <code>POWER</code>) to fetch, load and extract datasets. When extraction completes, units are converted to those declared in <code>cfg.varinfo</code>, the dataset is computed, and filenames are generated from the configuration.</p> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The extracted and computed dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef extract(self) -&gt; xr.Dataset:\n    \"\"\"Extract data from the configured provider using ``self.cfg``.\n\n    Uses provider-specific classes (e.g., ``CMIP``, ``DWD``, ``MSWX``, ``HYRAS``, ``POWER``)\n    to fetch, load and extract datasets. When extraction completes, units are converted to those declared in ``cfg.varinfo``, the dataset is computed, and filenames are generated from the configuration.\n\n    Returns:\n        xr.Dataset: The extracted and computed dataset (also sets ``self.current_ds``).\n    \"\"\"\n    cfg = self.cfg\n    extract_kwargs = {}\n\n    if cfg.lat is not None and cfg.lon is not None:\n        extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n        if cfg.dataset == \"dwd\":\n            extract_kwargs[\"buffer_km\"] = 30\n    elif cfg.region is not None:\n        extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n    elif cfg.shapefile is not None:\n        extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n    ds = None\n    dataset_upper = cfg.dataset.upper()\n\n    if dataset_upper == \"MSWX\":\n        ds_vars = []\n        for var in cfg.variables:\n            mswx = climdata.MSWX(cfg)\n            mswx.extract(**extract_kwargs)\n            mswx.load(var)\n            ds_vars.append(mswx.dataset)\n        ds = xr.merge(ds_vars)\n        self.dataset_class = mswx\n    elif dataset_upper == \"CMIP\":\n        cmip = climdata.CMIP(cfg)\n        cmip.fetch()\n        cmip.load()\n        cmip.extract(**extract_kwargs)\n        ds = cmip.ds\n        self.dataset_class = cmip\n    elif dataset_upper == \"POWER\":\n        power = climdata.POWER(cfg)\n        power.fetch()\n        power.load()\n        ds = power.ds\n        self.dataset_class = power\n    elif dataset_upper == \"DWD\":\n        ds_vars = []\n        for var in cfg.variables:\n            dwd = climdata.DWD(cfg)\n            ds_var = dwd.extract(variable=var, **extract_kwargs)\n            ds_vars.append(ds_var)\n        ds = xr.merge(ds_vars)\n        self.dataset_class = dwd\n    elif dataset_upper == \"HYRAS\":\n        hyras = climdata.HYRAS(cfg)\n        ds_vars = []\n        for var in cfg.variables:\n            hyras.extract(**extract_kwargs)\n            ds_vars.append(hyras.load(var, chunking={'time':\"auto\"})[[var]])\n        ds = xr.merge(ds_vars, compat=\"override\")\n        self.dataset_class = hyras\n    elif dataset_upper == \"W5E5\":\n        w5e5 = climdata.W5E5(cfg)\n        w5e5.fetch()  # Download from ISIMIP\n        w5e5.load()   # Load into xarray\n        w5e5.extract(**extract_kwargs)\n        ds = w5e5.ds\n        self.dataset_class = w5e5\n    for var in ds.data_vars:\n        ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n    # ds = ds.compute()\n\n    return ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_actions","title":"<code>get_actions(self)</code>","text":"<p>Return a dictionary of workflow actions with their outputs and descriptions.</p> <p>Supports <code>actionsinfo</code> in mapping style or list style and returns a consistent mapping of action name to description/output.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping action name -&gt; {'output': ..., 'description': ...}</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_actions(self) -&gt; dict:\n    \"\"\"Return a dictionary of workflow actions with their outputs and descriptions.\n\n    Supports ``actionsinfo`` in mapping style or list style and returns a consistent mapping of action name to description/output.\n\n    Returns:\n        dict: Mapping action name -&gt; {'output': ..., 'description': ...}\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n        raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n    actions_map = getattr(self.cfg, \"actionsinfo\")\n\n    # If 'actions' key exists, fallback to list style\n    if \"actions\" in actions_map:\n        actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                    for a in actions_map[\"actions\"]}\n\n    return actions_map\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_datasets","title":"<code>get_datasets(self)</code>","text":"<p>Return the list of dataset provider names available in configuration.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Names of available dataset providers from <code>cfg.dsinfo</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_datasets(self) -&gt; List[str]:\n    \"\"\"Return the list of dataset provider names available in configuration.\n\n    Returns:\n        List[str]: Names of available dataset providers from ``cfg.dsinfo``.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n    return list(self.cfg.dsinfo.keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_impute_methods","title":"<code>get_impute_methods(self)</code>","text":"<p>Return mapping of available imputation methods from config.</p> <p>Returns:</p> Type Description <code>Dict[str, dict]</code> <p>Mapping of method name -&gt; config (empty dict if none configured).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_impute_methods(self) -&gt; Dict[str, dict]:\n    \"\"\"Return mapping of available imputation methods from config.\n\n    Returns:\n        Dict[str, dict]: Mapping of method name -&gt; config (empty dict if none configured).\n    \"\"\"\n    if not hasattr(self.cfg, \"imputeinfo\") or not self.cfg.imputeinfo:\n        return {}\n    return dict(self.cfg.imputeinfo)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_indices","title":"<code>get_indices(self, variables, require_all=True)</code>","text":"<p>Fetch climate extreme indices from <code>cfg.extinfo</code> that involve the given variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>Variables to filter indices by (if <code>None</code>, uses <code>cfg.variables</code>).</p> required <code>require_all</code> <code>bool</code> <p>If True, return indices that require all provided variables; otherwise return indices if any variable matches.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping index_name -&gt; index_definition.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n    \"\"\"Fetch climate extreme indices from ``cfg.extinfo`` that involve the given variables.\n\n    Args:\n        variables (list[str]): Variables to filter indices by (if ``None``, uses ``cfg.variables``).\n        require_all (bool): If True, return indices that require all provided variables; otherwise return indices if any variable matches.\n\n    Returns:\n        dict: Mapping index_name -&gt; index_definition.\n    \"\"\"\n    cfg = self.cfg\n    variables = variables or cfg.variables \n    if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n        raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n    indices_def = cfg.extinfo.get(\"indices\", {})\n    if not indices_def:\n        return {}\n\n    matched_indices = {}\n    for idx_name, idx_info in indices_def.items():\n        idx_vars = idx_info.get(\"variables\", [])\n        if require_all:\n            if all(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n        else:\n            if any(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n\n    return matched_indices\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_variables","title":"<code>get_variables(self, dataset=None)</code>","text":"<p>Return the list of variables available for a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Dataset name to query. Defaults to <code>cfg.dataset</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of variable names.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n    \"\"\"Return the list of variables available for a dataset.\n\n    Args:\n        dataset (str, optional): Dataset name to query. Defaults to ``cfg.dataset``.\n\n    Returns:\n        List[str]: List of variable names.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n\n    dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n    if dataset_name is None:\n        raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n    dsinfo = self.cfg.dsinfo.get(dataset_name)\n    if not dsinfo or \"variables\" not in dsinfo:\n        raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n    return list(dsinfo[\"variables\"].keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo","title":"<code>get_varinfo(self, var)</code>","text":"<p>Get metadata for a variable from varinfo.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>Name of the variable, e.g., 'tas', 'tasmax', 'pr'.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary containing cf_name, long_name, units, etc.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If varinfo is not loaded or variable not found.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_varinfo(self, var: str) -&gt; dict:\n    \"\"\"Get metadata for a variable from varinfo.\n\n    Args:\n        var (str): Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n    Returns:\n        dict: Metadata dictionary containing cf_name, long_name, units, etc.\n\n    Raises:\n        ValueError: If varinfo is not loaded or variable not found.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        raise ValueError(\"Configuration or varinfo not loaded\")\n\n    if var not in self.cfg.varinfo:\n        raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n    return self.cfg.varinfo[var]\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.impute","title":"<code>impute(self, ds=None)</code>","text":"<p>Impute missing values using the configured imputation method.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to impute. If None, uses <code>self.current_ds</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset | None</code> <p>The imputed dataset (also sets     <code>self.current_ds</code> and <code>self.impute_ds</code>). Returns <code>None</code>     if no imputation method is configured.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>ds</code> is <code>None</code> and <code>self.current_ds</code> is not set.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='impute_ds')\ndef impute(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n    \"\"\"Impute missing values using the configured imputation method.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to impute. If None, uses\n            ``self.current_ds``.\n\n    Returns:\n        xr.Dataset | None: The imputed dataset (also sets\n            ``self.current_ds`` and ``self.impute_ds``). Returns ``None``\n            if no imputation method is configured.\n\n    Raises:\n        ValueError: If ``ds`` is ``None`` and ``self.current_ds`` is not set.\n    \"\"\"\n    cfg = self.cfg\n    impute_cfg = cfg.imputeinfo\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    if cfg.impute is None:\n        self.logger.warning(\"No imputation method selected.\")\n        return None\n    # select variables (optional)\n    # variables = cfg.get(\"variables\", None)\n    # if variables:\n    #     missing = [v for v in variables if v not in self.current_ds.data_vars]\n    #     if missing:\n    #         raise ValueError(f\"Variables not present in dataset: {missing}\")\n    #     ds_in = self.current_ds[variables]\n    # else:\n    #     ds_in = self.current_ds\n\n    method = cfg.impute\n    normalize = impute_cfg[method].get(\"normalize\", True)\n    time_dim = cfg.dsinfo[cfg.dataset].get(\"time_dim\", \"time\")\n    lat_dim = cfg.dsinfo[cfg.dataset].get(\"lat_dim\", \"lat\")\n    lon_dim = cfg.dsinfo[cfg.dataset].get(\"lon_dim\", \"lon\")\n    # epochs = impute_cfg[method].get(\"epochs\", 300)\n\n    # run imputer (Imputer expects dims (time, lat, lon))\n    imputer = Imputer(\n        ds,\n        time_dim=time_dim,\n        lat_dim=lat_dim,\n        lon_dim=lon_dim,\n        method=method,\n        normalize=normalize,\n    )\n    recovered = imputer.impute()\n\n    # merge imputed variables back into original dataset if we operated on a subset\n\n    ds_out = recovered\n\n    # Return dataset (decorator will set current_ds and impute_ds and generate filenames)\n    return ds_out\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.load_config","title":"<code>load_config(self, overrides=None)</code>","text":"<p>Load and compose the Hydra configuration.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply when composing the configuration.</p> <code>None</code> <p>Returns:</p> Type Description <code>DictConfig</code> <p>Composed Hydra configuration object stored on <code>self.cfg</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n    \"\"\"Load and compose the Hydra configuration.\n\n    Args:\n        overrides (list[str], optional): Hydra overrides to apply when composing the configuration.\n\n    Returns:\n        DictConfig: Composed Hydra configuration object stored on ``self.cfg``.\n    \"\"\"\n    overrides = overrides or []\n    conf_dir = _ensure_local_conf()\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    if not GlobalHydra.instance().is_initialized():\n        hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        hydra_ctx = None\n\n    if hydra_ctx:\n        with hydra_ctx:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    else:\n        self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    return self.cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.preprocess_aoi","title":"<code>preprocess_aoi(self, cfg)</code>","text":"<p>Process an 'aoi' specification in the configuration.</p> <p>Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration object with optional <code>aoi</code> entry.</p> required <p>Returns:</p> Type Description <code>DictConfig</code> <p>The modified configuration. When a Point is provided, <code>cfg.lat</code> and <code>cfg.lon</code> are set; when a Polygon is provided, <code>cfg.bounds</code> is set and <code>cfg.region</code> is set to \"custom\".</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"Process an 'aoi' specification in the configuration.\n\n    Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).\n\n    Args:\n        cfg (DictConfig): Configuration object with optional ``aoi`` entry.\n\n    Returns:\n        DictConfig: The modified configuration. When a Point is provided, ``cfg.lat`` and ``cfg.lon`` are set; when a Polygon is provided, ``cfg.bounds`` is set and ``cfg.region`` is set to \"custom\".\n    \"\"\"\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid AOI JSON string\")\n\n    aoi = cfg.aoi\n\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n    if isinstance(geom, Point):\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                 \"lon_min\": minx, \"lon_max\": maxx}}\n        cfg.region = \"custom\"\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n    return cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.run_workflow","title":"<code>run_workflow(self, overrides=None, actions=None, file=None)</code>","text":"<p>Execute a sequence of workflow actions.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply (not all actions will use these).</p> <code>None</code> <code>actions</code> <code>list[str]</code> <p>Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.</p> <code>None</code> <code>file</code> <code>str</code> <p>File path used for upload actions when required.</p> <code>None</code> <p>Returns:</p> Type Description <code>WorkflowResult</code> <p>Named result container with populated fields for dataframe/dataset/filenames.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def run_workflow(self, overrides: Optional[List[str]] = None,\n                 actions: Optional[List[str]] = None,\n                 file: Optional[str] = None) -&gt; WorkflowResult:\n    \"\"\"Execute a sequence of workflow actions.\n\n    Args:\n        overrides (list[str], optional): Hydra overrides to apply (not all actions will use these).\n        actions (list[str], optional): Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.\n        file (str, optional): File path used for upload actions when required.\n\n    Returns:\n        WorkflowResult: Named result container with populated fields for dataframe/dataset/filenames.\n    \"\"\"\n    actions = actions or [\"extract\", \"calc_index\", \"to_csv\", \"to_nc\"]\n    result = WorkflowResult(cfg=self.cfg)\n    for action in actions:\n        self.logger.info(\"Starting action: %s\", action)\n        try:\n            if action == \"upload_netcdf\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                        \"but none was provided.\"\n                    )\n                    # Validate extension\n                valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_netcdf: '{file}'. \"\n                        f\"Expected one of: {valid_nc_ext}\"\n                    )\n                self.upload_netcdf(file)\n                result.dataset = self.current_ds\n\n            elif action == \"upload_csv\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_csv' requires argument 'csv_file', \"\n                        \"but none was provided.\"\n                    )\n\n                # Validate CSV extension\n                valid_csv_ext = (\".csv\", \".csv.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_csv: '{file}'. \"\n                        f\"Expected one of: {valid_csv_ext}\"\n                    )\n\n                self.upload_csv(file)\n                result.dataset = self.current_ds\n\n            elif action == \"extract\":\n                if self.cfg.dataset is None:\n                    raise ValueError(\n                        \"Action 'extract' cannot run because no dataset provider is set \"\n                        \"(cfg.dataset is None).\"\n                    )\n                self.extract()\n                result.dataset = self.current_ds\n\n            elif action == \"calc_index\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before computing an index.\"\n                    )\n                self.calc_index()\n                result.index_ds = self.current_ds\n\n            elif action == \"to_csv\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before converting to a DataFrame.\"\n                    )\n                self.to_dataframe()\n                result.dataframe = self.current_df\n                result.filename = self.to_csv()\n\n            elif action == \"to_nc\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before saving to NetCDF.\"\n                    )\n                result.filename = self.to_nc()\n\n            elif action == \"impute\":\n                if self.current_ds is None:\n                    raise ValueError(\"Action 'impute' requires a dataset, but no dataset is available.\")\n                self.impute()\n                result.dataset = self.current_ds\n                result.impute_ds = getattr(self, \"impute_ds\", None)\n\n            else:\n                raise ValueError(f\"Unknown action '{action}'\")\n            self.logger.info(\"Completed action: %s\", action)\n        except Exception:\n            self.logger.exception(\"Action '%s' failed\", action)\n            raise\n\n    return result\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_csv","title":"<code>to_csv(self, df=None, filename=None)</code>","text":"<p>Save a DataFrame to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to save. Defaults to <code>self.current_df</code>.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Output filename. Defaults to <code>self.filename_csv</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the written CSV file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"Save a DataFrame to CSV.\n\n    Args:\n        df (pd.DataFrame, optional): DataFrame to save. Defaults to ``self.current_df``.\n        filename (str, optional): Output filename. Defaults to ``self.filename_csv``.\n\n    Returns:\n        str: The path of the written CSV file.\n    \"\"\"\n    df = df if df is not None else self.current_df\n\n    filename = filename or getattr(self, \"filename_csv\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_csv is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    df.to_csv(filename, index=False)\n    self.filename_csv = str(path)\n    self.current_filename = str(path)\n\n    # print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n    self.logger.info(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n    return filename\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_dataframe","title":"<code>to_dataframe(self, ds=None)</code>","text":"<p>Convert a dataset to a long-form pandas DataFrame.</p> <p>The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to convert. If <code>None</code>, uses <code>self.current_ds</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Long-form DataFrame (also sets <code>self.current_df</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_df()\ndef to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n    \"\"\"Convert a dataset to a long-form pandas DataFrame.\n\n    The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to convert. If ``None``, uses ``self.current_ds``.\n\n    Returns:\n        pd.DataFrame: Long-form DataFrame (also sets ``self.current_df``).\n    \"\"\"\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    df = ds.to_dataframe().reset_index()\n\n    id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n    value_vars = [v for v in ds.data_vars if v in df.columns]\n\n    if not value_vars:\n        raise ValueError(\"No variables in dataset available to melt into long format\")\n\n    df_long = df.melt(\n        id_vars=id_vars,\n        value_vars=value_vars,\n        var_name=\"variable\",\n        value_name=\"value\"\n    )\n\n    df_long[\"units\"] = df_long[\"variable\"].apply(\n        lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n    )\n    if getattr(self.cfg, \"dataset\") == 'cmip':\n        df_long[\"source_id\"] = getattr(self.cfg, \"source_id\")\n    df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n    df_long = df_long.drop_duplicates()\n    self._gen_fn_cfg()\n    return df_long\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_nc","title":"<code>to_nc(self, ds=None, filename=None)</code>","text":"<p>Save an xarray Dataset to NetCDF.</p> <p>Notes</p> <ul> <li>If <code>ds</code> is <code>None</code>: save <code>current_ds</code>.</li> <li>If <code>filename</code> is <code>None</code>: use <code>self.filename_nc</code>.</li> <li>Creates directories if needed and updates <code>self.filename_nc</code> and <code>self.current_filename</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to save. If <code>None</code>, uses <code>self.current_ds</code>.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Output filename. Defaults to <code>self.filename_nc</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the written NetCDF file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"Save an xarray Dataset to NetCDF.\n\n    Notes:\n        - If ``ds`` is ``None``: save ``current_ds``.\n        - If ``filename`` is ``None``: use ``self.filename_nc``.\n        - Creates directories if needed and updates ``self.filename_nc`` and ``self.current_filename``.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to save. If ``None``, uses ``self.current_ds``.\n        filename (str, optional): Output filename. Defaults to ``self.filename_nc``.\n\n    Returns:\n        str: The path of the written NetCDF file.\n    \"\"\"\n\n    # -------------------------------\n    # 1. Determine dataset to save\n    # -------------------------------\n    ds = ds or getattr(self, \"current_ds\", None)\n    if ds is None:\n        raise ValueError(\"No dataset available to save\")\n\n    # -------------------------------\n    # 2. Determine filename\n    # -------------------------------\n    filename = filename or getattr(self, \"filename_nc\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_nc is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------\n    # 3. Save to NetCDF\n    # -------------------------------\n    ds.to_netcdf(path)\n\n    # -------------------------------\n    # 4. Track filenames\n    # -------------------------------\n    self.filename_nc = str(path)\n    self.current_filename = str(path)\n\n    # print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n    self.logger.info(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n    return str(path)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.upload_csv","title":"<code>upload_csv(self, csv_file)</code>","text":"<p>Load a long-form CSV into an xarray.Dataset.</p> <p>The CSV must contain <code>time</code> and <code>lat</code>/<code>latitude</code>, <code>lon</code>/<code>longitude</code>, <code>variable</code>, <code>value</code>. Units may be supplied in a <code>units</code> column and an optional <code>source</code> column is recognized.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>Path to the CSV file to load.</p> required <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The converted dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n    \"\"\"Load a long-form CSV into an xarray.Dataset.\n\n    The CSV must contain ``time`` and ``lat``/``latitude``, ``lon``/``longitude``, ``variable``, ``value``. Units may be supplied in a ``units`` column and an optional ``source`` column is recognized.\n\n    Args:\n        csv_file (str): Path to the CSV file to load.\n\n    Returns:\n        xr.Dataset: The converted dataset (also sets ``self.current_ds``).\n    \"\"\"\n    if not os.path.exists(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n    df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n    lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n    lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n    if lat_col is None or lon_col is None:\n        raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n    id_vars = [\"time\", lat_col, lon_col]\n    df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n    ds = df_wide.set_index(id_vars).to_xarray()\n\n    # Attach units from CSV\n    for var in ds.data_vars:\n        units_series = df[df[\"variable\"] == var][\"units\"]\n        ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n    # Global source attribute\n    if \"source\" in df.columns:\n        source_series = df[\"source\"].dropna().unique()\n        if len(source_series) &gt; 0:\n            ds.attrs[\"source\"] = source_series[0]\n\n    # Update cfg variables &amp; varinfo\n    if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n        self.cfg.variables = list(ds.data_vars)\n    if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n    self._gen_fn(ds)\n    return ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.upload_netcdf","title":"<code>upload_netcdf(self, nc_file)</code>","text":"<p>Load a NetCDF file into an xarray.Dataset and update file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>nc_file</code> <code>str</code> <p>Path to the NetCDF file to open.</p> required <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The loaded dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n    \"\"\"Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n    Args:\n        nc_file (str): Path to the NetCDF file to open.\n\n    Returns:\n        xr.Dataset: The loaded dataset (also sets ``self.current_ds``).\n    \"\"\"\n    if not os.path.exists(nc_file):\n        raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n    ds = xr.open_dataset(nc_file)\n\n    # Update cfg variables &amp; varinfo\n    if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n        self.cfg.variables = list(ds.data_vars)\n    if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                            for v in ds.data_vars}\n    self._gen_fn(ds)\n    return ds\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"Common Concepts in climdata","text":"<p>This page describes common terminology, configuration patterns, and reusable components in the <code>climdata</code> package.</p>"},{"location":"common/#configuration-files","title":"Configuration Files","text":"<ul> <li>All configuration is managed via Hydra and YAML files in the <code>conf/</code> directory.</li> <li>See <code>config.yaml</code> for the main entry point.</li> </ul>"},{"location":"common/#standard-variable-names","title":"Standard Variable Names","text":"<ul> <li>Variables follow CF conventions (see <code>variables.yaml</code>).</li> <li>Example: <code>tas</code> for air temperature, <code>pr</code> for precipitation.</li> </ul>"},{"location":"common/#output-schema","title":"Output Schema","text":"<p>All outputs are standardized to the following columns:</p> Column Description latitude Latitude of observation/grid longitude Longitude of observation/grid time Timestamp source Data source/provider variable Variable name value Observed or modeled value units Units of measurement"},{"location":"common/#regions-and-bounds","title":"Regions and Bounds","text":"<ul> <li>Regions are defined in <code>config.yaml</code> under <code>bounds</code>.</li> <li>Example: <code>europe</code>, <code>global</code>.</li> </ul>"},{"location":"common/#usage-patterns","title":"Usage Patterns","text":"<ul> <li>Use <code>climdata.load_config()</code> to load configuration.</li> <li>Use <code>climdata.DWD(cfg)</code> or <code>climdata.MSWX(cfg)</code> for dataset access.</li> </ul> <p>Add more shared concepts as your documentation grows.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>climdata could always use more documentation, whether as part of the official climdata docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up climdata for local development.</p> <ol> <li> <p>Fork the climdata repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/climdata.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv climdata\n$ cd climdata/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 climdata tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/Kaushikreddym/climdata/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>It's recommended to create and activate a conda environment first, then install via pip:</p> <pre><code># create &amp; activate conda environment (recommended)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install climdata from PyPI\npip install climdata\n</code></pre> <p>This is the preferred method to install climdata, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install climdata from sources, create/activate a conda environment and then install from the repository:</p> <pre><code># create &amp; activate conda environment (optional)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install from GitHub (editable install if desired)\npip install git+https://github.com/Kaushikreddym/climdata\n# or for editable development install:\n# git clone https://github.com/Kaushikreddym/climdata\n# cd climdata\n# pip install -e .\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Quick examples to get started with the ClimData workflow utilities.</p>"},{"location":"usage/#quickstart","title":"Quickstart","text":"<p>Install into a conda env (recommended) and then pip: <pre><code>conda create -n climdata python=3.11 -y\nconda activate climdata\npip install climdata\n# or from source:\n# pip install -e .\n</code></pre></p>"},{"location":"usage/#minimal-example","title":"Minimal example","text":"<pre><code>from climdata import ClimData\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Extract dataset (updates extractor.current_ds)\nds = extractor.extract()\n\n# Compute configured extreme index (updates extractor.index_ds)\nindex_ds = extractor.calc_index(ds)\n\n# Convert to long-form DataFrame (updates extractor.current_df)\ndf = extractor.to_dataframe(index_ds)\n\n# Save DataFrame to CSV\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"usage/#single-call-workflow","title":"Single-call workflow","text":"<p>Use the high-level runner to chain common steps: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n# result contains produced dataset/dataframe and filenames\nprint(result.dataframe.head())\nprint(\"Saved to:\", result.filename)\n</code></pre></p>"},{"location":"usage/#uploading-existing-files","title":"Uploading existing files","text":"<ul> <li>Load NetCDF: extractor.upload_netcdf(\"path/to/file.nc\")</li> <li>Load long-form CSV: extractor.upload_csv(\"path/to/file.csv\")</li> </ul>"},{"location":"usage/#introspection-helpers","title":"Introspection helpers","text":"<ul> <li>extractor.get_datasets()</li> <li>extractor.get_variables(dataset_name)</li> <li>extractor.get_varinfo(varname)</li> <li>extractor.get_actions()</li> </ul>"},{"location":"usage/#notes","title":"Notes","text":"<ul> <li>See <code>docs/index.md</code> for installation details and full examples.</li> <li>For provider-specific options (MSWX, CMIP, POWER, DWD, HYRAS) consult the configuration files under <code>conf/</code> and the API docs.</li> </ul>"},{"location":"examples/climdata_cli/","title":"Climdata cli","text":"In\u00a0[\u00a0]: Copied! <pre>import climdata\nimport xarray as xr\nimport xclim\nimport pandas as pd\nfrom climdata.utils.config import _ensure_local_conf\nfrom omegaconf import DictConfig\nimport hydra\nfrom hydra.core.global_hydra import GlobalHydra\nimport sys\n</pre> import climdata import xarray as xr import xclim import pandas as pd from climdata.utils.config import _ensure_local_conf from omegaconf import DictConfig import hydra from hydra.core.global_hydra import GlobalHydra import sys <p>Example usage:</p> <p>Run the CLI with overrides:</p> <p>python climdata_cli.py  dataset=mswx  lat=52.507  lon=13.137  time_range.start_date=2000-01-01  time_range.end_date=2000-12-31  dsinfo.mswx.params.google_service_account=/home/muduchuru/.climdata_conf/service.json  data_dir=/beegfs/muduchuru/data/  variables=['tas']</p> <p>All Hydra overrides follow the format key=value.</p> In\u00a0[\u00a0]: Copied! <pre>_ensure_local_conf()\n@hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    overrides = sys.argv[1:]\n\n    # Extract data\n    cfg, filename, ds = climdata.extract_data(overrides=overrides)\n</pre> _ensure_local_conf() @hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None) def main(cfg: DictConfig) -&gt; None:     overrides = sys.argv[1:]      # Extract data     cfg, filename, ds = climdata.extract_data(overrides=overrides) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"examples/climdata_cli/#uncomment-the-below-snippet-for-parallel-processing","title":"uncomment the below snippet for parallel processing\u00b6","text":"<p>import dask from dask.distributed import Client</p>"},{"location":"examples/climdata_cli/#configure-dask","title":"Configure Dask\u00b6","text":"<p>client = Client( n_workers=20,        # or match number of physical cores threads_per_worker=2, memory_limit=\"10GB\"  # per worker (8 * 10GB = 80GB total) ) from multiprocessing import freeze_support</p>"},{"location":"examples/extremes/","title":"Extremes","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2014_v6-0_de.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) Out[\u00a0]: <pre>&lt;xarray.Dataset&gt; Size: 158kB\nDimensions:      (time: 9496, dayofyear: 366)\nCoordinates:\n  * time         (time) datetime64[ns] 76kB 1989-01-01 1989-01-02 ... 2014-12-31\n    lon          float64 8B 13.25\n    lat          float64 8B 52.9\n    x            float32 4B 4.54e+06\n    y            float32 4B 3.314e+06\n    percentiles  int64 8B 10\n  * dayofyear    (dayofyear) int64 3kB 1 2 3 4 5 6 7 ... 361 362 363 364 365 366\nData variables:\n    tasmin       (time) float64 76kB -0.5 3.2 -1.5 -3.1 ... -9.0 -7.9 -4.2 0.9\n    tasmin_per   (dayofyear) float64 3kB -11.02 -10.47 -10.85 ... -10.89 -11.02\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_1989_v6-0_de.nc</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 9496</li><li>dayofyear: 366</li></ul></li><li>Coordinates: (7)<ul><li>time(time)datetime64[ns]1989-01-01 ... 2014-12-31standard_name :timelong_name :Mid Of Twentyfour Hour Time Interval [UTC]bounds :time_bndsaxis :T<pre>array(['1989-01-01T00:00:00.000000000', '1989-01-02T00:00:00.000000000',\n       '1989-01-03T00:00:00.000000000', ..., '2014-12-29T00:00:00.000000000',\n       '2014-12-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lon()float6413.25standard_name :longitudelong_name :Longitude Of Cell Centerunits :degrees_east_CoordinateAxisType :Lon<pre>array(13.24777272)</pre></li><li>lat()float6452.9standard_name :latitudelong_name :Latitude Of Cell Centerunits :degrees_north_CoordinateAxisType :Lat<pre>array(52.89523559)</pre></li><li>x()float324.54e+06standard_name :projection_x_coordinatelong_name :X Coordinate Of Projection for Cell Centerunits :maxis :Xbounds :x_bnds<pre>array(4539500., dtype=float32)</pre></li><li>y()float323.314e+06standard_name :projection_y_coordinatelong_name :Y Coordinate Of Projection for Cell Centerunits :maxis :Ybounds :y_bnds<pre>array(3314500., dtype=float32)</pre></li><li>percentiles()int6410<pre>array(10)</pre></li><li>dayofyear(dayofyear)int641 2 3 4 5 6 ... 362 363 364 365 366<pre>array([  1,   2,   3, ..., 364, 365, 366])</pre></li></ul></li><li>Data variables: (2)<ul><li>tasmin(time)float64-0.5 3.2 -1.5 ... -7.9 -4.2 0.9standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]<pre>array([-0.50000001,  3.20000005, -1.50000002, ..., -7.90000012,\n       -4.20000006,  0.90000001])</pre></li><li>tasmin_per(dayofyear)float64-11.02 -10.47 ... -10.89 -11.02standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]climatology_bounds :['1989-01-01', '2014-12-31']window :5alpha :0.3333333333333333beta :0.3333333333333333history :[2025-11-27 15:54:07] per: percentile_doy(arr=tasmin, window=5, per=10, alpha=0.3333333333333333, beta=0.3333333333333333, copy=True) - xclim version: 0.56.0<pre>array([-11.01666683, -10.47482207, -10.8512513 , -10.99879468,\n       -11.00000016, -11.00000016, -10.85574445,  -9.87251156,\n        -9.50774443,  -8.09875811,  -6.32885854,  -5.71747954,\n        -5.82250237,  -5.7045115 ,  -6.25775352,  -6.490959  ,\n        -6.5000001 ,  -6.84641106,  -7.30381746,  -7.96494989,\n        -9.24452069, -10.47275815, -11.27005496, -12.8002285 ,\n       -13.1709317 , -12.97885864, -12.59801845, -10.42470335,\n        -9.61626498,  -9.56333348, -10.19662116, -10.68646591,\n       -10.72666683, -10.90858464, -11.56750702, -10.92812802,\n       -10.40264856, -10.6019545 , -10.11934262,  -8.50294077,\n        -8.5363015 ,  -8.56333346,  -8.56333346,  -7.57527865,\n        -6.84530604,  -6.38050238,  -6.15187224,  -6.12666676,\n        -6.61591791,  -6.20229233,  -6.12666676,  -6.35320557,\n        -7.24753435,  -7.28172614,  -7.73480377,  -8.27246588,\n        -8.01470332,  -7.30920559,  -6.61525124,  -5.34973524,\n        -5.12666674,  -5.02116902,  -5.738822  ,  -6.2512969 ,\n        -6.32666676,  -6.32666676,  -6.2229042 ,  -5.48697725,\n        -5.16392702,  -5.12666674,  -5.18593615,  -5.14093158,\n        -4.54869413,  -3.68133339,  -3.09871237,  -2.36369867,\n        -2.51671237,  -3.46794526,  -4.09315075,  -4.20000006,\n...\n        -0.5400822 ,  -0.77536987,  -1.10000002,  -1.14054796,\n        -1.42533335,  -1.80305026,  -1.30000002,  -1.30000002,\n        -1.30000002,  -1.30000002,  -1.37158906,  -1.69000003,\n        -1.69000003,  -1.6742192 ,  -1.61093153,  -1.63389044,\n        -1.50838358,  -1.60000002,  -1.61023747,  -1.63578998,\n        -1.47854797,  -1.36808221,  -1.05027399,  -0.68567124,\n        -0.24356165,  -0.51424658,  -0.74857535,  -1.68205482,\n        -1.88412788,  -2.51039273,  -3.06333338,  -3.04610963,\n        -2.92666671,  -2.98894982,  -3.50459366,  -3.99056627,\n        -4.28574436,  -5.06488592,  -5.91528776,  -6.10202749,\n        -5.76193616,  -4.61417358,  -4.49648409,  -4.4633334 ,\n        -4.46664847,  -4.53419185,  -4.9701188 ,  -5.88401835,\n        -6.35995443,  -5.31666675,  -5.31740647,  -5.32666675,\n        -5.36890419,  -5.99966219,  -6.87729691,  -8.05017364,\n        -8.42510515,  -8.55182661,  -8.283233  ,  -7.00666677,\n        -7.02032887,  -7.30000011,  -7.28794531,  -7.00244759,\n        -6.90117819,  -7.05763481,  -8.03612797,  -7.0433791 ,\n        -6.69986311,  -7.10490422,  -7.77000012,  -7.78200012,\n        -8.50945218,  -9.20581749, -10.63634719, -10.99941569,\n       -10.8936714 , -11.01666683])</pre></li></ul></li><li>Indexes: (2)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1989-01-01', '1989-01-02', '1989-01-03', '1989-01-04',\n               '1989-01-05', '1989-01-06', '1989-01-07', '1989-01-08',\n               '1989-01-09', '1989-01-10',\n               ...\n               '2014-12-22', '2014-12-23', '2014-12-24', '2014-12-25',\n               '2014-12-26', '2014-12-27', '2014-12-28', '2014-12-29',\n               '2014-12-30', '2014-12-31'],\n              dtype='datetime64[ns]', name='time', length=9496, freq=None))</pre></li><li>dayofyearPandasIndex<pre>PandasIndex(Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       357, 358, 359, 360, 361, 362, 363, 364, 365, 366],\n      dtype='int64', name='dayofyear', length=366))</pre></li></ul></li><li>Attributes: (21)source :surface observationsinstitution :Deutscher Wetterdienst (DWD)Conventions :CF-1.11title :gridded_temperature_dataset_(HYRAS-DE TASMIN)realization :v6-0project_id :HYRASlevel_type :surfacefrequency :dayhorizontal_resolution :1_kmreferences :https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_minauthor :Climate Monitoring (KU21)contact :klimaanalyse@dwd.decreation_date :created at 2024-09-01 11:11:16variable_id :tasminunique_dataset_id :DWD_HYRAS_tasmin_v6-0_1989_day_0066D4B2A8license :The HYRAS data, produced by DWD, is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0; (see https://creativecommons.org/licenses/).ConventionsURL :http://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.htmlrealm :atmosproduct :observationsinput_data_status :checkedfilename :tasmin_hyras_1_1989_v6-0_de.nc</li></ul> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin','tas']\",\n            f\"time_range.start_date=2013-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin','tas']\",             f\"time_range.start_date=2013-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n</pre> In\u00a0[6]: Copied! <pre>import json\nimport climdata\n# geojson = {\n#   \"type\": \"FeatureCollection\",\n#   \"features\": [\n#     {\n#       \"type\": \"Feature\",\n#       \"properties\": {},\n#       \"geometry\": {\n#         \"coordinates\": [\n#           13.246667038198012,\n#           52.891982026993958\n#         ],\n#         \"type\": \"Point\"\n#       }\n#     }\n#   ]\n# }\n\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json import climdata # geojson = { #   \"type\": \"FeatureCollection\", #   \"features\": [ #     { #       \"type\": \"Feature\", #       \"properties\": {}, #       \"geometry\": { #         \"coordinates\": [ #           13.246667038198012, #           52.891982026993958 #         ], #         \"type\": \"Point\" #       } #     } #   ] # }  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"data_dir=/beegfs/muduchuru/data\", #optional              \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\nSaved index to: cmip_tn10p_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n</pre>"},{"location":"examples/w5e5_example/","title":"W5e5 example","text":"In\u00a0[4]: Copied! <pre>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\nfrom climdata import ClimData\n# Configure\noverrides = [\n    \"dataset=w5e5\",  # Select the MSWX dataset for extraction\n    \"lat=52\",\n    \"lon=13\",\n    f\"time_range.start_date=2004-01-01\",  # Start date of extraction\n    f\"time_range.end_date=2004-12-31\",    # End date of extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Fetch, load, and extract\nw5e5 = W5E5(extractor.cfg)\nw5e5.fetch()  # Download from ISIMIP\nw5e5.load()   # Load into xarray\nw5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))\n</pre> from climdata.utils.config import load_config from climdata.datasets.W5E5 import W5E5 from climdata import ClimData # Configure overrides = [     \"dataset=w5e5\",  # Select the MSWX dataset for extraction     \"lat=52\",     \"lon=13\",     f\"time_range.start_date=2004-01-01\",  # Start date of extraction     f\"time_range.end_date=2004-12-31\",    # End date of extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ]  extractor = ClimData(overrides=overrides)  # Fetch, load, and extract w5e5 = W5E5(extractor.cfg) w5e5.fetch()  # Download from ISIMIP w5e5.load()   # Load into xarray w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))  <pre>\ud83d\udd0d Searching for W5E5 datasets in ISIMIP repository...\n\n\ud83d\udce5 Fetching tasmin...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_tasmin_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_tasmin_global_daily_2001_2010.nc\n\n\ud83d\udce5 Fetching tasmax...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_tasmax_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_tasmax_global_daily_2001_2010.nc\n\n\ud83d\udce5 Fetching pr...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_pr_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_pr_global_daily_2001_2010.nc\n\n\u2705 Downloaded 3 files\n\ud83d\udcc2 Loading 3 W5E5 files...\n  Loading tasmin from 1 file(s)...\n  Loading tasmax from 1 file(s)...\n  Loading pr from 1 file(s)...\n\u2705 Loaded dataset with 3 variables\n</pre> In\u00a0[5]: Copied! <pre>w5e5.ds\n</pre> w5e5.ds Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 7kB\nDimensions:  (time: 366)\nCoordinates:\n    lon      float64 8B 13.25\n    lat      float64 8B 52.25\n  * time     (time) datetime64[ns] 3kB 2004-01-01 2004-01-02 ... 2004-12-31\nData variables:\n    tasmin   (time) float32 1kB ...\n    tasmax   (time) float32 1kB ...\n    pr       (time) float32 1kB ...\nAttributes:\n    title:        20CRv3-W5E5 observational climate input data for ISIMIP3a\n    institution:  Potsdam Institute for Climate Impact Research (PIK)\n    project:      Inter-Sectoral Impact Model Intercomparison Project phase 3...\n    contact:      ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https...\n    summary:      20CRv3, member 001, bias-adjusted to W5E5 v2.0 with ISIMIP3...\n    references:   Slivinski et al. (2019) &lt;https://doi.org/10.1002/qj.3598&gt; a...\n    source:       W5E5 via ISIMIP\n    dataset:      W5E5v2.0\n    description:  WFDE5 over land merged with ERA5 over ocean</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 366</li></ul></li><li>Coordinates: (3)<ul><li>lon()float6413.25standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(13.25)</pre></li><li>lat()float6452.25standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.25)</pre></li><li>time(time)datetime64[ns]2004-01-01 ... 2004-12-31standard_name :timelong_name :Timeaxis :T<pre>array(['2004-01-01T00:00:00.000000000', '2004-01-02T00:00:00.000000000',\n       '2004-01-03T00:00:00.000000000', ..., '2004-12-29T00:00:00.000000000',\n       '2004-12-30T00:00:00.000000000', '2004-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (3)<ul><li>tasmin(time)float32...standard_name :air_temperaturelong_name :Daily Minimum Near-Surface Air Temperatureunits :K<pre>[366 values with dtype=float32]</pre></li><li>tasmax(time)float32...standard_name :air_temperaturelong_name :Daily Maximum Near-Surface Air Temperatureunits :K<pre>[366 values with dtype=float32]</pre></li><li>pr(time)float32...standard_name :precipitation_fluxlong_name :Precipitationunits :kg m-2 s-1<pre>[366 values with dtype=float32]</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2004-01-01', '2004-01-02', '2004-01-03', '2004-01-04',\n               '2004-01-05', '2004-01-06', '2004-01-07', '2004-01-08',\n               '2004-01-09', '2004-01-10',\n               ...\n               '2004-12-22', '2004-12-23', '2004-12-24', '2004-12-25',\n               '2004-12-26', '2004-12-27', '2004-12-28', '2004-12-29',\n               '2004-12-30', '2004-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))</pre></li></ul></li><li>Attributes: (9)title :20CRv3-W5E5 observational climate input data for ISIMIP3ainstitution :Potsdam Institute for Climate Impact Research (PIK)project :Inter-Sectoral Impact Model Intercomparison Project phase 3a (ISIMIP3a)contact :ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https://www.isimip.org&gt;summary :20CRv3, member 001, bias-adjusted to W5E5 v2.0 with ISIMIP3BASD v2.5.1 (for 1901-1978) combined with W5E5 v2.0 (for 1979-2019)references :Slivinski et al. (2019) &lt;https://doi.org/10.1002/qj.3598&gt; and Compo et al. (2011) &lt;https://doi.org/10.1002/qj.776&gt; for 20CRv3; Cucchi et al. (2020) &lt;https://doi.org/10.5194/essd-2020-28&gt; and Lange et al. (2021) &lt;https://doi.org/10.48364/ISIMIP.342217&gt; for W5E5; Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; and Lange (2021) &lt;https://doi.org/10.5281/zenodo.5776126&gt; for ISIMIP3BASDsource :W5E5 via ISIMIPdataset :W5E5v2.0description :WFDE5 over land merged with ERA5 over ocean</li></ul>"},{"location":"examples/wrapper/","title":"Wrapper","text":"In\u00a0[1]: Copied! <pre>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s:%(name)s:%(message)s\"\n)\n</pre> import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s:%(name)s:%(message)s\" )  In\u00a0[\u00a0]: Copied! <pre>from climdata import ClimData\noverrides = [\n    \"dataset=hyras\",  # Choose the MSWX dataset for extraction\n    \"lat=52\",\n    \"lon=13\",\n    f\"time_range.start_date=2024-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2024-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr,'tas']\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n]\nextractor = ClimData(overrides=overrides)\nds = extractor.extract()\n</pre> from climdata import ClimData overrides = [     \"dataset=hyras\",  # Choose the MSWX dataset for extraction     \"lat=52\",     \"lon=13\",     f\"time_range.start_date=2024-01-01\",  # Start date for data extraction     f\"time_range.end_date=2024-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr,'tas']\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files ] extractor = ClimData(overrides=overrides) ds = extractor.extract() <pre>INFO:numexpr.utils:Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\nINFO:numexpr.utils:Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\nINFO:numexpr.utils:NumExpr defaulting to 16 threads.\nWARNING:pint.util:Redefining 'percent' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '%' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'year' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'yr' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'C' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'd' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'h' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_north' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_east' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '[speed]' (&lt;class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'&gt;)\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2024_v6-0_de.nc\n</pre> <pre>&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2024_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2024_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2024_v6-0_de.nc\n</pre> In\u00a0[2]: Copied! <pre>import climdata\nfrom hydra import initialize, compose\nfrom omegaconf import OmegaConf\nclimdata.utils.config._ensure_local_conf()\noverrides = None\nwith initialize(config_path='conf', version_base=None):\n    cfg = compose(config_name='config', overrides=overrides or [])\nextractor_CMIP = climdata.CMIP(cfg)\n</pre> import climdata from hydra import initialize, compose from omegaconf import OmegaConf climdata.utils.config._ensure_local_conf() overrides = None with initialize(config_path='conf', version_base=None):     cfg = compose(config_name='config', overrides=overrides or []) extractor_CMIP = climdata.CMIP(cfg) <pre>INFO:numexpr.utils:Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\nINFO:numexpr.utils:Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\nINFO:numexpr.utils:NumExpr defaulting to 16 threads.\nWARNING:pint.util:Redefining 'percent' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '%' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'year' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'yr' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'C' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'd' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'h' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_north' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_east' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '[speed]' (&lt;class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'&gt;)\n</pre> In\u00a0[11]: Copied! <pre>print(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('ssp245'))\nprint(extractor_CMIP.get_variables(experiment_id='ssp119',source_id='CAMS-CSM1-0'))\n</pre> print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('ssp245')) print(extractor_CMIP.get_variables(experiment_id='ssp119',source_id='CAMS-CSM1-0')) <pre>['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>INFO:climdata.datasets.CMIPCloud:46 models found for experiment 'ssp245'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n['pr', 'tas', 'tasmax']\n</pre> In\u00a0[1]: Copied! <pre>import json\n\ngeojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}\n\nimport climdata\nmswx = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n)\n</pre> import json  geojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}  import climdata mswx = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] ) <pre>Saved NetCDF to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n</pre> In\u00a0[34]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name='tasmin'\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name='tasmin' data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  In\u00a0[22]: Copied! <pre>import climdata\nhyras = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"table_id=day\",\n            \"data_dir=./data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",\n            \"region=custom\",\n    ]\n)\n</pre> import climdata hyras = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"table_id=day\",             \"data_dir=./data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",             \"region=custom\",     ] ) <pre>../../examples/conf\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2020_v6-0_de.nc\n\u2714\ufe0f  Exists locally: ./data/hyras/TASMIN/tasmin_hyras_1_2020_v6-0_de.nc\n\ud83d\udce6 Extracted curvilinear box with shape: FrozenMappingWarningOnValuesAccess({'time': 366, 'bnds': 2, 'x': 145, 'y': 110})\n\u2705 Saved output to hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\n</pre> In\u00a0[\u00a0]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name = list(ds.data_vars)[0]  # replace with actual variable if needed\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name = list(ds.data_vars)[0]  # replace with actual variable if needed data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  <pre>&lt;xarray.Dataset&gt; Size: 47MB\nDimensions:  (time: 366, y: 110, x: 145)\nCoordinates:\n  * time     (time) datetime64[ns] 3kB 2020-01-01T12:00:00 ... 2020-12-31T12:...\n    lon      (y, x) float64 128kB ...\n    lat      (y, x) float64 128kB ...\n  * x        (x) float32 580B 4.178e+06 4.178e+06 ... 4.32e+06 4.322e+06\n  * y        (y) float32 440B 2.99e+06 2.99e+06 ... 3.098e+06 3.098e+06\nData variables:\n    tasmin   (time, y, x) float64 47MB ...\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_2020_v6-0_de.nc\n</pre>"},{"location":"examples/wrapper_workflow/","title":"Workflow Notebook","text":"In\u00a0[1]: Copied! <pre>from climdata import ClimData\nimport pandas as pd\nimport xarray as xr\n\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s | %(message)s\",\n    force=True,\n)\n</pre> from climdata import ClimData import pandas as pd import xarray as xr  import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s | %(message)s\",     force=True, ) In\u00a0[2]: Copied! <pre>extractor = ClimData()\ndatasets = extractor.get_datasets()\nprint(datasets)\n</pre> extractor = ClimData() datasets = extractor.get_datasets() print(datasets) <pre>['dwd', 'mswx', 'hyras', 'cmip', 'power', 'w5e5']\n</pre> In\u00a0[3]: Copied! <pre>variables = extractor.get_variables('w5e5')\nprint(variables)\n\n# for CMIP\nimport climdata\nextractor_CMIP = climdata.CMIP(extractor.cfg)\nprint(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('ssp245'))\nprint(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))\n</pre> variables = extractor.get_variables('w5e5') print(variables)  # for CMIP import climdata extractor_CMIP = climdata.CMIP(extractor.cfg) print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('ssp245')) print(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))  <pre>['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | 46 models found for experiment 'ssp245'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n</pre> In\u00a0[4]: Copied! <pre>variables = extractor.get_variables('w5e5')\nprint(variables)\nprint(\"*\"*70)\nvarinfo = extractor.get_varinfo('tasmax')\nprint(varinfo)\n</pre> variables = extractor.get_variables('w5e5') print(variables) print(\"*\"*70) varinfo = extractor.get_varinfo('tasmax') print(varinfo) <pre>['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n**********************************************************************\n{'cf_name': 'air_temperature', 'long_name': 'Daily maximum near-surface air temperature', 'units': 'degC'}\n</pre> In\u00a0[5]: Copied! <pre>actions = extractor.get_actions()\nprint(actions.keys())\n</pre> actions = extractor.get_actions() print(actions.keys()) <pre>dict_keys(['extract', 'calc_index', 'impute', 'to_nc', 'to_csv', 'upload_netcdf', 'upload_csv'])\n</pre> In\u00a0[6]: Copied! <pre>indices = extractor.get_indices(['tasmin', 'tasmax'])\nprint(indices.keys())\n\nimpute_methods = extractor.get_impute_methods()\nprint(impute_methods.keys())\n</pre> indices = extractor.get_indices(['tasmin', 'tasmax']) print(indices.keys())  impute_methods = extractor.get_impute_methods() print(impute_methods.keys()) <pre>dict_keys(['heat_wave_index', 'heat_wave_frequency', 'heat_wave_max_length', 'heat_wave_total_length', 'hot_spell_frequency', 'hot_spell_max_length', 'hot_spell_total_length', 'hot_spell_max_magnitude', 'ice_days', 'isothermality', 'maximum_consecutive_frost_days', 'maximum_consecutive_frost_free_days', 'maximum_consecutive_tx_days'])\ndict_keys(['BRITS', 'XGBOOST', 'CDRec', 'SoftImpute'])\n</pre> In\u00a0[7]: Copied! <pre>import json\n\n# -----------------------------\n# Step 1: Define the area of interest (AOI)\n# -----------------------------\n# The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude].\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          24.246667038198012,  # longitude\n          12.891982026993958   # latitude\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\n# -----------------------------\n# Step 2: Define configuration overrides\n# -----------------------------\n# Overrides are strings used by Hydra to modify default configurations at runtime.\noverrides = [\n    \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above\n    f\"time_range.start_date=2004-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n    \"index=tn10p\",  # Climate extreme index to calculate\n    \"impute=BRITS\"\n]\n\n# -----------------------------\n# Step 3: Define the workflow sequence\n# -----------------------------\nseq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]\n\n# -----------------------------\n# Step 4: Initialize the ClimData extractor\n# -----------------------------\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Step 5: Run the Multi-Step workflow\n# -----------------------------\nresult = extractor.run_workflow(\n    actions=seq,\n)\n</pre> import json  # ----------------------------- # Step 1: Define the area of interest (AOI) # ----------------------------- # The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude]. geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           24.246667038198012,  # longitude           12.891982026993958   # latitude         ],         \"type\": \"Point\"       }     }   ] }   # ----------------------------- # Step 2: Define configuration overrides # ----------------------------- # Overrides are strings used by Hydra to modify default configurations at runtime. overrides = [     \"dataset=cmip\",  # Choose the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above     f\"time_range.start_date=2004-01-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download     \"index=tn10p\",  # Climate extreme index to calculate     \"impute=BRITS\" ]  # ----------------------------- # Step 3: Define the workflow sequence # ----------------------------- seq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]  # ----------------------------- # Step 4: Initialize the ClimData extractor # ----------------------------- extractor = ClimData(overrides=overrides)  # ----------------------------- # Step 5: Run the Multi-Step workflow # ----------------------------- result = extractor.run_workflow(     actions=seq, ) <pre>INFO | Starting action: extract\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | Completed action: extract\nINFO | Starting action: impute\nINFO | No missing data found. Imputation not required.\nINFO | Completed action: impute\nINFO | Starting action: calc_index\n/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:618: UserWarning: Index tn10p usually requires \u226530 years, got 11\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\nINFO | Completed action: calc_index\nINFO | Starting action: to_nc\n&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\nINFO | Dataset saved to NetCDF file: cmip_tn10p_LAT12.891982026993958_LON24.246667038198012_2004-01-01_2014-12-31.nc\nINFO | Completed action: to_nc\n</pre> In\u00a0[8]: Copied! <pre>import json\n\n# -----------------------------\n# Define the area of interest (AOI)\n# -----------------------------\n# This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012\ngeojson = {\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"properties\": {},\n            \"geometry\": {\n                \"coordinates\": [24.246667038198012, 12.891982026993958],\n                \"type\": \"Point\"\n            }\n        }\n    ]\n}\n\n# -----------------------------\n# Define configuration overrides\n# -----------------------------\n# These strings override the default hydra config at runtime\noverrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    # Optional Google service account if needed for MSWX access\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    \"index=tn10p\",  # Extreme climate index to calculate\n]\n\n# -----------------------------\n# Initialize the ClimData extractor\n# -----------------------------\n# This loads the configuration with overrides and prepares the object\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Extract climate data\n# -----------------------------\n# Returns an xarray.Dataset for the selected variables, AOI, and time range\nds = extractor.extract()\n\n# -----------------------------\n# Compute the climate index\n# -----------------------------\n# Takes the extracted dataset and calculates the extreme index \"tn10p\"\n# Returns a new xarray.Dataset containing only the index\nds_index = extractor.calc_index(ds)\n\n# -----------------------------\n# Convert the index dataset to a long-form pandas DataFrame\n# -----------------------------\n# Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\")\ndf_index = extractor.to_dataframe(ds_index)\n\n# -----------------------------\n# Save the DataFrame to CSV\n# -----------------------------\n# This will write the index values to \"index.csv\" in the current working directory\nextractor.to_csv(df_index, filename=\"index.csv\")\n</pre> import json  # ----------------------------- # Define the area of interest (AOI) # ----------------------------- # This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012 geojson = {     \"type\": \"FeatureCollection\",     \"features\": [         {             \"type\": \"Feature\",             \"properties\": {},             \"geometry\": {                 \"coordinates\": [24.246667038198012, 12.891982026993958],                 \"type\": \"Point\"             }         }     ] }  # ----------------------------- # Define configuration overrides # ----------------------------- # These strings override the default hydra config at runtime overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     # Optional Google service account if needed for MSWX access     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     \"index=tn10p\",  # Extreme climate index to calculate ]  # ----------------------------- # Initialize the ClimData extractor # ----------------------------- # This loads the configuration with overrides and prepares the object extractor = ClimData(overrides=overrides)  # ----------------------------- # Extract climate data # ----------------------------- # Returns an xarray.Dataset for the selected variables, AOI, and time range ds = extractor.extract()  # ----------------------------- # Compute the climate index # ----------------------------- # Takes the extracted dataset and calculates the extreme index \"tn10p\" # Returns a new xarray.Dataset containing only the index ds_index = extractor.calc_index(ds)  # ----------------------------- # Convert the index dataset to a long-form pandas DataFrame # ----------------------------- # Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\") df_index = extractor.to_dataframe(ds_index)  # ----------------------------- # Save the DataFrame to CSV # ----------------------------- # This will write the index values to \"index.csv\" in the current working directory extractor.to_csv(df_index, filename=\"index.csv\")  <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n\u2705 All 31 pr files already exist locally.\n</pre> <pre>/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:618: UserWarning: Index tn10p usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\nINFO | DataFrame saved to CSV file: index.csv\n</pre> Out[8]: <pre>'index.csv'</pre> In\u00a0[9]: Copied! <pre>print(extractor.current_filename)\n# print(extractor_point.filename_nc)\n</pre> print(extractor.current_filename) # print(extractor_point.filename_nc) <pre>index.csv\n</pre> In\u00a0[10]: Copied! <pre>box_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\n\nextractor_box = ClimData(overrides=box_overrides)\nresult_box = extractor_box.run_workflow(actions=[\"extract\", \"to_dataframe\"])\n</pre> box_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ]  extractor_box = ClimData(overrides=box_overrides) result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_dataframe\"])  <pre>INFO | Starting action: extract\n</pre> <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> <pre>INFO | Completed action: extract\nINFO | Starting action: to_dataframe\nERROR | Action 'to_dataframe' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 848, in run_workflow\n    raise ValueError(f\"Unknown action '{action}'\")\nValueError: Unknown action 'to_dataframe'\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 11\n      1 box_overrides = [\n      2     \"dataset=mswx\",  # Select the MSWX dataset for extraction\n      3     \"region=europe\", # Select the region\n   (...)\n      7     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n      8 ]\n     10 extractor_box = ClimData(overrides=box_overrides)\n---&gt; 11 result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_dataframe\"])\n\nFile /beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:848, in ClimateExtractor.run_workflow(self, overrides, actions, file)\n    845         result.impute_ds = getattr(self, \"impute_ds\", None)\n    847     else:\n--&gt; 848         raise ValueError(f\"Unknown action '{action}'\")\n    849     self.logger.info(\"Completed action: %s\", action)\n    850 except Exception:\n\nValueError: Unknown action 'to_dataframe'</pre> In\u00a0[\u00a0]: Copied! <pre>result_box.dataframe\n</pre> result_box.dataframe Out[\u00a0]: time lat lon variable value units source 0 2014-12-01 34.049999 0.050000 tasmin 8.3750 \u00b0C mswx 1 2014-12-01 34.049999 0.150006 tasmin 7.7500 \u00b0C mswx 2 2014-12-01 34.049999 0.249997 tasmin 7.5000 \u00b0C mswx 3 2014-12-01 34.049999 0.350003 tasmin 7.3125 \u00b0C mswx 4 2014-12-01 34.049999 0.450009 tasmin 7.0625 \u00b0C mswx ... ... ... ... ... ... ... ... 10322995 2014-12-31 70.949997 44.549999 tasmax -3.8125 \u00b0C mswx 10322996 2014-12-31 70.949997 44.650005 tasmax -3.7500 \u00b0C mswx 10322997 2014-12-31 70.949997 44.749996 tasmax -3.8125 \u00b0C mswx 10322998 2014-12-31 70.949997 44.850002 tasmax -3.8125 \u00b0C mswx 10322999 2014-12-31 70.949997 44.950008 tasmax -3.7500 \u00b0C mswx <p>10323000 rows \u00d7 7 columns</p> In\u00a0[\u00a0]: Copied! <pre>lat_berlin, lon_berlin = [52.5,13.4]\nidx_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"lat={lat_berlin}\", # Select the region\n    f\"lon={lon_berlin}\",\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    \"index=heat_wave_max_length\"\n]\n\n\nextractor_idx = ClimData(overrides=idx_overrides)\nresult_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\"])\nresult_idx.dataframe.head()\n</pre> lat_berlin, lon_berlin = [52.5,13.4] idx_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"lat={lat_berlin}\", # Select the region     f\"lon={lon_berlin}\",     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     \"index=heat_wave_max_length\" ]   extractor_idx = ClimData(overrides=idx_overrides) result_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\"]) result_idx.dataframe.head() <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> <pre>/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:610: UserWarning: Index heat_wave_max_length usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n</pre> Out[\u00a0]: time lat lon variable value units source 0 2014-01-01 52.549999 13.350003 heat_wave_max_length 0.0 d mswx In\u00a0[\u00a0]: Copied! <pre>try:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"calc_index\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"to_csv\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"upload_netcdf\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n</pre> try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"calc_index\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"to_csv\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"upload_netcdf\"]) except Exception as e:     print(\"Error:\", e) <pre>INFO | Starting action: calc_index\nERROR | Action 'calc_index' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 760, in run_workflow\n    raise ValueError(\nValueError: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n</pre> <pre>Error: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n</pre> <pre>INFO | Starting action: to_csv\nERROR | Action 'to_csv' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 778, in run_workflow\n    raise ValueError(\nValueError: Action 'to_csv' requires a DataFrame, but no DataFrame is available. Use 'to_dataframe' or upload a CSV before saving.\n</pre> <pre>Error: Action 'to_csv' requires a DataFrame, but no DataFrame is available. Use 'to_dataframe' or upload a CSV before saving.\n</pre> <pre>INFO | Starting action: upload_netcdf\nERROR | Action 'upload_netcdf' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 717, in run_workflow\n    raise ValueError(\nValueError: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n</pre> <pre>Error: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/wrapper_workflow/#climdata-tutorial","title":"ClimData Tutorial\u00b6","text":"<p>This notebook demonstrates usage of the <code>ClimData</code> class for climate data extraction, extreme index computation, and workflow management. Includes examples for point-based and box-based extraction, variable exploration, and error handling.</p>"},{"location":"examples/wrapper_workflow/#1-imports","title":"1\ufe0f\u20e3 Imports\u00b6","text":""},{"location":"examples/wrapper_workflow/#2-explore-available-datasets","title":"2\ufe0f\u20e3 Explore available datasets\u00b6","text":""},{"location":"examples/wrapper_workflow/#3-explore-variables-for-a-dataset","title":"3\ufe0f\u20e3 Explore variables for a dataset\u00b6","text":""},{"location":"examples/wrapper_workflow/#4-explore-metadata-for-a-variable","title":"4\ufe0f\u20e3 Explore metadata for a variable\u00b6","text":""},{"location":"examples/wrapper_workflow/#5-explore-available-workflow-actions","title":"5\ufe0f\u20e3 Explore available workflow actions\u00b6","text":""},{"location":"examples/wrapper_workflow/#6-point-extraction-workflow","title":"6\ufe0f\u20e3 Point extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#output-filenames","title":"Output filenames\u00b6","text":""},{"location":"examples/wrapper_workflow/#7-box-extraction-workflow","title":"7\ufe0f\u20e3 Box extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#8-compute-extreme-index-only","title":"8\ufe0f\u20e3 Compute extreme index only\u00b6","text":""},{"location":"examples/wrapper_workflow/#9-error-examples","title":"9\ufe0f\u20e3 Error examples\u00b6","text":""},{"location":"examples/advanced/cmip_obs_comparison/","title":"CMIP validation","text":"In\u00a0[1]: Copied! <pre>import climdata\nimport pandas as pd\nimport xarray as xr\nimport cftime\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s | %(message)s\",\n    force=True,\n)\n\nextractor = climdata.ClimData()\n\nextractor_CMIP = climdata.CMIP(extractor.cfg)\nprint(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('historical'))\nprint(extractor_CMIP.get_variables(experiment_id='historical',source_id=['GISS-E2-1-H','CanESM5','GFDL-ESM4']))\n</pre> import climdata import pandas as pd import xarray as xr import cftime import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s | %(message)s\",     force=True, )  extractor = climdata.ClimData()  extractor_CMIP = climdata.CMIP(extractor.cfg) print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('historical')) print(extractor_CMIP.get_variables(experiment_id='historical',source_id=['GISS-E2-1-H','CanESM5','GFDL-ESM4']))  <pre>['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | 65 models found for experiment 'historical'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-FV2', 'CESM2-WACCM', 'CESM2-WACCM-FV2', 'CIESM', 'CMCC-CM2-HR4', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-0', 'E3SM-1-1', 'E3SM-1-1-ECA', 'EC-Earth3', 'EC-Earth3-AerChem', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'EC-Earth3P-VHR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-G-CC', 'GISS-E2-1-H', 'GISS-E2-2-H', 'HadGEM3-GC31-LL', 'HadGEM3-GC31-MM', 'ICON-ESM-LR', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM5A2-INCA', 'IPSL-CM6A-LR', 'IPSL-CM6A-LR-INCA', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2H', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM-1-2-HAM', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorCPM1', 'NorESM2-LM', 'NorESM2-MM', 'SAM0-UNICON', 'TaiESM1', 'UKESM1-0-LL']\n['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n</pre> In\u00a0[2]: Copied! <pre>from climdata import ClimData\noverrides = [\n    \"dataset=hyras\",\n    \"lat=52\",\n    \"lon=13\",\n    # \"region=germany\",\n    f\"time_range.start_date=1989-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr,tas]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n]\nextractor = ClimData(overrides=overrides)\nds_obs = extractor.extract()\ndf_obs = extractor.to_dataframe()\n</pre> from climdata import ClimData overrides = [     \"dataset=hyras\",     \"lat=52\",     \"lon=13\",     # \"region=germany\",     f\"time_range.start_date=1989-01-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr,tas]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files ] extractor = ClimData(overrides=overrides) ds_obs = extractor.extract() df_obs = extractor.to_dataframe() <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2014_v6-0_de.nc\n</pre> <pre>&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2014_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2014_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2014_v6-0_de.nc\n</pre> In\u00a0[3]: Copied! <pre>ds_cmip = []\ndf_cmip = []\nimport cftime\nimport xarray as xr\n\nfor source_id in ['ACCESS-CM2','CanESM5','GFDL-ESM4']:\n    overrides = [\n        \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n        \"lat=52\",\n        \"lon=13\",\n        f\"time_range.start_date=1989-01-01\",  # Start date for data extraction\n        f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n        f\"experiment_id=historical\",\n        f\"source_id='{source_id}'\",\n        f\"table_id=Amon\",\n        \"variables=['tasmin','tasmax','pr','tas']\",       # Variables to extract: min/max temp and precipitation\n        \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    ]\n    extractor = ClimData(overrides=overrides)\n    ds = extractor.extract()\n    df_cmip.append(extractor.to_dataframe())\ndf_cmip = pd.concat(df_cmip,axis=0)\n</pre> ds_cmip = [] df_cmip = [] import cftime import xarray as xr  for source_id in ['ACCESS-CM2','CanESM5','GFDL-ESM4']:     overrides = [         \"dataset=cmip\",  # Choose the MSWX dataset for extraction         \"lat=52\",         \"lon=13\",         f\"time_range.start_date=1989-01-01\",  # Start date for data extraction         f\"time_range.end_date=2014-12-31\",    # End date for data extraction         f\"experiment_id=historical\",         f\"source_id='{source_id}'\",         f\"table_id=Amon\",         \"variables=['tasmin','tasmax','pr','tas']\",       # Variables to extract: min/max temp and precipitation         \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     ]     extractor = ClimData(overrides=overrides)     ds = extractor.extract()     df_cmip.append(extractor.to_dataframe()) df_cmip = pd.concat(df_cmip,axis=0) <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n</pre> In\u00a0[4]: Copied! <pre>import pandas as pd\nimport numpy as np\ndef cftime_to_datetime(df, time_col='time'):\n    \"\"\"\n    Convert cftime.Datetime* to pd.Timestamp where possible.\n    \"\"\"\n    import cftime\n    times = df[time_col].values\n    new_times = []\n    for t in times:\n        if isinstance(t, (cftime.DatetimeNoLeap, cftime.DatetimeJulian, cftime.DatetimeProlepticGregorian)):\n            # convert to pandas Timestamp; keep year, month, day\n            # ignore hour/min/sec for safety\n            new_times.append(pd.Timestamp(f\"{t.year:04d}-{t.month:02d}-{t.day:02d}\"))\n        else:\n            new_times.append(pd.Timestamp(t))\n    df[time_col] = new_times\n    return df\ndef compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None):\n    \"\"\"\n    Compute RMSE, CORR, MAPE, MNB for CMIP models vs observations with optional time resampling.\n    \n    Parameters\n    ----------\n    df_cmip : pd.DataFrame\n        Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source_id','source']\n    df_obs : pd.DataFrame\n        Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source']\n    variables : list or None\n        List of variable names to compute metrics. If None, all variables in df_obs are used.\n    time_resample : str or None\n        Optional pandas resample frequency: 'D' (daily), 'MS' (month start), 'YS' (year start).\n        If None, no temporal aggregation is performed.\n    \n    Returns\n    -------\n    pd.DataFrame\n        Metrics per variable and source_id with columns ['variable','source_id','RMSE','CORR','MAPE','MNB']\n    \"\"\"\n    \n    if variables is None:\n        variables = df_obs['variable'].unique()\n    \n    # Ensure time is datetime\n    df_cmip = cftime_to_datetime(df_cmip, time_col='time')\n    df_obs  = cftime_to_datetime(df_obs, time_col='time')\n        \n    # 1\ufe0f\u20e3 Aggregate spatially: mean over lat/lon\n    df_cmip_mean = (\n        df_cmip.groupby(['time','variable','source_id'])['value']\n        .mean()\n        .reset_index()\n    )\n    df_obs_mean = (\n        df_obs.groupby(['time','variable'])['value']\n        .mean()\n        .reset_index()\n    )\n    \n    # 2\ufe0f\u20e3 Resample in time if requested\n    if time_resample is not None:\n        # CMIP\n        df_cmip_mean = (\n            df_cmip_mean.set_index('time')\n            .groupby(['variable','source_id'])\n            .resample(time_resample)['value']\n            .mean()\n            .reset_index()\n        )\n        # Observations\n        df_obs_mean = (\n            df_obs_mean.set_index('time')\n            .groupby('variable')\n            .resample(time_resample)['value']\n            .mean()\n            .reset_index()\n        )\n    \n    records = []\n    \n    # 3\ufe0f\u20e3 Loop over variables and source_ids\n    for var in variables:\n        o = df_obs_mean[df_obs_mean['variable']==var]\n        \n        for src in df_cmip_mean['source_id'].unique():\n            m = df_cmip_mean[(df_cmip_mean['variable']==var) &amp; (df_cmip_mean['source_id']==src)]\n            \n            # Merge on time\n            df_combined = pd.merge(\n                o[['time','value']], \n                m[['time','value']], \n                on='time', \n                suffixes=('_obs', f'_{src}')\n            )\n            \n            obs_vals = df_combined[f'value_obs'].values\n            mod_vals = df_combined[f'value_{src}'].values\n\n            # Small jitter to avoid divide-by-zero in MAPE\n            epsilon = obs_vals.std() * 0.001\n            obs_safe = obs_vals + epsilon\n\n            rmse = np.sqrt(np.mean((mod_vals - obs_vals)**2))\n            corr = np.corrcoef(obs_vals, mod_vals)[0,1]\n            mape = np.mean(np.abs((mod_vals - obs_safe)/obs_safe)) * 100\n            mnb  = np.mean(mod_vals - obs_vals) / np.mean(obs_safe) * 100  # mean normalized bias\n            \n            records.append({\n                'variable': var,\n                'source_id': src,\n                'Model Mean': mod_vals.mean(),\n                'Observation Mean': obs_vals.mean(),\n                'RMSE': rmse,\n                'CORR': corr,\n                'MAPE': mape,\n                'MNB': mnb\n            })\n    \n    df_metrics = pd.DataFrame(records)\n    return df_metrics\n</pre> import pandas as pd import numpy as np def cftime_to_datetime(df, time_col='time'):     \"\"\"     Convert cftime.Datetime* to pd.Timestamp where possible.     \"\"\"     import cftime     times = df[time_col].values     new_times = []     for t in times:         if isinstance(t, (cftime.DatetimeNoLeap, cftime.DatetimeJulian, cftime.DatetimeProlepticGregorian)):             # convert to pandas Timestamp; keep year, month, day             # ignore hour/min/sec for safety             new_times.append(pd.Timestamp(f\"{t.year:04d}-{t.month:02d}-{t.day:02d}\"))         else:             new_times.append(pd.Timestamp(t))     df[time_col] = new_times     return df def compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None):     \"\"\"     Compute RMSE, CORR, MAPE, MNB for CMIP models vs observations with optional time resampling.          Parameters     ----------     df_cmip : pd.DataFrame         Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source_id','source']     df_obs : pd.DataFrame         Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source']     variables : list or None         List of variable names to compute metrics. If None, all variables in df_obs are used.     time_resample : str or None         Optional pandas resample frequency: 'D' (daily), 'MS' (month start), 'YS' (year start).         If None, no temporal aggregation is performed.          Returns     -------     pd.DataFrame         Metrics per variable and source_id with columns ['variable','source_id','RMSE','CORR','MAPE','MNB']     \"\"\"          if variables is None:         variables = df_obs['variable'].unique()          # Ensure time is datetime     df_cmip = cftime_to_datetime(df_cmip, time_col='time')     df_obs  = cftime_to_datetime(df_obs, time_col='time')              # 1\ufe0f\u20e3 Aggregate spatially: mean over lat/lon     df_cmip_mean = (         df_cmip.groupby(['time','variable','source_id'])['value']         .mean()         .reset_index()     )     df_obs_mean = (         df_obs.groupby(['time','variable'])['value']         .mean()         .reset_index()     )          # 2\ufe0f\u20e3 Resample in time if requested     if time_resample is not None:         # CMIP         df_cmip_mean = (             df_cmip_mean.set_index('time')             .groupby(['variable','source_id'])             .resample(time_resample)['value']             .mean()             .reset_index()         )         # Observations         df_obs_mean = (             df_obs_mean.set_index('time')             .groupby('variable')             .resample(time_resample)['value']             .mean()             .reset_index()         )          records = []          # 3\ufe0f\u20e3 Loop over variables and source_ids     for var in variables:         o = df_obs_mean[df_obs_mean['variable']==var]                  for src in df_cmip_mean['source_id'].unique():             m = df_cmip_mean[(df_cmip_mean['variable']==var) &amp; (df_cmip_mean['source_id']==src)]                          # Merge on time             df_combined = pd.merge(                 o[['time','value']],                  m[['time','value']],                  on='time',                  suffixes=('_obs', f'_{src}')             )                          obs_vals = df_combined[f'value_obs'].values             mod_vals = df_combined[f'value_{src}'].values              # Small jitter to avoid divide-by-zero in MAPE             epsilon = obs_vals.std() * 0.001             obs_safe = obs_vals + epsilon              rmse = np.sqrt(np.mean((mod_vals - obs_vals)**2))             corr = np.corrcoef(obs_vals, mod_vals)[0,1]             mape = np.mean(np.abs((mod_vals - obs_safe)/obs_safe)) * 100             mnb  = np.mean(mod_vals - obs_vals) / np.mean(obs_safe) * 100  # mean normalized bias                          records.append({                 'variable': var,                 'source_id': src,                 'Model Mean': mod_vals.mean(),                 'Observation Mean': obs_vals.mean(),                 'RMSE': rmse,                 'CORR': corr,                 'MAPE': mape,                 'MNB': mnb             })          df_metrics = pd.DataFrame(records)     return df_metrics  In\u00a0[9]: Copied! <pre>from bokeh.models import HoverTool\nimport hvplot.pandas\n\nvar = 'pr'\n# Rename columns to remove spaces\nmetrics_df = compute_cmip_metrics(df_cmip, df_obs, time_resample='MS')\n\nmetrics_df = metrics_df.rename(columns={\n    'Model Mean': 'Model_Mean',\n    'Observation Mean': 'Obs_Mean'\n})\n\n# Update your HoverTool to match\ncustom_hover = HoverTool(\n    tooltips=[\n        (\"Source ID\", \"@source_id\"),\n        (\"Model Mean\", \"@Model_Mean\"),\n        (\"Obs Mean\", \"@Obs_Mean\"), # No braces needed now\n        (\"RMSE\", \"@RMSE\"),\n        (\"Pearson r\", \"@CORR\"),\n    ],\n    mode='vline' \n)\n\n# Update your scatter call\npoints = metrics_df[metrics_df['variable']==var].hvplot.scatter(\n    x='source_id', \n    y='Model_Mean', \n    hover_cols=['RMSE','CORR', 'MAPE', 'MNB', 'Obs_Mean'], # Use new name\n    size=15,\n    tools=[custom_hover]\n)\n\n# 3. Create the box plot (disable its default hover so they don't clash)\nbox_plot = df_cmip[df_cmip['variable']==var].hvplot.box(\n    y='value',\n    by=['source_id'],\n    ylabel=var,\n    hover=False # We want the 'points' hover to take the lead\n)\n\ntop_plot = (box_plot * points).opts(\n    title=f\"Distribution: {var}\",\n    width=800, \n    height=400\n)\n\n# 2. New Time Series Plot (Bottom Panel)\n# We filter df_cmip for the variable, then plot time vs value\nts_plot = df_cmip[df_cmip['variable'] == var].hvplot.line(\n    x='time',      # Ensure this matches your date/time column name\n    y='value',\n    by='source_id',\n    ylabel=var,\n    title=f\"{var} Time Series by Model\",\n    width=800,\n    height=300,\n    alpha=0.6      # Slight transparency helps if lines overlap\n)\n\n# Optional: Add the observation line to the time series for reference\nif 'df_obs' in locals():\n    obs_ts = df_obs[df_obs['variable'] == var].hvplot.line(\n        x='time', \n        y='value', \n        color='black', \n        line_width=2, \n        label='Observations'\n    )\n    ts_plot = ts_plot * obs_ts\n\n# 3. Combine and stack vertically\n# The '+' operator creates a layout, '.cols(1)' forces the vertical stack\nlayout = (top_plot + ts_plot).cols(1)\nlayout\n</pre> from bokeh.models import HoverTool import hvplot.pandas  var = 'pr' # Rename columns to remove spaces metrics_df = compute_cmip_metrics(df_cmip, df_obs, time_resample='MS')  metrics_df = metrics_df.rename(columns={     'Model Mean': 'Model_Mean',     'Observation Mean': 'Obs_Mean' })  # Update your HoverTool to match custom_hover = HoverTool(     tooltips=[         (\"Source ID\", \"@source_id\"),         (\"Model Mean\", \"@Model_Mean\"),         (\"Obs Mean\", \"@Obs_Mean\"), # No braces needed now         (\"RMSE\", \"@RMSE\"),         (\"Pearson r\", \"@CORR\"),     ],     mode='vline'  )  # Update your scatter call points = metrics_df[metrics_df['variable']==var].hvplot.scatter(     x='source_id',      y='Model_Mean',      hover_cols=['RMSE','CORR', 'MAPE', 'MNB', 'Obs_Mean'], # Use new name     size=15,     tools=[custom_hover] )  # 3. Create the box plot (disable its default hover so they don't clash) box_plot = df_cmip[df_cmip['variable']==var].hvplot.box(     y='value',     by=['source_id'],     ylabel=var,     hover=False # We want the 'points' hover to take the lead )  top_plot = (box_plot * points).opts(     title=f\"Distribution: {var}\",     width=800,      height=400 )  # 2. New Time Series Plot (Bottom Panel) # We filter df_cmip for the variable, then plot time vs value ts_plot = df_cmip[df_cmip['variable'] == var].hvplot.line(     x='time',      # Ensure this matches your date/time column name     y='value',     by='source_id',     ylabel=var,     title=f\"{var} Time Series by Model\",     width=800,     height=300,     alpha=0.6      # Slight transparency helps if lines overlap )  # Optional: Add the observation line to the time series for reference if 'df_obs' in locals():     obs_ts = df_obs[df_obs['variable'] == var].hvplot.line(         x='time',          y='value',          color='black',          line_width=2,          label='Observations'     )     ts_plot = ts_plot * obs_ts  # 3. Combine and stack vertically # The '+' operator creates a layout, '.cols(1)' forces the vertical stack layout = (top_plot + ts_plot).cols(1) layout Out[9]: In\u00a0[6]: Copied! <pre>import holoviews as hv\nhv.save(layout, f'distribution_{var}.html')\n</pre> import holoviews as hv hv.save(layout, f'distribution_{var}.html') In\u00a0[8]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom matplotlib.colors import Normalize\nimport matplotlib.cm as cm\n\nrmse = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"RMSE\")\ncorr = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"CORR\")\n\nmodels = rmse.index.tolist()\nvariables = rmse.columns.tolist()\n\n# -------------------------------------------------------\n# Colormap and normalization\n# -------------------------------------------------------\ncmap_rmse = plt.cm.jet\nrmse_norm = Normalize(vmin=0, vmax=10)\ncmap_corr = plt.cm.Blues\ncorr_norm = Normalize(vmin=-1, vmax=1)\n\n# -------------------------------------------------------\n# Plot\n# -------------------------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\nfor i, model in enumerate(models):\n    for j, var in enumerate(variables):\n\n        x0, x1 = j, j + 1\n        y0, y1 = i, i + 1\n\n        # Upper-left triangle (RMSE)\n        tri_rmse = Polygon(\n            [[x0, y1], [x0, y0], [x1, y1]],\n            closed=True,\n            facecolor=cmap_rmse(rmse_norm(rmse.loc[model, var])),\n            edgecolor=\"none\"\n        )\n        ax.add_patch(tri_rmse)\n\n        # Lower-right triangle (CORR)\n        tri_corr = Polygon(\n            [[x1, y0], [x0, y0], [x1, y1]],\n            closed=True,\n            facecolor=cmap_corr(corr_norm(corr.loc[model, var])),\n            edgecolor=\"none\"\n        )\n        ax.add_patch(tri_corr)\n\n# -------------------------------------------------------\n# Axes formatting\n# -------------------------------------------------------\nax.set_xticks(np.arange(len(variables)) + 0.5)\nax.set_xticklabels(variables, fontsize=14)\n\nax.set_yticks(np.arange(len(models)) + 0.5)\nax.set_yticklabels(models, fontsize=14)\n\nax.set_xlim(0, len(variables))\nax.set_ylim(0, len(models))\nax.invert_yaxis()\nax.set_aspect(\"equal\")\n\nax.set_xlabel(\"Variables\", fontsize=16)\nax.set_ylabel(\"\", fontsize=16)\nax.set_title(\"Portrait Plot (RMSE &amp; CORR)\", fontsize=18)\n\nax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n\n# -------------------------------------------------------\n# Colorbars (larger, better spacing)\n# -------------------------------------------------------\ncbar_rmse = fig.colorbar(\n    cm.ScalarMappable(norm=rmse_norm, cmap=cmap_rmse),\n    ax=ax,\n    fraction=0.05,\n    pad=0.02,\n    shrink=0.8,\n    aspect=20\n)\ncbar_rmse.set_label(\"Root Mean Sq Error\", fontsize=14)\ncbar_rmse.ax.tick_params(labelsize=12)\n\ncbar_corr = fig.colorbar(\n    cm.ScalarMappable(norm=corr_norm, cmap=cmap_corr),\n    ax=ax,\n    fraction=0.15,\n    pad=0.02,\n    shrink=0.8,\n    aspect=20\n)\ncbar_corr.set_label(\"Pearson Correlation Coefficient\", fontsize=14)\ncbar_corr.ax.tick_params(labelsize=12)\n\nplt.tight_layout()\nplt.show()\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Polygon from matplotlib.colors import Normalize import matplotlib.cm as cm  rmse = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"RMSE\") corr = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"CORR\")  models = rmse.index.tolist() variables = rmse.columns.tolist()  # ------------------------------------------------------- # Colormap and normalization # ------------------------------------------------------- cmap_rmse = plt.cm.jet rmse_norm = Normalize(vmin=0, vmax=10) cmap_corr = plt.cm.Blues corr_norm = Normalize(vmin=-1, vmax=1)  # ------------------------------------------------------- # Plot # ------------------------------------------------------- fig, ax = plt.subplots(figsize=(12, 8))  for i, model in enumerate(models):     for j, var in enumerate(variables):          x0, x1 = j, j + 1         y0, y1 = i, i + 1          # Upper-left triangle (RMSE)         tri_rmse = Polygon(             [[x0, y1], [x0, y0], [x1, y1]],             closed=True,             facecolor=cmap_rmse(rmse_norm(rmse.loc[model, var])),             edgecolor=\"none\"         )         ax.add_patch(tri_rmse)          # Lower-right triangle (CORR)         tri_corr = Polygon(             [[x1, y0], [x0, y0], [x1, y1]],             closed=True,             facecolor=cmap_corr(corr_norm(corr.loc[model, var])),             edgecolor=\"none\"         )         ax.add_patch(tri_corr)  # ------------------------------------------------------- # Axes formatting # ------------------------------------------------------- ax.set_xticks(np.arange(len(variables)) + 0.5) ax.set_xticklabels(variables, fontsize=14)  ax.set_yticks(np.arange(len(models)) + 0.5) ax.set_yticklabels(models, fontsize=14)  ax.set_xlim(0, len(variables)) ax.set_ylim(0, len(models)) ax.invert_yaxis() ax.set_aspect(\"equal\")  ax.set_xlabel(\"Variables\", fontsize=16) ax.set_ylabel(\"\", fontsize=16) ax.set_title(\"Portrait Plot (RMSE &amp; CORR)\", fontsize=18)  ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)  # ------------------------------------------------------- # Colorbars (larger, better spacing) # ------------------------------------------------------- cbar_rmse = fig.colorbar(     cm.ScalarMappable(norm=rmse_norm, cmap=cmap_rmse),     ax=ax,     fraction=0.05,     pad=0.02,     shrink=0.8,     aspect=20 ) cbar_rmse.set_label(\"Root Mean Sq Error\", fontsize=14) cbar_rmse.ax.tick_params(labelsize=12)  cbar_corr = fig.colorbar(     cm.ScalarMappable(norm=corr_norm, cmap=cmap_corr),     ax=ax,     fraction=0.15,     pad=0.02,     shrink=0.8,     aspect=20 ) cbar_corr.set_label(\"Pearson Correlation Coefficient\", fontsize=14) cbar_corr.ax.tick_params(labelsize=12)  plt.tight_layout() plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/advanced/cmip_obs_comparison/#cmipobservations-comparison-tutorial","title":"CMIP\u2013Observations Comparison (Tutorial)\u00b6","text":"<p>Purpose: Compare CMIP model output with local observations at a single point. This notebook demonstrates extracting observational and CMIP data using <code>climdata</code>, computing standard performance metrics (RMSE, Pearson correlation, MAPE, MNB), and producing visual summaries (boxplots + time series + portrait plot).</p> <p>Prerequisites:</p> <ul> <li><code>climdata</code> package</li> <li>Additional python packages: <code>cftime</code>, <code>hvplot</code>, <code>holoviews</code>, <code>bokeh</code>, <code>matplotlib</code>, <code>numpy</code></li> </ul> <p>How to run:</p> <ol> <li>Run cells top-to-bottom in order. Edit the <code>overrides</code> blocks to change <code>dataset</code>, <code>lat</code>, <code>lon</code>, <code>time_range</code>, <code>variables</code>, and CMIP <code>source_id</code> list.</li> <li>Outputs: an interactive HTML per variable (e.g. <code>distribution_tas.html</code>) and static portrait plot shown inline.</li> </ol> <p>Notes &amp; tips:</p> <ul> <li>The metrics function supports optional time resampling (e.g., <code>time_resample='MS'</code> for monthly means).</li> </ul>"},{"location":"examples/advanced/cmip_obs_comparison/#setup-imports","title":"Setup &amp; imports \ud83d\udd27\u00b6","text":"<p>The first cell imports core packages used in this tutorial. If any imports fail, install requirements via:</p> <pre># plotting support\npip install hvplot holoviews bokeh\n</pre> <p>If you are working on a cluster, ensure the <code>data_dir</code> and any data access permissions are correct before running extraction cells.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#observational-data-extraction-step","title":"Observational data extraction (step)\u00b6","text":"<p>This cell demonstrates extracting local/observational data using <code>ClimData</code> with <code>overrides</code>.</p> <ul> <li><code>dataset</code>: choose dataset (e.g., <code>hyras</code> or <code>mswx</code>)</li> <li><code>lat</code>/<code>lon</code>: location for point extraction</li> <li><code>time_range.start_date</code> and <code>.end_date</code>: extraction period</li> <li><code>variables</code>: list of variable names to extract</li> </ul> <p>After running this cell you should have:</p> <ul> <li><code>ds_obs</code>: an xarray Dataset with observation variables</li> <li><code>df_obs</code>: a Pandas long-form DataFrame (<code>time</code>, <code>variable</code>, <code>value</code>, <code>units</code>, <code>source</code>)</li> </ul> <p>The notebook converts <code>time</code> values to <code>cftime.DatetimeNoLeap</code> where needed to support calendar-aware comparisons.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#cmip-models-extraction-step","title":"CMIP models extraction (step)\u00b6","text":"<p>This loop extracts CMIP model output for a list of <code>source_id</code> values and concatenates models along <code>source_id</code>:</p> <ul> <li><code>dataset=cmip</code>, choose <code>experiment_id</code> (e.g., <code>historical</code>)</li> <li><code>source_id</code>: set a single model name per loop iteration (e.g., <code>CanESM5</code>)</li> <li><code>table_id</code>: typically <code>Amon</code> for monthly output</li> <li>Result variables stored in <code>ds_cmip</code> (xarray, concatenated) and <code>df_cmip</code> (Pandas long-form concatenated DataFrame)</li> </ul> <p>Tip: narrow <code>time_range</code> to speed up downloads and processing during testing.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#compute-cmip-vs-observations-metrics","title":"Compute CMIP-vs-Observations metrics \ud83e\uddee\u00b6","text":"<p>This cell defines <code>compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None)</code> which:</p> <ul> <li>Aggregates spatially (mean over lat/lon), optionally resamples time (e.g., monthly <code>'MS'</code>), and computes:<ul> <li>RMSE (Root Mean Square Error)</li> <li>CORR (Pearson correlation)</li> <li>MAPE (Mean Absolute Percentage Error)</li> <li>MNB (Mean Normalized Bias)</li> </ul> </li> </ul> <p>Output: a DataFrame <code>metrics_df</code> with one row per (<code>variable</code>,<code>source_id</code>) and columns like <code>RMSE</code>, <code>CORR</code>, <code>MAPE</code>, <code>MNB</code>, <code>Model Mean</code>, <code>Observation Mean</code>.</p> <p>Notes:</p> <ul> <li>The function converts <code>cftime</code> dates to <code>pandas.Timestamp</code> where possible for reliable joins and resampling.</li> <li>Missing or misaligned dates between model and observations are handled by an inner merge on <code>time</code> in this implementation.</li> </ul>"},{"location":"examples/advanced/cmip_obs_comparison/#visualization-outputs","title":"Visualization &amp; outputs \ud83d\udcca\u00b6","text":"<p>This section creates two linked visual summaries per selected variable:</p> <ul> <li>Top panel: a box plot of model value distributions by <code>source_id</code> combined with point markers showing model mean and hover details (RMSE, CORR, MAPE, MNB, Obs mean).</li> <li>Bottom panel: time series of each model (by <code>source_id</code>) with observations overplotted (if available).</li> </ul> <p>Files saved:</p> <ul> <li><code>distribution_&lt;variable&gt;.html</code> \u2014 interactive Holoviews/HvPlot HTML for the chosen variable.</li> <li>Inline portrait plot created with Matplotlib shows per-model RMSE (upper-left triangle) and correlation (lower-right triangle).</li> </ul> <p>Tip: Change <code>var</code> to iterate through variables and call <code>hv.save()</code> for each variable to persist results to disk.</p>"}]}