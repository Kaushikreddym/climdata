{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to climdata","text":""},{"location":"#climdata-quickstart-overview","title":"ClimData \u2014 Quickstart &amp; Overview","text":"<p>ClimData provides a unified interface for extracting climate data from multiple providers (MSWX, CMIP, POWER, DWD, HYRAS), computing extreme indices, and converting results to tabular form. The ClimData (or ClimateExtractor) class is central: it manages configuration, extraction, index computation, and common I/O.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Provider-agnostic extraction (point / region / shapefile)</li> <li>Unit normalization via xclim</li> <li>Compute extreme indices using package indices</li> <li>Convert xarray Datasets \u2192 long-form pandas DataFrames</li> <li>Simple workflow runner for chained actions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>1) Create and activate a conda environment: <pre><code># create\nconda create -n climdata python=3.11 -y\n\n# activate\nconda activate climdata\n</code></pre></p> <p>2) Install via pip (PyPI, if available) or from source: <pre><code># from PyPI\npip install climdata\n\n# or from local source (editable)\ngit clone &lt;repo-url&gt;\ncd climdata\npip install -e .\n</code></pre></p> <p>Install optional extras as needed (e.g., xclim, shapely, hydra, dask): <pre><code>pip install xarray xclim shapely hydra-core dask \"pandas&gt;=1.5\"\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from climdata import ClimData  # or from climdata.utils.wrapper_workflow import ClimateExtractor\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n    \"data_dir=/path/to/data\",\n    \"index=tn10p\",\n]\n\n# initialize\nextractor = ClimData(overrides=overrides)\n\n# extract data (returns xarray.Dataset and updates internal state)\nds = extractor.extract()\n\n# compute index (uses cfg.index)\nds_index = extractor.calc_index(ds)\n\n# convert to long-form dataframe and save\ndf = extractor.to_dataframe(ds_index)\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"#workflow-runner","title":"Workflow runner","text":"<p>Use <code>run_workflow</code> for multi-step sequences: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n</code></pre> <code>WorkflowResult</code> contains produced dataset(s), dataframe(s), and filenames.</p>"},{"location":"#documentation-api","title":"Documentation &amp; API","text":"<ul> <li>See API docs under <code>docs/api/</code> for detailed descriptions of ClimData/ClimateExtractor methods.</li> <li>Examples and notebooks are under <code>examples/</code>.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Run tests and lint locally.</li> <li>Follow project coding and documentation conventions; submit PRs with tests.</li> </ul>"},{"location":"#license","title":"License","text":"<p>Refer to the repository LICENSE file for terms.</p>"},{"location":"#tip","title":"\u26a1\ufe0f Tip","text":"<ul> <li> <p>Make sure <code>yq</code> is installed:   <pre><code>brew install yq   # macOS\n# OR\npip install yq\n</code></pre></p> </li> <li> <p>To see available variables for a specific dataset (for example <code>mswx</code>), run:   <pre><code>python download_location.py --cfg job | yq '.mappings.mswx.variables | keys'\n</code></pre></p> </li> </ul>"},{"location":"#key-features_1","title":"\u2699\ufe0f Key Features","text":"<ul> <li>Supports multiple weather data providers</li> <li>Uses <code>xarray</code> for robust gridded data extraction</li> <li>Handles curvilinear and rectilinear grids</li> <li>Uses a Google Drive Service Account for secure downloads</li> <li>Easily reproducible runs using Hydra</li> </ul>"},{"location":"#google-drive-api-setup","title":"\ud83d\udce1 Google Drive API Setup","text":"<p>This project uses the Google Drive API with a Service Account to securely download weather data files from a shared Google Drive folder.</p> <p>Follow these steps to set it up correctly:</p>"},{"location":"#1-create-a-google-cloud-project","title":"\u2705 1. Create a Google Cloud Project","text":"<ul> <li>Go to Google Cloud Console.</li> <li>Click \u201cSelect Project\u201d \u2192 \u201cNew Project\u201d.</li> <li>Enter a project name (e.g. <code>WeatherDataDownloader</code>).</li> <li>Click \u201cCreate\u201d.</li> </ul>"},{"location":"#2-enable-the-google-drive-api","title":"\u2705 2. Enable the Google Drive API","text":"<ul> <li>In the left sidebar, go to APIs &amp; Services \u2192 Library.</li> <li>Search for \u201cGoogle Drive API\u201d.</li> <li>Click it, then click \u201cEnable\u201d.</li> </ul>"},{"location":"#3-create-a-service-account","title":"\u2705 3. Create a Service Account","text":"<ul> <li>Go to IAM &amp; Admin \u2192 Service Accounts.</li> <li>Click \u201cCreate Service Account\u201d.</li> <li>Enter a name (e.g. <code>weather-downloader-sa</code>).</li> <li>Click \u201cCreate and Continue\u201d. You can skip assigning roles for read-only Drive access.</li> <li>Click \u201cDone\u201d to finish.</li> </ul>"},{"location":"#4-create-and-download-a-json-key","title":"\u2705 4. Create and Download a JSON Key","text":"<ul> <li>After creating the Service Account, click on its email address to open its details.</li> <li>Go to the \u201cKeys\u201d tab.</li> <li>Click \u201cAdd Key\u201d \u2192 \u201cCreate new key\u201d \u2192 choose <code>JSON</code> \u2192 click \u201cCreate\u201d.</li> <li>A <code>.json</code> key file will download automatically. Store it securely!</li> </ul>"},{"location":"#5-store-the-json-key-securely","title":"\u2705 5. Store the JSON Key Securely","text":"<ul> <li>Place the downloaded <code>.json</code> key in the conf folder with the name service.json. </li> </ul>"},{"location":"#setup-instructions-from-era5-api","title":"Setup Instructions from ERA5 api","text":""},{"location":"#1-cds-api-key-setup","title":"1. CDS API Key Setup","text":"<ol> <li>Create a free account on the Copernicus Climate Data Store</li> <li>Once logged in, go to your user profile</li> <li>Click on the \"Show API key\" button</li> <li>Create the file <code>~/.cdsapirc</code> with the following content:</li> </ol> <pre><code>url: https://cds.climate.copernicus.eu/api/v2\nkey: &lt;your-api-key-here&gt;\n</code></pre> <ol> <li>Make sure the file has the correct permissions: <code>chmod 600 ~/.cdsapirc</code></li> </ol>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"climdata/","title":"climdata module","text":""},{"location":"climdata/#climdata","title":"<code>climdata</code>  <code>special</code>","text":"<p>Top-level package for climdata.</p>"},{"location":"climdata/#climdata.utils","title":"<code>utils</code>  <code>special</code>","text":""},{"location":"climdata/#climdata.utils.config","title":"<code>config</code>","text":""},{"location":"climdata/#climdata.utils.config.load_config","title":"<code>load_config(config_name='config', overrides=None, verbose=False)</code>","text":"<p>Load Hydra config using ./conf in cwd.</p> Source code in <code>climdata/utils/config.py</code> <pre><code>def load_config(config_name=\"config\", overrides=None, verbose=False):\n    \"\"\"\n    Load Hydra config using ./conf in cwd.\n    \"\"\"\n    config_path = _ensure_local_conf()\n    print(config_path+config_name)\n    with initialize(config_path=config_path, version_base=None):\n        cfg = compose(config_name=config_name, overrides=overrides or [])\n        if verbose:\n            print(OmegaConf.to_yaml(cfg))\n        return cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download","title":"<code>utils_download</code>","text":""},{"location":"climdata/#climdata.utils.utils_download.download_drive_file","title":"<code>download_drive_file(file_id, local_path, service)</code>","text":"<p>Download a single file from Drive to a local path.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def download_drive_file(file_id, local_path, service):\n    \"\"\"\n    Download a single file from Drive to a local path.\n    \"\"\"\n    request = service.files().get_media(fileId=file_id)\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n    with io.FileIO(local_path, 'wb') as fh:\n        downloader = MediaIoBaseDownload(fh, request)\n\n        done = False\n        while not done:\n            status, done = downloader.next_chunk()\n            print(f\"   \u2192 Download {int(status.progress() * 100)}% complete\")\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.fetch_dwd","title":"<code>fetch_dwd(var_cfg, var)</code>","text":"<p>Download HYRAS data for one variable and a list of years.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def fetch_dwd(var_cfg,var):\n    \"\"\"Download HYRAS data for one variable and a list of years.\"\"\"\n    param_mapping = var_cfg.dsinfo\n    provider = var_cfg.dataset.lower()\n    parameter_key = var\n    # Validate provider and parameter\n\n    param_info = param_mapping[provider]['variables'][parameter_key]\n    base_url = param_info[\"base_url\"]\n    prefix = param_info[\"prefix\"]\n    version = param_info[\"version\"]\n\n    start_date = var_cfg.time_range.start_date\n    end_date = var_cfg.time_range.end_date\n\n    # Parse dates &amp; extract unique years\n    start_year = datetime.fromisoformat(start_date).year\n    end_year = datetime.fromisoformat(end_date).year\n    years = list(range(start_year, end_year + 1))\n\n    # output_file = cfg.output.filename\n    os.makedirs(parameter_key, exist_ok=True)\n\n    for year in years:\n        file_name = f\"{prefix}_{year}_{version}_de.nc\"\n        file_url = f\"{base_url}{file_name}\"\n        local_path = os.path.join(var_cfg.data_dir,provider,parameter_key.upper(), file_name)\n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        print(f\"\u2b07\ufe0f  Checking: {file_url}\")\n\n        if os.path.exists(local_path):\n            print(f\"\u2714\ufe0f  Exists locally: {local_path}\")\n            continue\n\n        # Check if file exists on server first (HEAD request)\n        head = requests.head(file_url)\n        if head.status_code != 200:\n            raise FileNotFoundError(f\"\u274c Not found on server: {file_url} (HTTP {head.status_code})\")\n\n        print(f\"\u2b07\ufe0f  Downloading: {file_url}\")\n        try:\n            response = requests.get(file_url, stream=True)\n            response.raise_for_status()\n            with open(local_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            print(f\"\u2705 Saved: {local_path}\")\n        except requests.HTTPError as e:\n            raise RuntimeError(f\"\u274c Failed download: {file_url} \u2014 {e}\")\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.find_nearest_xy","title":"<code>find_nearest_xy(ds, target_lat, target_lon)</code>","text":"<p>Given a dataset with curvilinear grid, find the nearest x,y index.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def find_nearest_xy(ds, target_lat, target_lon):\n    \"\"\"\n    Given a dataset with curvilinear grid, find the nearest x,y index.\n    \"\"\"\n    lat = ds['lat'].values  # shape (y,x) or (x,y)\n    lon = ds['lon'].values\n\n    # Flatten to 1D for k-d tree\n    lat_flat = lat.flatten()\n    lon_flat = lon.flatten()\n\n    tree = cKDTree(np.column_stack((lat_flat, lon_flat)))\n    _, idx = tree.query([target_lat, target_lon])\n    iy, ix = np.unravel_index(idx, lat.shape)\n\n    return iy, ix\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.get_output_filename","title":"<code>get_output_filename(cfg, output_type='nc', lat=None, lon=None, shp_name=None, param='surface')</code>","text":"<p>Generate output filename based on config, output type, and extraction mode. output_type: \"nc\", \"csv\", or \"zarr\"</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def get_output_filename(cfg, output_type=\"nc\", lat=None, lon=None, shp_name = None, param=\"surface\"):\n    \"\"\"\n    Generate output filename based on config, output type, and extraction mode.\n    output_type: \"nc\", \"csv\", or \"zarr\"\n    \"\"\"\n    if output_type == \"csv\":\n        template = cfg.output.filename_csv\n    elif output_type == \"zarr\":\n        template = cfg.output.filename_zarr\n    else:\n        template = cfg.output.filename_nc\n\n    # If lat/lon are provided, use point template\n    if lat is not None and lon is not None:\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat=f\"{lat}\",\n            lon=f\"{lon}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    elif shp_name is not None:\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat_range=f\"{shp_name}\",\n            lon_range=f\"{shp_name}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    else:\n        # Use region bounds\n        region_bounds = cfg.bounds[cfg.region]\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat_range=f\"{region_bounds['lat_min']}-{region_bounds['lat_max']}\",\n            lon_range=f\"{region_bounds['lon_min']}-{region_bounds['lon_max']}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    return filename\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.list_drive_files","title":"<code>list_drive_files(folder_id, service)</code>","text":"<p>List all files in a Google Drive folder, handling pagination.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def list_drive_files(folder_id, service):\n    \"\"\"\n    List all files in a Google Drive folder, handling pagination.\n    \"\"\"\n    files = []\n    page_token = None\n\n    while True:\n        results = service.files().list(\n            q=f\"'{folder_id}' in parents and trashed = false\",\n            fields=\"files(id, name), nextPageToken\",\n            pageToken=page_token\n        ).execute()\n\n        files.extend(results.get(\"files\", []))\n        page_token = results.get(\"nextPageToken\", None)\n\n        if not page_token:\n            break\n\n    return files\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper","title":"<code>wrapper</code>","text":""},{"location":"climdata/#climdata.utils.wrapper.extract_index","title":"<code>extract_index(csv_path, cfg_name='config', extra_overrides=None, save_to_file=True, output_path='output_index.csv')</code>","text":"<p>Load climate CSV \u2192 generate dataset + time_range overrides automatically \u2192 convert to xarray \u2192 compute extreme index.</p> Source code in <code>climdata/utils/wrapper.py</code> <pre><code>def extract_index(\n    csv_path: str,\n    cfg_name: str = \"config\",\n    extra_overrides: list = None,       # Only additional overrides (e.g., \"index=tx90p\")\n    save_to_file: bool = True,\n    output_path: str = \"output_index.csv\",\n):\n    \"\"\"\n    Load climate CSV \u2192 generate dataset + time_range overrides automatically \u2192\n    convert to xarray \u2192 compute extreme index.\n    \"\"\"\n\n    # ---- Load CSV ----\n    df = pd.read_csv(csv_path, parse_dates=[\"time\"])\n\n    # ---- Always extract dataset + time_range from CSV ----\n    dataset = df[\"source\"].unique()[0]\n    time_range_start = df[\"time\"].min().strftime(\"%Y-%m-%d\")\n    time_range_end   = df[\"time\"].max().strftime(\"%Y-%m-%d\")\n\n    auto_overrides = [\n        f\"dataset={dataset}\",\n        f\"time_range.start_date={time_range_start}\",\n        f\"time_range.end_date={time_range_end}\",\n    ]\n\n    # ---- Merge with any user-provided overrides ----\n    # (e.g., \"index=tn10p\")\n    if extra_overrides:\n        overrides = auto_overrides + extra_overrides\n    else:\n        overrides = auto_overrides + [\"index=tn10p\"]  # default index\n\n    print(\"Using Hydra overrides:\", overrides)\n\n    # 1. Ensure local configs are available\n    conf_dir = _ensure_local_conf()  # copies conf/ to cwd\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    # 2. Initialize Hydra only if not already initialized\n    if not GlobalHydra.instance().is_initialized():\n        hydra_context = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        # If already initialized, just set context to None for clarity\n        hydra_context = None\n\n    # Use compose within context manager if newly initialized\n    if hydra_context is not None:\n        with hydra_context:\n            cfg: DictConfig = compose(config_name=cfg_name, overrides=overrides)\n    else:\n        # Already initialized: compose directly\n        cfg: DictConfig = compose(config_name=cfg_name, overrides=overrides)\n    # ---- Convert CSV \u2192 xarray ----\n    df_pivot = df.pivot_table(\n        index=[\"time\", \"lat\", \"lon\"],\n        columns=\"variable\",\n        values=\"value\"\n    ).reset_index()\n\n    ds = df_pivot.set_index([\"time\", \"lat\", \"lon\"]).to_xarray()\n\n    # ---- Attach units ----\n    for var in df[\"variable\"].unique():\n        units = df[df[\"variable\"] == var][\"units\"].iloc[0]\n        ds[var].attrs[\"units\"] = units\n\n    print(ds)\n\n    # ---- Compute extreme index ----\n    indices = climdata.extreme_index(cfg, ds)\n    print(f\"Calculating index: {cfg.index}\")\n\n    result = indices.calculate(cfg.index).compute()\n\n    # ---- Save result ----\n    if save_to_file:\n        result.to_dataframe().reset_index().to_csv(output_path)\n        print(f\"Saved result \u2192 {output_path}\")\n\n    return result\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper.preprocess_aoi","title":"<code>preprocess_aoi(cfg)</code>","text":"<p>Normalize AOI in cfg: - Converts string AOI \u2192 dict - Extracts shapely geometry from FeatureCollection, Feature, or raw geometry - Detects type: point, bbox, or polygon - Updates cfg with:     cfg.aoi_type: \"point\" | \"bbox\" | \"polygon\"     cfg.lat, cfg.lon: for point     cfg.bounds: [minx, miny, maxx, maxy] for bbox/polygon     cfg.geometry: shapely geometry object</p> Source code in <code>climdata/utils/wrapper.py</code> <pre><code>def preprocess_aoi(cfg):\n    \"\"\"\n    Normalize AOI in cfg:\n    - Converts string AOI \u2192 dict\n    - Extracts shapely geometry from FeatureCollection, Feature, or raw geometry\n    - Detects type: point, bbox, or polygon\n    - Updates cfg with:\n        cfg.aoi_type: \"point\" | \"bbox\" | \"polygon\"\n        cfg.lat, cfg.lon: for point\n        cfg.bounds: [minx, miny, maxx, maxy] for bbox/polygon\n        cfg.geometry: shapely geometry object\n    \"\"\"\n    # -----------------------------\n    # 1) Load AOI value from string\n    # -----------------------------\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(f\"AOI string is not valid JSON: {cfg.aoi}\")\n\n    aoi = cfg.aoi\n\n    # -----------------------------\n    # 2) Extract shapely geometry\n    # -----------------------------\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        if not aoi.get(\"features\"):\n            raise ValueError(\"FeatureCollection contains no features\")\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi and \"coordinates\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n    # -----------------------------\n    # 3) Determine AOI type\n    # -----------------------------\n    if isinstance(geom, Point):\n        # cfg.aoi_type = \"point\"\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        # Check if axis-aligned bbox\n        coords = list(geom.exterior.coords)\n        is_bbox = len(coords) == 5 and len({c[0] for c in coords}) == 2 and len({c[1] for c in coords}) == 2\n\n        # if is_bbox:\n        #     cfg.aoi_type = \"bbox\"\n        # else:\n        #     cfg.aoi_type = \"polygon\"\n\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds['custom'] = {\n                \"lat_min\": miny,\n                \"lat_max\": maxy,\n                \"lon_min\": minx,\n                \"lon_max\": maxx\n            }\n        cfg.region='custom'\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unsupported geometry type: {geom.geom_type}\")\n\n    # -----------------------------\n    # 4) Store geometry itself\n    # -----------------------------\n    # cfg.shapefile = geom\n\n    return cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow","title":"<code>wrapper_workflow</code>","text":""},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor","title":"<code> ClimateExtractor        </code>","text":"<p>Climate data extraction &amp; extreme index workflow manager.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>class ClimateExtractor:\n    \"\"\"Climate data extraction &amp; extreme index workflow manager.\"\"\"\n\n    def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n        self.cfg_name = cfg_name\n        self.conf_path = conf_path\n        self.cfg: Optional[DictConfig] = None\n\n        # Stage datasets\n        self.ds = None\n        self.current_ds = None\n        self.index_ds = None\n        self.impute_ds = None\n        self.bias_corrected_ds = None\n\n        # Stage DataFrames\n        self.raw_df = None\n        self.current_df = None\n        self.index_df = None\n        self.impute_df = None\n        self.bias_corrected_df = None\n        self.df = None  # alias for current_df\n\n        # filenames\n        self.filename = None\n        self.filetype = None\n\n        # Automatically load config on init\n        self.load_config(overrides)\n        self.cfg = self.preprocess_aoi(self.cfg)\n    def _gen_fn(self, ds: xr.Dataset):\n        \"\"\"\n        Create filenames (csv, nc, zarr) using config templates and dataset metadata.\n        Automatically handles coordinate aliases such as lat/latitude, lon/longitude,\n        time/date.\n        \"\"\"\n\n        # ------------------------\n        # Helper: find coord alias\n        # ------------------------\n        def find_coord(ds, names):\n            \"\"\"Return ds coordinate if any alias in names exists.\"\"\"\n            for name in names:\n                if name in ds.coords:\n                    return ds[name]\n            return None\n\n        # ------------------------\n        # Extract coordinates\n        # ------------------------\n        lat = find_coord(ds, [\"lat\", \"latitude\"])\n        lon = find_coord(ds, [\"lon\", \"longitude\"])\n        time = find_coord(ds, [\"time\", \"date\"])\n\n        # ------------------------\n        # Provider\n        # ------------------------\n        provider = ds.attrs.get(\"source\", \"unknown\")\n\n        # ------------------------\n        # Parameter(s)\n        # ------------------------\n        vars_list = list(ds.data_vars)\n        parameter = vars_list[0] if len(vars_list) == 1 else \"_\".join(vars_list)\n\n        # ------------------------\n        # Latitude range\n        # ------------------------\n        if lat is None:\n            lat_str = \"unknown\"\n            lat_range = \"unknown\"\n        else:\n            # Flatten in case of 2D lat/lon grid\n            lat_vals = lat.values.reshape(-1)\n            lat_min = float(lat_vals.min())\n            lat_max = float(lat_vals.max())\n\n            if lat_min == lat_max:\n                lat_str = f\"{lat_min}\"\n                lat_range = f\"{lat_min}\"\n            else:\n                lat_str = f\"{lat_min}_{lat_max}\"\n                lat_range = f\"{lat_min}-{lat_max}\"\n\n        # ------------------------\n        # Longitude range\n        # ------------------------\n        if lon is None:\n            lon_str = \"unknown\"\n            lon_range = \"unknown\"\n        else:\n            lon_vals = lon.values.reshape(-1)\n            lon_min = float(lon_vals.min())\n            lon_max = float(lon_vals.max())\n\n            if lon_min == lon_max:\n                lon_str = f\"{lon_min}\"\n                lon_range = f\"{lon_min}\"\n            else:\n                lon_str = f\"{lon_min}_{lon_max}\"\n                lon_range = f\"{lon_min}-{lon_max}\"\n\n        # ------------------------\n        # Time range\n        # ------------------------\n        if time is None:\n            start = end = \"unknown\"\n        else:\n            tvals = pd.to_datetime(time.values)\n            start = tvals.min().strftime(\"%Y-%m-%d\")\n            end = tvals.max().strftime(\"%Y-%m-%d\")\n\n        # ------------------------\n        # Build filenames\n        # ------------------------\n        outdir = Path(self.cfg.output.out_dir)\n\n        def build(fn_template):\n            return fn_template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range,\n                lon_range=lon_range,\n            )\n\n        self.filename_csv = str(outdir / build(self.cfg.output.filename_csv))\n        self.filename_nc = str(outdir / build(self.cfg.output.filename_nc))\n        self.filename_zarr = str(outdir / build(self.cfg.output.filename_zarr))\n\n\n    def _gen_fn_cfg(self):\n        \"\"\"\n        Generate output filenames using ONLY cfg and extracted ds,\n        without relying on uploaded dataset metadata.\n        \"\"\"\n\n        cfg = self.cfg\n        out = cfg.output\n        provider = cfg.dataset.lower()\n        if self.current_ds:\n            if len(self.current_ds.data_vars) == 0:\n                parameter = \"unknown\"\n            elif len(self.current_ds.data_vars) == 1:\n                parameter = next(iter(self.current_ds.data_vars))\n            else:\n                parameter = \"_\".join(self.current_ds.data_vars)\n        else:\n            parameter = \"_\".join(self.cfg.variables)\n        # --------------------------------\n        # Determine lat/lon values\n        # --------------------------------\n        if cfg.lat is not None and cfg.lon is not None:\n            lat_range = lon_range = None   # single point\n            lat_str = str(cfg.lat)\n            lon_str = str(cfg.lon)\n        else:\n            b = cfg.bounds[cfg.region]\n            lat_min, lat_max = b[\"lat_min\"], b[\"lat_max\"]\n            lon_min, lon_max = b[\"lon_min\"], b[\"lon_max\"]\n\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # --------------------------------\n        # Time range from cfg\n        # --------------------------------\n        start = pd.to_datetime(cfg.time_range.start_date).strftime(\"%Y-%m-%d\")\n        end = pd.to_datetime(cfg.time_range.end_date).strftime(\"%Y-%m-%d\")\n\n        # --------------------------------\n        # Format filenames\n        # --------------------------------\n        def format_template(template):\n            return template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range or lat_str,\n                lon_range=lon_range or lon_str,\n            )\n\n        out_dir = Path('./')\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        self.filename_csv = str(out_dir / format_template(out.filename_csv))\n        self.filename_nc = str(out_dir / format_template(out.filename_nc))\n        self.filename_zarr = str(out_dir / format_template(out.filename_zarr))\n\n    # ----------------------------\n    # Hydra config\n    # ----------------------------\n    def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n        overrides = overrides or []\n        conf_dir = _ensure_local_conf()\n        rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n        if not GlobalHydra.instance().is_initialized():\n            hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n        else:\n            hydra_ctx = None\n\n        if hydra_ctx:\n            with hydra_ctx:\n                self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        else:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        return self.cfg\n\n    # ----------------------------\n    # AOI preprocessing\n    # ----------------------------\n    def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n        if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n            return cfg\n\n        if isinstance(cfg.aoi, str):\n            try:\n                cfg.aoi = json.loads(cfg.aoi)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid AOI JSON string\")\n\n        aoi = cfg.aoi\n\n        if aoi.get(\"type\") == \"FeatureCollection\":\n            geom = shape(aoi[\"features\"][0][\"geometry\"])\n        elif aoi.get(\"type\") == \"Feature\":\n            geom = shape(aoi[\"geometry\"])\n        elif \"type\" in aoi:\n            geom = shape(aoi)\n        else:\n            raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n        if isinstance(geom, Point):\n            cfg.lat = geom.y\n            cfg.lon = geom.x\n            cfg.bounds = None\n        elif isinstance(geom, Polygon):\n            minx, miny, maxx, maxy = geom.bounds\n            cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                     \"lon_min\": minx, \"lon_max\": maxx}}\n            cfg.region = \"custom\"\n            cfg.lat = None\n            cfg.lon = None\n        else:\n            raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n        return cfg\n\n    # ----------------------------\n    # Upload NetCDF\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n        if not os.path.exists(nc_file):\n            raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n        ds = xr.open_dataset(nc_file)\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                                for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Upload CSV \u2192 xarray.Dataset\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n        if not os.path.exists(csv_file):\n            raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n        df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n        lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n        lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n        if lat_col is None or lon_col is None:\n            raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n        id_vars = [\"time\", lat_col, lon_col]\n        df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n        ds = df_wide.set_index(id_vars).to_xarray()\n\n        # Attach units from CSV\n        for var in ds.data_vars:\n            units_series = df[df[\"variable\"] == var][\"units\"]\n            ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n        # Global source attribute\n        if \"source\" in df.columns:\n            source_series = df[\"source\"].dropna().unique()\n            if len(source_series) &gt; 0:\n                ds.attrs[\"source\"] = source_series[0]\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Extract data from datasets like CMIP, DWD, etc.\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def extract(self) -&gt; xr.Dataset:\n        cfg = self.cfg\n        extract_kwargs = {}\n\n        if cfg.lat is not None and cfg.lon is not None:\n            extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n            if cfg.dataset == \"dwd\":\n                extract_kwargs[\"buffer_km\"] = 30\n        elif cfg.region is not None:\n            extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n        elif cfg.shapefile is not None:\n            extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n        ds = None\n        dataset_upper = cfg.dataset.upper()\n\n        if dataset_upper == \"MSWX\":\n            ds_vars = []\n            for var in cfg.variables:\n                mswx = climdata.MSWX(cfg)\n                mswx.extract(**extract_kwargs)\n                mswx.load(var)\n                ds_vars.append(mswx.dataset)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"CMIP\":\n            cmip = climdata.CMIP(cfg)\n            cmip.fetch()\n            cmip.load()\n            cmip.extract(**extract_kwargs)\n            ds = cmip.ds\n\n        elif dataset_upper == \"POWER\":\n            power = climdata.POWER(cfg)\n            power.fetch()\n            power.load()\n            ds = power.ds\n\n        elif dataset_upper == \"DWD\":\n            ds_vars = []\n            for var in cfg.variables:\n                dwd = climdata.DWD(cfg)\n                ds_var = dwd.extract(variable=var, **extract_kwargs)\n                ds_vars.append(ds_var)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"HYRAS\":\n            hyras = climdata.HYRAS(cfg)\n            ds_vars = []\n            for var in cfg.variables:\n                hyras.extract(**extract_kwargs)\n                ds_vars.append(hyras.load(var)[[var]])\n            ds = xr.merge(ds_vars, compat=\"override\")\n\n        for var in ds.data_vars:\n            ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n        ds = ds.compute()\n\n        self._gen_fn_cfg()\n\n        return ds\n    # ----------------------------\n    # Compute extreme index\n    # ----------------------------\n    @update_ds(attr_name='index_ds')\n    def calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        cfg = self.cfg\n\n        # Use provided ds or fallback\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.index is None:\n            print(\"No index selected.\")\n            return None\n\n        if \"time\" in ds.coords:\n            years = pd.to_datetime(ds.time.values).year\n            n_years = len(pd.unique(years))\n            if n_years &lt; 30:\n                warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n        indices = extreme_index(cfg, ds)\n        index_ds = indices.calculate(cfg.index).compute()\n        index_ds = index_ds.to_dataset(name=cfg.index)\n\n        self._gen_fn_cfg()\n\n        return index_ds\n    # ----------------------------\n    # Dataset \u2192 Long-form DataFrame\n    # ----------------------------\n    @update_df()\n    def to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        df = ds.to_dataframe().reset_index()\n\n        id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n        value_vars = [v for v in ds.data_vars if v in df.columns]\n\n        if not value_vars:\n            raise ValueError(\"No variables in dataset available to melt into long format\")\n\n        df_long = df.melt(\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"variable\",\n            value_name=\"value\"\n        )\n\n        df_long[\"units\"] = df_long[\"variable\"].apply(\n            lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n        )\n        df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n        self._gen_fn_cfg()\n        return df_long\n\n    # ----------------------------\n    # Save CSV\n    # ----------------------------\n    def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n        df = df if df is not None else self.current_df\n\n        filename = filename or getattr(self, \"filename_csv\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_csv is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        df.to_csv(filename, index=False)\n        self.filename_csv = str(path)\n        self.current_filename = str(path)\n\n        print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n        return filename\n\n    def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Save an xarray Dataset to NetCDF.\n        - If ds is None: save current_ds.\n        - If filename is None: use self.filename_nc.\n        - Creates directories if needed.\n        - Updates self.filename_nc and self.current_filename.\n        \"\"\"\n\n        # -------------------------------\n        # 1. Determine dataset to save\n        # -------------------------------\n        ds = ds or getattr(self, \"current_ds\", None)\n        if ds is None:\n            raise ValueError(\"No dataset available to save\")\n\n        # -------------------------------\n        # 2. Determine filename\n        # -------------------------------\n        filename = filename or getattr(self, \"filename_nc\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_nc is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # -------------------------------\n        # 3. Save to NetCDF\n        # -------------------------------\n        ds.to_netcdf(path)\n\n        # -------------------------------\n        # 4. Track filenames\n        # -------------------------------\n        self.filename_nc = str(path)\n        self.current_filename = str(path)\n\n        print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n        return str(path)\n\n    # ----------------------------\n    # Unified workflow\n    # ----------------------------\n    def run_workflow(self, overrides: Optional[List[str]] = None,\n                     actions: Optional[List[str]] = None,\n                     file: Optional[str] = None) -&gt; WorkflowResult:\n\n        actions = actions or [\"extract\", \"compute_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n        result = WorkflowResult(cfg=self.cfg)\n        for action in actions:\n\n            if action == \"upload_netcdf\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                        \"but none was provided.\"\n                    )\n                    # Validate extension\n                valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_netcdf: '{file}'. \"\n                        f\"Expected one of: {valid_nc_ext}\"\n                    )\n                self.upload_netcdf(file)\n                result.dataset = self.current_ds\n\n            elif action == \"upload_csv\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_csv' requires argument 'csv_file', \"\n                        \"but none was provided.\"\n                    )\n\n                # Validate CSV extension\n                valid_csv_ext = (\".csv\", \".csv.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_csv: '{file}'. \"\n                        f\"Expected one of: {valid_csv_ext}\"\n                    )\n\n                self.upload_csv(file)\n                result.dataset = self.current_ds\n\n            elif action == \"extract\":\n                if self.cfg.dataset is None:\n                    raise ValueError(\n                        \"Action 'extract' cannot run because no dataset provider is set \"\n                        \"(cfg.dataset is None).\"\n                    )\n                self.extract()\n                result.dataset = self.current_ds\n\n            elif action == \"calc_index\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before computing an index.\"\n                    )\n                self.calc_index()\n                result.index_ds = self.current_ds\n\n            elif action == \"to_dataframe\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before converting to a DataFrame.\"\n                    )\n                self.to_dataframe()\n                result.dataframe = self.current_df\n\n            elif action == \"to_csv\":\n                if self.current_df is None:\n                    raise ValueError(\n                        \"Action 'to_csv' requires a DataFrame, but no DataFrame is available. \"\n                        \"Use 'to_dataframe' or upload a CSV before saving.\"\n                    )\n                result.filename = self.to_csv()\n\n            elif action == \"to_nc\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before saving to NetCDF.\"\n                    )\n                result.filename = self.to_nc()\n\n            else:\n                raise ValueError(f\"Unknown action '{action}'\")\n\n        return result\n\n    # ----------------------------\n    # Exploration helpers using cfg.dsinfo\n    # ----------------------------\n    def get_datasets(self) -&gt; List[str]:\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n        return list(self.cfg.dsinfo.keys())\n\n    def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n\n        dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n        if dataset_name is None:\n            raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n        dsinfo = self.cfg.dsinfo.get(dataset_name)\n        if not dsinfo or \"variables\" not in dsinfo:\n            raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n        return list(dsinfo[\"variables\"].keys())\n\n    def get_varinfo(self, var: str) -&gt; dict:\n        \"\"\"\n        Get metadata for a variable from varinfo.\n\n        Parameters\n        ----------\n        var : str\n            Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n        Returns\n        -------\n        dict\n            Metadata dictionary containing cf_name, long_name, units, etc.\n\n        Raises\n        ------\n        ValueError\n            If varinfo is not loaded or variable not found.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            raise ValueError(\"Configuration or varinfo not loaded\")\n\n        if var not in self.cfg.varinfo:\n            raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n        return self.cfg.varinfo[var]\n\n\n    def get_actions(self) -&gt; dict:\n        \"\"\"\n        Return a dictionary of workflow actions with their outputs and descriptions.\n        Supports actions.yaml in mapping style (key: action_name).\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n            raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n        actions_map = getattr(self.cfg, \"actionsinfo\")\n\n        # If 'actions' key exists, fallback to list style\n        if \"actions\" in actions_map:\n            actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                        for a in actions_map[\"actions\"]}\n\n        return actions_map\n    def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n        \"\"\"\n        Fetch climate extreme indices from cfg.extinfo that involve the given variables.\n\n        Parameters\n        ----------\n        cfg : DictConfig\n            Configuration object containing 'extinfo' which has the indices definitions.\n        variables : List[str]\n            List of variable names to filter indices by (e.g., ['pr', 'tasmin']).\n        require_all : bool, default False\n            If True, only return indices where all required variables are present in `variables`.\n            If False, return indices if any variable matches.\n\n        Returns\n        -------\n        Dict[str, dict]\n            Dictionary of indices matching the provided variables.\n        \"\"\"\n        cfg = self.cfg\n        variables = variables or cfg.variables \n        if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n            raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n        indices_def = cfg.extinfo.get(\"indices\", {})\n        if not indices_def:\n            return {}\n\n        matched_indices = {}\n        for idx_name, idx_info in indices_def.items():\n            idx_vars = idx_info.get(\"variables\", [])\n            if require_all:\n                if all(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n            else:\n                if any(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n\n        return matched_indices\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_actions","title":"<code>get_actions(self)</code>","text":"<p>Return a dictionary of workflow actions with their outputs and descriptions. Supports actions.yaml in mapping style (key: action_name).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_actions(self) -&gt; dict:\n    \"\"\"\n    Return a dictionary of workflow actions with their outputs and descriptions.\n    Supports actions.yaml in mapping style (key: action_name).\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n        raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n    actions_map = getattr(self.cfg, \"actionsinfo\")\n\n    # If 'actions' key exists, fallback to list style\n    if \"actions\" in actions_map:\n        actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                    for a in actions_map[\"actions\"]}\n\n    return actions_map\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices","title":"<code>get_indices(self, variables, require_all=True)</code>","text":"<p>Fetch climate extreme indices from cfg.extinfo that involve the given variables.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices--parameters","title":"Parameters","text":"<p>cfg : DictConfig     Configuration object containing 'extinfo' which has the indices definitions. variables : List[str]     List of variable names to filter indices by (e.g., ['pr', 'tasmin']). require_all : bool, default False     If True, only return indices where all required variables are present in <code>variables</code>.     If False, return indices if any variable matches.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices--returns","title":"Returns","text":"<p>Dict[str, dict]     Dictionary of indices matching the provided variables.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n    \"\"\"\n    Fetch climate extreme indices from cfg.extinfo that involve the given variables.\n\n    Parameters\n    ----------\n    cfg : DictConfig\n        Configuration object containing 'extinfo' which has the indices definitions.\n    variables : List[str]\n        List of variable names to filter indices by (e.g., ['pr', 'tasmin']).\n    require_all : bool, default False\n        If True, only return indices where all required variables are present in `variables`.\n        If False, return indices if any variable matches.\n\n    Returns\n    -------\n    Dict[str, dict]\n        Dictionary of indices matching the provided variables.\n    \"\"\"\n    cfg = self.cfg\n    variables = variables or cfg.variables \n    if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n        raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n    indices_def = cfg.extinfo.get(\"indices\", {})\n    if not indices_def:\n        return {}\n\n    matched_indices = {}\n    for idx_name, idx_info in indices_def.items():\n        idx_vars = idx_info.get(\"variables\", [])\n        if require_all:\n            if all(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n        else:\n            if any(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n\n    return matched_indices\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo","title":"<code>get_varinfo(self, var)</code>","text":"<p>Get metadata for a variable from varinfo.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--parameters","title":"Parameters","text":"<p>var : str     Name of the variable, e.g., 'tas', 'tasmax', 'pr'.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--returns","title":"Returns","text":"<p>dict     Metadata dictionary containing cf_name, long_name, units, etc.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--raises","title":"Raises","text":"<p>ValueError     If varinfo is not loaded or variable not found.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_varinfo(self, var: str) -&gt; dict:\n    \"\"\"\n    Get metadata for a variable from varinfo.\n\n    Parameters\n    ----------\n    var : str\n        Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n    Returns\n    -------\n    dict\n        Metadata dictionary containing cf_name, long_name, units, etc.\n\n    Raises\n    ------\n    ValueError\n        If varinfo is not loaded or variable not found.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        raise ValueError(\"Configuration or varinfo not loaded\")\n\n    if var not in self.cfg.varinfo:\n        raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n    return self.cfg.varinfo[var]\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_nc","title":"<code>to_nc(self, ds=None, filename=None)</code>","text":"<p>Save an xarray Dataset to NetCDF. - If ds is None: save current_ds. - If filename is None: use self.filename_nc. - Creates directories if needed. - Updates self.filename_nc and self.current_filename.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Save an xarray Dataset to NetCDF.\n    - If ds is None: save current_ds.\n    - If filename is None: use self.filename_nc.\n    - Creates directories if needed.\n    - Updates self.filename_nc and self.current_filename.\n    \"\"\"\n\n    # -------------------------------\n    # 1. Determine dataset to save\n    # -------------------------------\n    ds = ds or getattr(self, \"current_ds\", None)\n    if ds is None:\n        raise ValueError(\"No dataset available to save\")\n\n    # -------------------------------\n    # 2. Determine filename\n    # -------------------------------\n    filename = filename or getattr(self, \"filename_nc\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_nc is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------\n    # 3. Save to NetCDF\n    # -------------------------------\n    ds.to_netcdf(path)\n\n    # -------------------------------\n    # 4. Track filenames\n    # -------------------------------\n    self.filename_nc = str(path)\n    self.current_filename = str(path)\n\n    print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n    return str(path)\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.WorkflowResult","title":"<code> WorkflowResult        </code>  <code>dataclass</code>","text":"<p>WorkflowResult(cfg: omegaconf.dictconfig.DictConfig, dataset: Optional[xarray.core.dataset.Dataset] = None, dataframe: Optional[pandas.core.frame.DataFrame] = None, filename: Optional[str] = None, index_ds: Optional[xarray.core.dataset.Dataset] = None, index_filename: Optional[str] = None)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@dataclass\nclass WorkflowResult:\n    cfg: DictConfig\n    dataset: Optional[xr.Dataset] = None\n    dataframe: Optional[pd.DataFrame] = None\n    filename: Optional[str] = None\n    index_ds: Optional[xr.Dataset] = None\n    index_filename: Optional[str] = None\n\n    def keys(self):\n        return [k for k, v in self.__dict__.items() if v is not None]\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.update_df","title":"<code>update_df(attr_name=None)</code>","text":"<p>Decorator to update self.current_df with the result of a method, and optionally store it in a named attribute.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def update_df(attr_name=None):\n    \"\"\"\n    Decorator to update self.current_df with the result of a method,\n    and optionally store it in a named attribute.\n    \"\"\"\n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n            df = func(self, *args, **kwargs)\n            if df is not None:\n                self.current_df = df\n                self.df = df  # keep alias for convenience\n                if attr_name:\n                    setattr(self, attr_name, df)\n            return df\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"common/","title":"Common Concepts in climdata","text":"<p>This page describes common terminology, configuration patterns, and reusable components in the <code>climdata</code> package.</p>"},{"location":"common/#configuration-files","title":"Configuration Files","text":"<ul> <li>All configuration is managed via Hydra and YAML files in the <code>conf/</code> directory.</li> <li>See <code>config.yaml</code> for the main entry point.</li> </ul>"},{"location":"common/#standard-variable-names","title":"Standard Variable Names","text":"<ul> <li>Variables follow CF conventions (see <code>variables.yaml</code>).</li> <li>Example: <code>tas</code> for air temperature, <code>pr</code> for precipitation.</li> </ul>"},{"location":"common/#output-schema","title":"Output Schema","text":"<p>All outputs are standardized to the following columns:</p> Column Description latitude Latitude of observation/grid longitude Longitude of observation/grid time Timestamp source Data source/provider variable Variable name value Observed or modeled value units Units of measurement"},{"location":"common/#regions-and-bounds","title":"Regions and Bounds","text":"<ul> <li>Regions are defined in <code>config.yaml</code> under <code>bounds</code>.</li> <li>Example: <code>europe</code>, <code>global</code>.</li> </ul>"},{"location":"common/#usage-patterns","title":"Usage Patterns","text":"<ul> <li>Use <code>climdata.load_config()</code> to load configuration.</li> <li>Use <code>climdata.DWD(cfg)</code> or <code>climdata.MSWX(cfg)</code> for dataset access.</li> </ul> <p>Add more shared concepts as your documentation grows.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>climdata could always use more documentation, whether as part of the official climdata docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up climdata for local development.</p> <ol> <li> <p>Fork the climdata repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/climdata.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv climdata\n$ cd climdata/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 climdata tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/Kaushikreddym/climdata/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>It's recommended to create and activate a conda environment first, then install via pip:</p> <pre><code># create &amp; activate conda environment (recommended)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install climdata from PyPI\npip install climdata\n</code></pre> <p>This is the preferred method to install climdata, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install climdata from sources, create/activate a conda environment and then install from the repository:</p> <pre><code># create &amp; activate conda environment (optional)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install from GitHub (editable install if desired)\npip install git+https://github.com/Kaushikreddym/climdata\n# or for editable development install:\n# git clone https://github.com/Kaushikreddym/climdata\n# cd climdata\n# pip install -e .\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Quick examples to get started with the ClimData workflow utilities.</p>"},{"location":"usage/#quickstart","title":"Quickstart","text":"<p>Install into a conda env (recommended) and then pip: <pre><code>conda create -n climdata python=3.11 -y\nconda activate climdata\npip install climdata\n# or from source:\n# pip install -e .\n</code></pre></p>"},{"location":"usage/#minimal-example","title":"Minimal example","text":"<pre><code>from climdata import ClimData\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Extract dataset (updates extractor.current_ds)\nds = extractor.extract()\n\n# Compute configured extreme index (updates extractor.index_ds)\nindex_ds = extractor.calc_index(ds)\n\n# Convert to long-form DataFrame (updates extractor.current_df)\ndf = extractor.to_dataframe(index_ds)\n\n# Save DataFrame to CSV\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"usage/#single-call-workflow","title":"Single-call workflow","text":"<p>Use the high-level runner to chain common steps: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n# result contains produced dataset/dataframe and filenames\nprint(result.dataframe.head())\nprint(\"Saved to:\", result.filename)\n</code></pre></p>"},{"location":"usage/#uploading-existing-files","title":"Uploading existing files","text":"<ul> <li>Load NetCDF: extractor.upload_netcdf(\"path/to/file.nc\")</li> <li>Load long-form CSV: extractor.upload_csv(\"path/to/file.csv\")</li> </ul>"},{"location":"usage/#introspection-helpers","title":"Introspection helpers","text":"<ul> <li>extractor.get_datasets()</li> <li>extractor.get_variables(dataset_name)</li> <li>extractor.get_varinfo(varname)</li> <li>extractor.get_actions()</li> </ul>"},{"location":"usage/#notes","title":"Notes","text":"<ul> <li>See <code>docs/index.md</code> for installation details and full examples.</li> <li>For provider-specific options (MSWX, CMIP, POWER, DWD, HYRAS) consult the configuration files under <code>conf/</code> and the API docs.</li> </ul>"}]}