{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to climdata","text":""},{"location":"#climdata-quickstart-overview","title":"ClimData \u2014 Quickstart &amp; Overview","text":"<p>ClimData provides a unified interface for extracting climate data from multiple providers (MSWX, CMIP, POWER, DWD, HYRAS), computing extreme indices, and converting results to tabular form. The ClimData (or ClimateExtractor) class is central: it manages configuration, extraction, index computation, and common I/O.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Provider-agnostic extraction (point / region / shapefile)</li> <li>Unit normalization via xclim</li> <li>Compute extreme indices using package indices</li> <li>Convert xarray Datasets \u2192 long-form pandas DataFrames</li> <li>Simple workflow runner for chained actions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>1) Create and activate a conda environment: <pre><code># create\nconda create -n climdata python=3.11 -y\n\n# activate\nconda activate climdata\n</code></pre></p> <p>2) Install via pip (PyPI, if available) or from source: <pre><code># from PyPI\npip install climdata\n\n# or from local source (editable)\ngit clone &lt;repo-url&gt;\ncd climdata\npip install -e .\n</code></pre></p> <p>Install optional extras as needed (e.g., xclim, shapely, hydra, dask): <pre><code>pip install xarray xclim shapely hydra-core dask \"pandas&gt;=1.5\"\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from climdata import ClimData  # or from climdata.utils.wrapper_workflow import ClimateExtractor\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n    \"data_dir=/path/to/data\",\n    \"index=tn10p\",\n]\n\n# initialize\nextractor = ClimData(overrides=overrides)\n\n# extract data (returns xarray.Dataset and updates internal state)\nds = extractor.extract()\n\n# compute index (uses cfg.index)\nds_index = extractor.calc_index(ds)\n\n# convert to long-form dataframe and save\ndf = extractor.to_dataframe(ds_index)\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"#workflow-runner","title":"Workflow runner","text":"<p>Use <code>run_workflow</code> for multi-step sequences: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n</code></pre> <code>WorkflowResult</code> contains produced dataset(s), dataframe(s), and filenames.</p>"},{"location":"#documentation-api","title":"Documentation &amp; API","text":"<ul> <li>See API docs under <code>docs/api/</code> for detailed descriptions of ClimData/ClimateExtractor methods.</li> <li>Examples and notebooks are under <code>examples/</code>.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Run tests and lint locally.</li> <li>Follow project coding and documentation conventions; submit PRs with tests.</li> </ul>"},{"location":"#license","title":"License","text":"<p>Refer to the repository LICENSE file for terms.</p>"},{"location":"#tip","title":"\u26a1\ufe0f Tip","text":"<ul> <li> <p>Make sure <code>yq</code> is installed:   <pre><code>brew install yq   # macOS\n# OR\npip install yq\n</code></pre></p> </li> <li> <p>To see available variables for a specific dataset (for example <code>mswx</code>), run:   <pre><code>python download_location.py --cfg job | yq '.mappings.mswx.variables | keys'\n</code></pre></p> </li> </ul>"},{"location":"#key-features_1","title":"\u2699\ufe0f Key Features","text":"<ul> <li>Supports multiple weather data providers</li> <li>Uses <code>xarray</code> for robust gridded data extraction</li> <li>Handles curvilinear and rectilinear grids</li> <li>Uses a Google Drive Service Account for secure downloads</li> <li>Easily reproducible runs using Hydra</li> </ul>"},{"location":"#google-drive-api-setup","title":"\ud83d\udce1 Google Drive API Setup","text":"<p>This project uses the Google Drive API with a Service Account to securely download weather data files from a shared Google Drive folder.</p> <p>Follow these steps to set it up correctly:</p>"},{"location":"#1-create-a-google-cloud-project","title":"\u2705 1. Create a Google Cloud Project","text":"<ul> <li>Go to Google Cloud Console.</li> <li>Click \u201cSelect Project\u201d \u2192 \u201cNew Project\u201d.</li> <li>Enter a project name (e.g. <code>WeatherDataDownloader</code>).</li> <li>Click \u201cCreate\u201d.</li> </ul>"},{"location":"#2-enable-the-google-drive-api","title":"\u2705 2. Enable the Google Drive API","text":"<ul> <li>In the left sidebar, go to APIs &amp; Services \u2192 Library.</li> <li>Search for \u201cGoogle Drive API\u201d.</li> <li>Click it, then click \u201cEnable\u201d.</li> </ul>"},{"location":"#3-create-a-service-account","title":"\u2705 3. Create a Service Account","text":"<ul> <li>Go to IAM &amp; Admin \u2192 Service Accounts.</li> <li>Click \u201cCreate Service Account\u201d.</li> <li>Enter a name (e.g. <code>weather-downloader-sa</code>).</li> <li>Click \u201cCreate and Continue\u201d. You can skip assigning roles for read-only Drive access.</li> <li>Click \u201cDone\u201d to finish.</li> </ul>"},{"location":"#4-create-and-download-a-json-key","title":"\u2705 4. Create and Download a JSON Key","text":"<ul> <li>After creating the Service Account, click on its email address to open its details.</li> <li>Go to the \u201cKeys\u201d tab.</li> <li>Click \u201cAdd Key\u201d \u2192 \u201cCreate new key\u201d \u2192 choose <code>JSON</code> \u2192 click \u201cCreate\u201d.</li> <li>A <code>.json</code> key file will download automatically. Store it securely!</li> </ul>"},{"location":"#5-store-the-json-key-securely","title":"\u2705 5. Store the JSON Key Securely","text":"<ul> <li>Place the downloaded <code>.json</code> key in the conf folder with the name service.json. </li> </ul>"},{"location":"#setup-instructions-from-era5-api","title":"Setup Instructions from ERA5 api","text":""},{"location":"#1-cds-api-key-setup","title":"1. CDS API Key Setup","text":"<ol> <li>Create a free account on the Copernicus Climate Data Store</li> <li>Once logged in, go to your user profile</li> <li>Click on the \"Show API key\" button</li> <li>Create the file <code>~/.cdsapirc</code> with the following content:</li> </ol> <pre><code>url: https://cds.climate.copernicus.eu/api/v2\nkey: &lt;your-api-key-here&gt;\n</code></pre> <ol> <li>Make sure the file has the correct permissions: <code>chmod 600 ~/.cdsapirc</code></li> </ol>"},{"location":"INTEGRATION_GUIDE/","title":"W5E5 Dataset Integration Guide","text":""},{"location":"INTEGRATION_GUIDE/#quick-start","title":"Quick Start","text":"<p>The W5E5 dataset from ISIMIP has been successfully integrated into the climdata package. Here's everything you need to know:</p>"},{"location":"INTEGRATION_GUIDE/#what-was-added","title":"What Was Added","text":""},{"location":"INTEGRATION_GUIDE/#1-new-dataset-module","title":"1. New Dataset Module","text":"<p>File: <code>climdata/datasets/W5E5.py</code></p> <p>A complete implementation following the same pattern as existing datasets (ERA5, MSWX, POWER) with: - <code>fetch()</code> - Download data from ISIMIP repository - <code>load()</code> - Load NetCDF files into xarray - <code>extract()</code> - Spatial subsetting (point, box, shapefile) - <code>save_csv()</code> / <code>save_netcdf()</code> - Save results</p>"},{"location":"INTEGRATION_GUIDE/#2-configuration","title":"2. Configuration","text":"<p>File: <code>climdata/conf/mappings/parameters.yaml</code></p> <p>Added W5E5 configuration section with: - Dataset parameters (simulation_round, product, forcing, scenario) - Variable mappings (tas, tasmax, tasmin, pr, rsds, hurs, sfcWind, etc.) - Metadata (units, long names)</p>"},{"location":"INTEGRATION_GUIDE/#3-dependency","title":"3. Dependency","text":"<p>File: <code>requirements.txt</code></p> <p>Added <code>isimip-client</code> package for accessing ISIMIP data repository</p>"},{"location":"INTEGRATION_GUIDE/#4-documentation-examples","title":"4. Documentation &amp; Examples","text":"<ul> <li>Example Notebook: <code>docs/examples/w5e5_example.ipynb</code></li> <li>Test Script: <code>tests/test_w5e5.py</code></li> <li>Full Documentation: <code>docs/W5E5_README.md</code></li> </ul>"},{"location":"INTEGRATION_GUIDE/#usage-comparison","title":"Usage Comparison","text":""},{"location":"INTEGRATION_GUIDE/#w5e5-vs-other-datasets","title":"W5E5 vs Other Datasets","text":"<pre><code># ========== W5E5 (NEW) ==========\nfrom climdata.datasets.W5E5 import W5E5\nw5e5 = W5E5(cfg)\nw5e5.fetch()  # Downloads from ISIMIP\nw5e5.load()\nw5e5.extract(point=(lon, lat))\nw5e5.save_csv('output.csv')\n\n# ========== ERA5 (Existing) ==========\nfrom climdata.datasets.ERA5 import ERA5Mirror\nera5 = ERA5Mirror(base_path, fs)\nera5.download_chunk(variable, year, month)\n# ... different API\n\n# ========== MSWX (Existing) ==========\nfrom climdata.datasets.MSWX import MSWXmirror\nmswx = MSWXmirror(cfg)\nmswx.fetch(folder_id, variable)\nmswx.load()\nmswx.extract(point=(lon, lat))\n\n# ========== NASA POWER (Existing) ==========\nfrom climdata.datasets.NASAPOWER import POWER\npower = POWER(cfg)\npower.fetch()\npower.load()\npower.extract(start, end)\n</code></pre> <p>W5E5 follows the MSWX/POWER pattern - simpler and more consistent!</p>"},{"location":"INTEGRATION_GUIDE/#configuration-examples","title":"Configuration Examples","text":""},{"location":"INTEGRATION_GUIDE/#using-config-file","title":"Using Config File","text":"<pre><code># In config.yaml or as overrides\ndataset: w5e5\nlat: 52.52\nlon: 13.405\nvariables: [tas, tasmax, tasmin, pr, rsds]\ntime_range:\n  start_date: \"2010-01-01\"\n  end_date: \"2010-12-31\"\ndata_dir: ./data\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#using-python-overrides","title":"Using Python Overrides","text":"<pre><code>from climdata.utils.config import load_config\n\ncfg = load_config(\n    config_name='config',\n    overrides=[\n        'dataset=w5e5',\n        'lat=40.7128',\n        'lon=-74.0060',\n        'variables=[tas,pr,rsds]',\n        'time_range.start_date=2015-01-01',\n        'time_range.end_date=2015-12-31'\n    ]\n)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#data-access-flow","title":"Data Access Flow","text":"<pre><code>User Request\n    \u2193\nW5E5(cfg) - Initialize with config\n    \u2193\nfetch() - Search ISIMIP API \u2192 Download files \u2192 Cache locally\n    \u2193\nload() - Open NetCDF \u2192 Merge variables \u2192 Subset time range\n    \u2193\nextract() - Spatial subset (point/box/shape)\n    \u2193\nsave_csv() / save_netcdf() - Export results\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#integration-with-existing-workflows","title":"Integration with Existing Workflows","text":""},{"location":"INTEGRATION_GUIDE/#works-with-wrapperpy","title":"Works with wrapper.py","text":"<pre><code># If you're using the wrapper workflow, W5E5 can be integrated:\nfrom climdata.utils.wrapper import DatasetWrapper\n\nwrapper = DatasetWrapper(\n    dataset='w5e5',\n    lat=52.52,\n    lon=13.405,\n    variables=['tas', 'pr'],\n    start_date='2010-01-01',\n    end_date='2010-12-31'\n)\n\n# The wrapper would need to be updated to recognize 'w5e5' dataset\n# and instantiate the W5E5 class accordingly\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#works-with-cli","title":"Works with CLI","text":"<pre><code># After updating the CLI to recognize w5e5:\nclimdata fetch --dataset w5e5 --lat 52.52 --lon 13.405 \\\n               --variables tas,pr --start 2010-01-01 --end 2010-12-31\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#testing-the-implementation","title":"Testing the Implementation","text":""},{"location":"INTEGRATION_GUIDE/#1-quick-test-python-script","title":"1. Quick Test (Python Script)","text":"<pre><code>cd /beegfs/muduchuru/pkgs_fnl/climdata\npython tests/test_w5e5.py\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#2-notebook-test","title":"2. Notebook Test","text":"<pre><code>jupyter notebook docs/examples/w5e5_example.ipynb\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#3-manual-test","title":"3. Manual Test","text":"<pre><code>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\n\ncfg = load_config(overrides=['dataset=w5e5', 'lat=52.52', 'lon=13.405'])\nw5e5 = W5E5(cfg)\nprint(\"\u2713 W5E5 initialized successfully!\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#next-steps-optional-enhancements","title":"Next Steps (Optional Enhancements)","text":""},{"location":"INTEGRATION_GUIDE/#1-add-to-wrappercli","title":"1. Add to Wrapper/CLI","text":"<p>Update <code>climdata/utils/wrapper.py</code> or CLI to recognize 'w5e5' dataset:</p> <pre><code>def get_dataset_instance(dataset_name, cfg):\n    if dataset_name == 'w5e5':\n        from climdata.datasets.W5E5 import W5E5\n        return W5E5(cfg)\n    elif dataset_name == 'mswx':\n        from climdata.datasets.MSWX import MSWXmirror\n        return MSWXmirror(cfg)\n    # ... etc\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#2-add-unit-tests","title":"2. Add Unit Tests","text":"<p>Create proper unit tests in <code>tests/test_climdata.py</code>:</p> <pre><code>def test_w5e5_init():\n    cfg = create_test_config('w5e5')\n    w5e5 = W5E5(cfg)\n    assert w5e5.client is not None\n\ndef test_w5e5_variable_mapping():\n    w5e5 = W5E5(test_cfg)\n    assert w5e5._map_variable_name('tas') == 'tas'\n    assert w5e5._map_variable_name('sfcWind') == 'sfcwind'\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#3-add-to-documentation","title":"3. Add to Documentation","text":"<p>Update main documentation files: - <code>README.md</code> - Add W5E5 to list of supported datasets - <code>docs/usage.md</code> - Add W5E5 usage examples - <code>docs/api.md</code> - Add W5E5 API documentation</p>"},{"location":"INTEGRATION_GUIDE/#dataset-comparison-summary","title":"Dataset Comparison Summary","text":"Feature W5E5 ERA5 MSWX POWER Resolution 0.5\u00b0 0.25\u00b0 0.1\u00b0 0.5\u00b0 Period 1979+ 1950+ 1979-2020 1981+ Variables 10+ 100+ 7 6+ Source ISIMIP CDS Google Drive NASA API Access isimip-client cdsapi Google API REST API Speed Medium Slow Fast Very Fast Quality High Very High High Good"},{"location":"INTEGRATION_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INTEGRATION_GUIDE/#import-error","title":"Import Error","text":"<p><pre><code>ImportError: isimip-client is required\n</code></pre> Solution: <code>pip install isimip-client</code></p>"},{"location":"INTEGRATION_GUIDE/#download-error","title":"Download Error","text":"<p><pre><code>Error fetching tas: ...\n</code></pre> Solution: Check internet connection and ISIMIP API status</p>"},{"location":"INTEGRATION_GUIDE/#memory-error","title":"Memory Error","text":"<p><pre><code>MemoryError during load\n</code></pre> Solution: Extract smaller region or shorter time period</p>"},{"location":"INTEGRATION_GUIDE/#files-createdmodified","title":"Files Created/Modified","text":""},{"location":"INTEGRATION_GUIDE/#created","title":"Created","text":"<ul> <li>\u2705 <code>climdata/datasets/W5E5.py</code> - Main implementation</li> <li>\u2705 <code>docs/W5E5_README.md</code> - Full documentation</li> <li>\u2705 <code>docs/examples/w5e5_example.ipynb</code> - Example notebook</li> <li>\u2705 <code>tests/test_w5e5.py</code> - Test script</li> <li>\u2705 <code>docs/INTEGRATION_GUIDE.md</code> - This file</li> </ul>"},{"location":"INTEGRATION_GUIDE/#modified","title":"Modified","text":"<ul> <li>\u2705 <code>requirements.txt</code> - Added isimip-client</li> <li>\u2705 <code>climdata/conf/mappings/parameters.yaml</code> - Added w5e5 config</li> </ul>"},{"location":"INTEGRATION_GUIDE/#summary","title":"Summary","text":"<p>\u2705 W5E5 dataset is fully integrated and ready to use!</p> <p>The implementation: - \u2705 Follows existing dataset patterns - \u2705 Uses ISIMIP API via isimip-client - \u2705 Supports all standard operations (fetch, load, extract, save) - \u2705 Includes comprehensive documentation and examples - \u2705 Is configurable via YAML/Python - \u2705 Works with existing climdata infrastructure</p> <p>You can start using W5E5 immediately with the examples provided!</p>"},{"location":"W5E5_README/","title":"W5E5 Dataset Support","text":"<p>The climdata package now includes support for the W5E5 (WFDE5 over land merged with ERA5 over ocean) global meteorological forcing dataset from ISIMIP.</p>"},{"location":"W5E5_README/#about-w5e5","title":"About W5E5","text":"<p>W5E5 is a high-quality global meteorological forcing dataset that combines: - WFDE5 (WATCH Forcing Data ERA5) over land - ERA5 reanalysis over ocean - Available through the ISIMIP (Inter-Sectoral Impact Model Intercomparison Project) data repository</p>"},{"location":"W5E5_README/#dataset-characteristics","title":"Dataset Characteristics","text":"<ul> <li>Spatial Resolution: 0.5\u00b0 \u00d7 0.5\u00b0 (approximately 50 km)</li> <li>Temporal Resolution: Daily</li> <li>Temporal Coverage: 1979 - present</li> <li>Global Coverage: Complete global coverage</li> <li>Data Source: ISIMIP3a (observational/historical climate)</li> </ul>"},{"location":"W5E5_README/#available-variables","title":"Available Variables","text":"Variable Name Description Units <code>tas</code> Near-Surface Air Temperature Mean daily temperature at 2m height K <code>tasmax</code> Maximum Temperature Daily maximum temperature at 2m height K <code>tasmin</code> Minimum Temperature Daily minimum temperature at 2m height K <code>pr</code> Precipitation Total daily precipitation kg m\u207b\u00b2 s\u207b\u00b9 <code>rsds</code> Shortwave Radiation Surface downwelling shortwave radiation W m\u207b\u00b2 <code>rlds</code> Longwave Radiation Surface downwelling longwave radiation W m\u207b\u00b2 <code>hurs</code> Relative Humidity Near-surface relative humidity % <code>sfcWind</code> Wind Speed Near-surface wind speed m s\u207b\u00b9 <code>ps</code> Surface Pressure Surface air pressure Pa <code>huss</code> Specific Humidity Near-surface specific humidity 1"},{"location":"W5E5_README/#installation","title":"Installation","text":"<p>To use the W5E5 dataset, you need to install the <code>isimip-client</code> package:</p> <pre><code>pip install isimip-client\n</code></pre> <p>Or if installing climdata from the updated requirements:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"W5E5_README/#basic-usage","title":"Basic Usage","text":""},{"location":"W5E5_README/#1-using-the-configuration-system","title":"1. Using the Configuration System","text":"<pre><code>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\n\n# Load configuration\ncfg = load_config(\n    config_name='config',\n    overrides=[\n        'dataset=w5e5',\n        'lat=52.52',  # Berlin, Germany\n        'lon=13.405',\n        'variables=[tas,tasmax,tasmin,pr,rsds]',\n        'time_range.start_date=2010-01-01',\n        'time_range.end_date=2010-12-31',\n        'data_dir=./data'\n    ]\n)\n\n# Initialize and fetch data\nw5e5 = W5E5(cfg)\nw5e5.fetch()  # Download from ISIMIP\nw5e5.load()   # Load into xarray\nw5e5.extract(point=(cfg.lon, cfg.lat))  # Extract for location\n\n# Save results\nw5e5.save_csv('output.csv')\nw5e5.save_netcdf('output.nc')\n</code></pre>"},{"location":"W5E5_README/#2-direct-instantiation","title":"2. Direct Instantiation","text":"<pre><code>from omegaconf import OmegaConf\nfrom climdata.datasets.W5E5 import W5E5\n\n# Create configuration manually\ncfg = OmegaConf.create({\n    'dataset': 'w5e5',\n    'lat': 40.7128,  # New York\n    'lon': -74.0060,\n    'variables': ['tas', 'pr'],\n    'time_range': {\n        'start_date': '2015-01-01',\n        'end_date': '2015-12-31'\n    },\n    'data_dir': './w5e5_data'\n})\n\nw5e5 = W5E5(cfg)\nw5e5.fetch()\nw5e5.load()\nw5e5.extract(point=(cfg.lon, cfg.lat))\n</code></pre>"},{"location":"W5E5_README/#advanced-usage","title":"Advanced Usage","text":""},{"location":"W5E5_README/#spatial-extraction-options","title":"Spatial Extraction Options","text":""},{"location":"W5E5_README/#point-extraction","title":"Point Extraction","text":"<pre><code># Extract for a single point\nw5e5.extract(point=(lon, lat))\n\n# Extract with buffer (average over surrounding area)\nw5e5.extract(point=(lon, lat), buffer_km=50)\n</code></pre>"},{"location":"W5E5_README/#bounding-box-extraction","title":"Bounding Box Extraction","text":"<pre><code># Extract for a rectangular region\nw5e5.extract(box={\n    'lon_min': 10.0,\n    'lon_max': 15.0,\n    'lat_min': 50.0,\n    'lat_max': 55.0\n})\n</code></pre>"},{"location":"W5E5_README/#shapefile-extraction","title":"Shapefile Extraction","text":"<pre><code>import geopandas as gpd\n\n# Extract for a shapefile region\ngdf = gpd.read_file('region.shp')\nw5e5.extract(shapefile=gdf)\n</code></pre>"},{"location":"W5E5_README/#working-with-the-xarray-dataset","title":"Working with the xarray Dataset","text":"<pre><code># Access the loaded dataset\nds = w5e5.ds\n\n# Convert to pandas DataFrame\ndf = ds.to_dataframe()\n\n# Unit conversions\ndf['tas_celsius'] = df['tas'] - 273.15  # K to \u00b0C\ndf['pr_mm_day'] = df['pr'] * 86400      # kg/m\u00b2/s to mm/day\n\n# Basic statistics\nprint(df.describe())\n\n# Plotting\nimport matplotlib.pyplot as plt\ndf['tas_celsius'].plot(title='Temperature Time Series')\nplt.show()\n</code></pre>"},{"location":"W5E5_README/#data-access-details","title":"Data Access Details","text":""},{"location":"W5E5_README/#isimip-repository-structure","title":"ISIMIP Repository Structure","text":"<p>W5E5 data is organized in the ISIMIP repository as: <pre><code>ISIMIP3a/InputData/climate/atmosphere/obsclim/global/daily/historical/w5e5v2.0/\n</code></pre></p> <p>The W5E5 class automatically: 1. Searches for datasets matching your criteria 2. Downloads relevant files covering your time range 3. Caches files locally to avoid re-downloading 4. Loads and merges multiple files as needed</p>"},{"location":"W5E5_README/#file-naming-convention","title":"File Naming Convention","text":"<p>W5E5 files follow this pattern: <pre><code>w5e5v2.0_obsclim_{variable}_global_daily_{start_year}_{end_year}.nc\n</code></pre></p> <p>For example: <pre><code>w5e5v2.0_obsclim_tas_global_daily_2010_2019.nc\n</code></pre></p>"},{"location":"W5E5_README/#performance-tips","title":"Performance Tips","text":"<ol> <li>Download once: Downloaded files are cached in <code>data_dir/w5e5/</code></li> <li>Subset early: Extract for your region immediately after loading to reduce memory usage</li> <li>Use appropriate time ranges: W5E5 files cover multi-year periods, so choose ranges that minimize file downloads</li> <li>Parallel processing: The <code>load()</code> method uses dask for parallel file reading</li> </ol>"},{"location":"W5E5_README/#comparison-with-other-datasets","title":"Comparison with Other Datasets","text":"Feature W5E5 ERA5 MSWX NASA POWER Resolution 0.5\u00b0 0.25\u00b0 0.1\u00b0 0.5\u00b0 Start Year 1979 1950 1979 1981 Coverage Global Global Global Global Variables 10+ 100+ 7 6+ Update Freq Annual Monthly Annual Near real-time Quality High Very High High Good"},{"location":"W5E5_README/#when-to-use-w5e5","title":"When to Use W5E5","text":"<ul> <li>\u2705 Need bias-corrected ERA5 data</li> <li>\u2705 Working with ISIMIP climate impact models</li> <li>\u2705 Require consistent land-ocean dataset</li> <li>\u2705 0.5\u00b0 resolution is sufficient</li> <li>\u2705 Post-1979 period is adequate</li> </ul>"},{"location":"W5E5_README/#when-to-use-alternatives","title":"When to Use Alternatives","text":"<ul> <li>Use ERA5 if you need higher resolution (0.25\u00b0) or more variables</li> <li>Use MSWX if you need higher resolution (0.1\u00b0) and focus on precipitation</li> <li>Use NASA POWER if you need near real-time data or solar energy applications</li> </ul>"},{"location":"W5E5_README/#troubleshooting","title":"Troubleshooting","text":""},{"location":"W5E5_README/#installation-issues","title":"Installation Issues","text":"<pre><code># If isimip-client installation fails\npip install --upgrade pip\npip install isimip-client\n\n# Or use conda\nconda install -c conda-forge isimip-client\n</code></pre>"},{"location":"W5E5_README/#download-issues","title":"Download Issues","text":"<pre><code># Check ISIMIP API status\nfrom isimip_client.client import ISIMIPClient\nclient = ISIMIPClient()\nresponse = client.datasets(query='w5e5')\nprint(f\"Found {len(response.get('results', []))} datasets\")\n</code></pre>"},{"location":"W5E5_README/#memory-issues","title":"Memory Issues","text":"<pre><code># Process data in chunks for large regions\nw5e5.load()\n# Extract smaller region first\nw5e5.extract(box={'lon_min': 10, 'lon_max': 11, \n                   'lat_min': 50, 'lat_max': 51})\n</code></pre>"},{"location":"W5E5_README/#references","title":"References","text":"<ol> <li> <p>W5E5 Dataset: Lange, S. (2019). WFDE5 over land merged with ERA5 over the ocean (W5E5). V. 1.0. DOI: 10.5880/pik.2019.023</p> </li> <li> <p>ISIMIP Project: https://www.isimip.org/</p> </li> <li> <p>ISIMIP Data Repository: https://data.isimip.org/</p> </li> <li> <p>ISIMIP Client: https://github.com/ISI-MIP/isimip-client</p> </li> </ol>"},{"location":"W5E5_README/#example-notebooks","title":"Example Notebooks","text":"<p>See the following example notebooks: - <code>docs/examples/w5e5_example.ipynb</code> - Complete usage example</p>"},{"location":"W5E5_README/#support","title":"Support","text":"<p>For issues related to: - W5E5 implementation: Open an issue in the climdata repository - ISIMIP data access: Visit ISIMIP support - Data quality/methodology: See W5E5 documentation at PIK</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#climdata.utils.wrapper_workflow.ClimData","title":"<code> climdata.utils.wrapper_workflow.ClimData        </code>","text":"<p>Climate data extraction and extreme-index workflow manager.</p> <p>Provides a high-level API for:     - loading/configuring dataset providers via Hydra config     - uploading NetCDF/CSV content into xarray Datasets     - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER)     - computing extreme indices using configured xclim indices     - converting datasets to long-form DataFrames and saving results</p> <p>Attributes:</p> Name Type Description <code>cfg</code> <code>DictConfig</code> <p>Hydra configuration object describing dataset, region/time/variables, outputs.</p> <code>current_ds</code> <code>xr.Dataset</code> <p>The most recently loaded or extracted dataset.</p> <code>current_df</code> <code>pd.DataFrame</code> <p>The most recently produced long-form DataFrame.</p> <code>filename_csv/filename_nc/filename_zarr</code> <code>str</code> <p>Generated output filename templates/paths.</p> <p>Examples:</p> <p>extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe']) extractor.extract() idx_ds = extractor.calc_index() df = extractor.to_dataframe(idx_ds) extractor.to_csv(df)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>class ClimateExtractor:\n    \"\"\"Climate data extraction and extreme-index workflow manager.\n\n    Provides a high-level API for:\n        - loading/configuring dataset providers via Hydra config\n        - uploading NetCDF/CSV content into xarray Datasets\n        - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER)\n        - computing extreme indices using configured xclim indices\n        - converting datasets to long-form DataFrames and saving results\n\n    Attributes:\n        cfg (DictConfig): Hydra configuration object describing dataset, region/time/variables, outputs.\n        current_ds (xr.Dataset): The most recently loaded or extracted dataset.\n        current_df (pd.DataFrame): The most recently produced long-form DataFrame.\n        filename_csv/filename_nc/filename_zarr (str): Generated output filename templates/paths.\n\n    Example:\n        extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe'])\n        extractor.extract()\n        idx_ds = extractor.calc_index()\n        df = extractor.to_dataframe(idx_ds)\n        extractor.to_csv(df)\n    \"\"\"\n\n    def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n        \"\"\"Initialize the workflow manager and load configuration.\n\n        Args:\n            cfg_name (str): Name of the Hydra configuration (default: \"config\").\n            conf_path (str, optional): Optional config path override.\n            overrides (list[str], optional): Hydra overrides to apply to the configuration.\n        \"\"\"\n        self.cfg_name = cfg_name\n        self.conf_path = conf_path\n        self.cfg: Optional[DictConfig] = None\n\n        # Stage datasets\n        self.ds = None\n        self.current_ds = None\n        self.index_ds = None\n        self.impute_ds = None\n        self.bias_corrected_ds = None\n\n        # Stage DataFrames\n        self.raw_df = None\n        self.current_df = None\n        self.index_df = None\n        self.impute_df = None\n        self.bias_corrected_df = None\n        self.df = None  # alias for current_df\n\n        # filenames\n        self.filename = None\n        self.filetype = None\n\n        # Automatically load config on init\n        self.load_config(overrides)\n        self.cfg = self.preprocess_aoi(self.cfg)\n\n        # instance logger for this extractor\n        self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n    def _gen_fn(self, *, ds: xr.Dataset = None, df: pd.DataFrame = None):\n        \"\"\"Create filenames (csv, nc, zarr) using config templates and dataset metadata.\n\n        Accepts either:\n        - ds : xarray.Dataset\n        - df : pandas.DataFrame (long form with columns: lat, lon, time/date, variable, value, optional source)\n\n        Exactly one must be provided (keyword-only).\n        \"\"\"\n\n        # ------------------------\n        # Validate inputs\n        # ------------------------\n        if (ds is None) == (df is None):\n            raise ValueError(\"Provide exactly one of `ds` or `df` as a keyword argument.\")\n\n        # ------------------------\n        # Helper: find coord alias (xarray)\n        # ------------------------\n        def find_coord(ds, names):\n            for name in names:\n                if name in ds.coords:\n                    return ds[name]\n            return None\n\n        # ------------------------\n        # Case 1: xarray.Dataset\n        # ------------------------\n        if ds is not None:\n            lat = find_coord(ds, [\"lat\", \"latitude\"])\n            lon = find_coord(ds, [\"lon\", \"longitude\"])\n            time = find_coord(ds, [\"time\", \"date\"])\n\n            provider = ds.attrs.get(\"source\", \"unknown\")\n            vars_list = list(ds.data_vars)\n            parameter = vars_list[0] if len(vars_list) == 1 else \"_\".join(vars_list)\n\n            # Latitude range\n            if lat is not None:\n                lat_vals = lat.values.reshape(-1)\n                lat_min, lat_max = float(lat_vals.min()), float(lat_vals.max())\n            else:\n                lat_min = lat_max = None\n\n            # Longitude range\n            if lon is not None:\n                lon_vals = lon.values.reshape(-1)\n                lon_min, lon_max = float(lon_vals.min()), float(lon_vals.max())\n            else:\n                lon_min = lon_max = None\n\n            # Time range\n            if time is not None:\n                tvals = pd.to_datetime(time.values)\n                start, end = tvals.min().strftime(\"%Y-%m-%d\"), tvals.max().strftime(\"%Y-%m-%d\")\n            else:\n                start = end = \"unknown\"\n\n        # ------------------------\n        # Case 2: pandas.DataFrame (long form)\n        # ------------------------\n        else:\n            cols = df.columns.astype(str)\n\n            # Identify coordinate columns\n            lat_cols = [c for c in cols if \"lat\" in c.lower()]\n            lon_cols = [c for c in cols if \"lon\" in c.lower()]\n            time_cols = [c for c in cols if \"time\" in c.lower() or \"date\" in c.lower()]\n\n            # Provider from 'source' column\n            if \"source\" in df.columns:\n                unique_sources = df[\"source\"].dropna().unique()\n                provider = unique_sources[0] if len(unique_sources) == 1 else \"_\".join(map(str, unique_sources))\n            else:\n                provider = \"unknown\"\n\n            # Unique parameters from 'variable' column\n            if \"variable\" in df.columns:\n                unique_parameters = sorted(df[\"variable\"].dropna().unique())\n                parameter = unique_parameters[0] if len(unique_parameters) == 1 else \"_\".join(unique_parameters)\n            else:\n                parameter = \"unknown\"\n\n            # Latitude range\n            if lat_cols:\n                lat_vals = pd.to_numeric(df[lat_cols[0]], errors=\"coerce\")\n                lat_min, lat_max = float(lat_vals.min()), float(lat_vals.max())\n            else:\n                lat_min = lat_max = None\n\n            # Longitude range\n            if lon_cols:\n                lon_vals = pd.to_numeric(df[lon_cols[0]], errors=\"coerce\")\n                lon_min, lon_max = float(lon_vals.min()), float(lon_vals.max())\n            else:\n                lon_min = lon_max = None\n\n            # Time range\n            if time_cols:\n                tvals = pd.to_datetime(df[time_cols[0]], errors=\"coerce\")\n                start = tvals.min().strftime(\"%Y-%m-%d\")\n                end = tvals.max().strftime(\"%Y-%m-%d\")\n            else:\n                start = end = \"unknown\"\n\n        # ------------------------\n        # Format lat/lon strings\n        # ------------------------\n        if lat_min is None:\n            lat_str = lat_range = \"unknown\"\n        elif lat_min == lat_max:\n            lat_str = lat_range = f\"{lat_min}\"\n        else:\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n\n        if lon_min is None:\n            lon_str = lon_range = \"unknown\"\n        elif lon_min == lon_max:\n            lon_str = lon_range = f\"{lon_min}\"\n        else:\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # ------------------------\n        # Build filenames\n        # ------------------------\n        outdir = Path(self.cfg.output.out_dir)\n        outdir.mkdir(parents=True, exist_ok=True)\n        def build(fn_template):\n            return fn_template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range,\n                lon_range=lon_range,\n            )\n\n        self.filename_csv = str(outdir / build(self.cfg.output.filename_csv))\n        self.filename_nc = str(outdir / build(self.cfg.output.filename_nc))\n        self.filename_zarr = str(outdir / build(self.cfg.output.filename_zarr))\n        return self\n    def _gen_fn_cfg(self):\n        \"\"\"Generate output filenames using configuration and extracted dataset metadata.\n\n        Uses settings from ``self.cfg`` and ``self.current_ds`` (if available) to build filename templates.\n        \"\"\"\n\n        cfg = self.cfg\n        out = cfg.output\n        provider = cfg.dataset.lower()\n        if self.current_ds:\n            if len(self.current_ds.data_vars) == 0:\n                parameter = \"unknown\"\n            elif len(self.current_ds.data_vars) == 1:\n                parameter = next(iter(self.current_ds.data_vars))\n            else:\n                parameter = \"_\".join(self.current_ds.data_vars)\n        else:\n            parameter = \"_\".join(self.cfg.variables)\n        # --------------------------------\n        # Determine lat/lon values\n        # --------------------------------\n        if cfg.lat is not None and cfg.lon is not None:\n            lat_range = lon_range = None   # single point\n            lat_str = str(cfg.lat)\n            lon_str = str(cfg.lon)\n        else:\n            b = cfg.bounds[cfg.region]\n            lat_min, lat_max = b[\"lat_min\"], b[\"lat_max\"]\n            lon_min, lon_max = b[\"lon_min\"], b[\"lon_max\"]\n\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # --------------------------------\n        # Time range from cfg\n        # --------------------------------\n        start = pd.to_datetime(cfg.time_range.start_date).strftime(\"%Y-%m-%d\")\n        end = pd.to_datetime(cfg.time_range.end_date).strftime(\"%Y-%m-%d\")\n\n        # --------------------------------\n        # Format filenames\n        # --------------------------------\n        def format_template(template):\n            return template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range or lat_str,\n                lon_range=lon_range or lon_str,\n            )\n\n        out_dir = Path(self.cfg.output.out_dir)\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        self.filename_csv = str(out_dir / format_template(out.filename_csv))\n        self.filename_nc = str(out_dir / format_template(out.filename_nc))\n        self.filename_zarr = str(out_dir / format_template(out.filename_zarr))\n\n    # ----------------------------\n    # Hydra config\n    # ----------------------------\n    def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n        \"\"\"Load and compose the Hydra configuration.\n\n        Args:\n            overrides (list[str], optional): Hydra overrides to apply when composing the configuration.\n\n        Returns:\n            DictConfig: Composed Hydra configuration object stored on ``self.cfg``.\n        \"\"\"\n        overrides = overrides or []\n        conf_dir = _ensure_local_conf()\n        rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n        if not GlobalHydra.instance().is_initialized():\n            hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n        else:\n            hydra_ctx = None\n\n        if hydra_ctx:\n            with hydra_ctx:\n                self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        else:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        return self.cfg\n\n    # ----------------------------\n    # AOI preprocessing\n    # ----------------------------\n    def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n        \"\"\"Process an 'aoi' specification in the configuration.\n\n        Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).\n\n        Args:\n            cfg (DictConfig): Configuration object with optional ``aoi`` entry.\n\n        Returns:\n            DictConfig: The modified configuration. When a Point is provided, ``cfg.lat`` and ``cfg.lon`` are set; when a Polygon is provided, ``cfg.bounds`` is set and ``cfg.region`` is set to \"custom\".\n        \"\"\"\n        if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n            return cfg\n\n        if isinstance(cfg.aoi, str):\n            try:\n                cfg.aoi = json.loads(cfg.aoi)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid AOI JSON string\")\n\n        aoi = cfg.aoi\n\n        if aoi.get(\"type\") == \"FeatureCollection\":\n            geom = shape(aoi[\"features\"][0][\"geometry\"])\n        elif aoi.get(\"type\") == \"Feature\":\n            geom = shape(aoi[\"geometry\"])\n        elif \"type\" in aoi:\n            geom = shape(aoi)\n        else:\n            raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n        if isinstance(geom, Point):\n            cfg.lat = geom.y\n            cfg.lon = geom.x\n            cfg.bounds = None\n        elif isinstance(geom, Polygon):\n            minx, miny, maxx, maxy = geom.bounds\n            cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                     \"lon_min\": minx, \"lon_max\": maxx}}\n            cfg.region = \"custom\"\n            cfg.lat = None\n            cfg.lon = None\n        else:\n            raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n        return cfg\n\n    # ----------------------------\n    # Upload NetCDF\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n        \"\"\"Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n        Args:\n            nc_file (str): Path to the NetCDF file to open.\n\n        Returns:\n            xr.Dataset: The loaded dataset (also sets ``self.current_ds``).\n        \"\"\"\n        if not os.path.exists(nc_file):\n            raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n        ds = xr.open_dataset(nc_file)\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                                for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Upload CSV \u2192 xarray.Dataset\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n        \"\"\"Load a long-form CSV into an xarray.Dataset.\n\n        The CSV must contain ``time`` and ``lat``/``latitude``, ``lon``/``longitude``, ``variable``, ``value``. Units may be supplied in a ``units`` column and an optional ``source`` column is recognized.\n\n        Args:\n            csv_file (str): Path to the CSV file to load.\n\n        Returns:\n            xr.Dataset: The converted dataset (also sets ``self.current_ds``).\n        \"\"\"\n        if not os.path.exists(csv_file):\n            raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n        df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n        lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n        lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n        if lat_col is None or lon_col is None:\n            raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n        id_vars = [\"time\", lat_col, lon_col]\n        df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n        ds = df_wide.set_index(id_vars).to_xarray()\n\n        # Attach units from CSV\n        for var in ds.data_vars:\n            units_series = df[df[\"variable\"] == var][\"units\"]\n            ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n        # Global source attribute\n        if \"source\" in df.columns:\n            source_series = df[\"source\"].dropna().unique()\n            if len(source_series) &gt; 0:\n                ds.attrs[\"source\"] = source_series[0]\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Extract data from datasets like CMIP, DWD, etc.\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def extract(self) -&gt; xr.Dataset:\n        \"\"\"Extract data from the configured provider using ``self.cfg``.\n\n        Uses provider-specific classes (e.g., ``CMIP``, ``DWD``, ``MSWX``, ``HYRAS``, ``POWER``)\n        to fetch, load and extract datasets. When extraction completes, units are converted to those declared in ``cfg.varinfo``, the dataset is computed, and filenames are generated from the configuration.\n\n        Returns:\n            xr.Dataset: The extracted and computed dataset (also sets ``self.current_ds``).\n        \"\"\"\n        cfg = self.cfg\n        extract_kwargs = {}\n\n        if cfg.lat is not None and cfg.lon is not None:\n            extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n            if cfg.dataset == \"dwd\":\n                extract_kwargs[\"buffer_km\"] = 30\n        elif cfg.region is not None:\n            extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n        elif cfg.shapefile is not None:\n            extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n        ds = None\n        dataset_upper = cfg.dataset.upper()\n\n        if dataset_upper == \"MSWX\":\n            ds_vars = []\n            for var in cfg.variables:\n                mswx = climdata.MSWX(cfg)\n                mswx.extract(**extract_kwargs)\n                mswx.load(var)\n                ds_vars.append(mswx.dataset)\n            ds = xr.merge(ds_vars)\n            self.dataset_class = mswx\n        elif dataset_upper == \"CMIP\":\n            cmip = climdata.CMIP(cfg)\n            cmip.fetch()\n            cmip.load()\n            cmip.extract(**extract_kwargs)\n            ds = cmip.ds\n            self.dataset_class = cmip\n        elif dataset_upper == \"POWER\":\n            power = climdata.POWER(cfg)\n            power.fetch()\n            power.load()\n            ds = power.ds\n            self.dataset_class = power\n        elif dataset_upper == \"DWD\":\n            ds_vars = []\n            for var in cfg.variables:\n                dwd = climdata.DWD(cfg)\n                ds_var = dwd.extract(variable=var, **extract_kwargs)\n                ds_vars.append(ds_var)\n            ds = xr.merge(ds_vars)\n            self.dataset_class = dwd\n        elif dataset_upper == \"HYRAS\":\n            hyras = climdata.HYRAS(cfg)\n            ds_vars = []\n            for var in cfg.variables:\n                hyras.extract(**extract_kwargs)\n                ds_vars.append(hyras.load(var, chunking={'time':\"auto\"})[[var]])\n            ds = xr.merge(ds_vars, compat=\"override\")\n            self.dataset_class = hyras\n        elif dataset_upper == \"W5E5\":\n            w5e5 = climdata.W5E5(cfg)\n            w5e5.fetch()  # Download from ISIMIP\n            w5e5.load()   # Load into xarray\n            w5e5.extract(**extract_kwargs)\n            ds = w5e5.ds\n            self.dataset_class = w5e5\n        elif dataset_upper == \"CMIP_W5E5\":\n            cmip_w5e5 = climdata.CMIPW5E5(cfg)\n            cmip_w5e5.fetch()  # Download CMIP6 data from ISIMIP\n            cmip_w5e5.load()   # Load into xarray\n            cmip_w5e5.extract(**extract_kwargs)\n            ds = cmip_w5e5.ds\n            self.dataset_class = cmip_w5e5\n        elif dataset_upper == \"NEXGDDP\":\n            nexgddp = climdata.NEXGDDP(cfg)\n            nexgddp.fetch()  # Download NEX-GDDP-CMIP6 data from NASA THREDDS\n            nexgddp.load()   # Load into xarray\n            nexgddp.extract(**extract_kwargs)\n            ds = nexgddp.ds\n            self.dataset_class = nexgddp\n        for var in ds.data_vars:\n            ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n        # ds = ds.compute()\n\n        return ds\n    # ----------------------------\n    # Compute extreme index\n    # ----------------------------\n    @update_ds(attr_name='index_ds')\n    def calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"Calculate the configured extreme index using xclim indices.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to operate on. If ``None``, ``self.current_ds`` is used.\n\n        Returns:\n            xr.Dataset: The computed index as an xarray Dataset (also sets ``self.index_ds``).\n        \"\"\"\n        cfg = self.cfg\n\n        # Use provided ds or fallback\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.index is None:\n            self.logger.info(\"No index selected.\")\n            return None\n\n        if \"time\" in ds.coords:\n            years = pd.to_datetime(ds.time.values).year\n            n_years = len(pd.unique(years))\n            if n_years &lt; 30:\n                warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n        indices = extreme_index(cfg, ds)\n        index_ds = indices.calculate(cfg.index).compute()\n        index_ds = index_ds.to_dataset(name=cfg.index)\n\n        return index_ds\n    # ----------------------------\n    # Dataset \u2192 Long-form DataFrame\n    # ----------------------------\n    @update_df()\n    def to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n        \"\"\"Convert a dataset to a long-form pandas DataFrame.\n\n        The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to convert. If ``None``, uses ``self.current_ds``.\n\n        Returns:\n            pd.DataFrame: Long-form DataFrame (also sets ``self.current_df``).\n        \"\"\"\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        df = ds.to_dataframe().reset_index()\n\n        id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n        value_vars = [v for v in ds.data_vars if v in df.columns]\n\n        if not value_vars:\n            raise ValueError(\"No variables in dataset available to melt into long format\")\n\n        df_long = df.melt(\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"variable\",\n            value_name=\"value\"\n        )\n\n        df_long[\"units\"] = df_long[\"variable\"].apply(\n            lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n        )\n        if getattr(self.cfg, \"dataset\") == 'cmip':\n            df_long[\"source_id\"] = getattr(self.cfg, \"source_id\")\n        df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n        df_long = df_long.drop_duplicates()\n        self._gen_fn_cfg()\n        return df_long\n\n    # ----------------------------\n    # Save CSV\n    # ----------------------------\n    def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"Save a DataFrame to CSV.\n\n        Args:\n            df (pd.DataFrame, optional): DataFrame to save. Defaults to ``self.current_df``.\n            filename (str, optional): Output filename. Defaults to ``self.filename_csv``.\n\n        Returns:\n            str: The path of the written CSV file.\n        \"\"\"\n        df = df if df is not None else self.current_df\n\n        filename = filename or getattr(self, \"filename_csv\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_csv is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        df.to_csv(filename, index=False)\n        self.filename_csv = str(path)\n        self.current_filename = str(path)\n\n        # print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n        self.logger.info(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n        return filename\n\n    def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"Save an xarray Dataset to NetCDF.\n\n        Notes:\n            - If ``ds`` is ``None``: save ``current_ds``.\n            - If ``filename`` is ``None``: use ``self.filename_nc``.\n            - Creates directories if needed and updates ``self.filename_nc`` and ``self.current_filename``.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to save. If ``None``, uses ``self.current_ds``.\n            filename (str, optional): Output filename. Defaults to ``self.filename_nc``.\n\n        Returns:\n            str: The path of the written NetCDF file.\n        \"\"\"\n\n        # -------------------------------\n        # 1. Determine dataset to save\n        # -------------------------------\n        ds = ds or getattr(self, \"current_ds\", None)\n        if ds is None:\n            raise ValueError(\"No dataset available to save\")\n\n        # -------------------------------\n        # 2. Determine filename\n        # -------------------------------\n        filename = filename or getattr(self, \"filename_nc\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_nc is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # -------------------------------\n        # 3. Save to NetCDF\n        # -------------------------------\n        ds.to_netcdf(path)\n\n        # -------------------------------\n        # 4. Track filenames\n        # -------------------------------\n        self.filename_nc = str(path)\n        self.current_filename = str(path)\n\n        # print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n        self.logger.info(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n        return str(path)\n\n    # ----------------------------\n    # Unified workflow\n    # ----------------------------\n    def run_workflow(self, overrides: Optional[List[str]] = None,\n                     actions: Optional[List[str]] = None,\n                     file: Optional[str] = None) -&gt; WorkflowResult:\n        \"\"\"Execute a sequence of workflow actions.\n\n        Args:\n            overrides (list[str], optional): Hydra overrides to apply (not all actions will use these).\n            actions (list[str], optional): Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.\n            file (str, optional): File path used for upload actions when required.\n\n        Returns:\n            WorkflowResult: Named result container with populated fields for dataframe/dataset/filenames.\n        \"\"\"\n        actions = actions or [\"extract\", \"calc_index\", \"to_csv\", \"to_nc\"]\n        result = WorkflowResult(cfg=self.cfg)\n        for action in actions:\n            self.logger.info(\"Starting action: %s\", action)\n            try:\n                if action == \"upload_netcdf\":\n                    if file is None:\n                        raise ValueError(\n                            \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                            \"but none was provided.\"\n                        )\n                        # Validate extension\n                    valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                    if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                        raise ValueError(\n                            f\"Invalid file format for upload_netcdf: '{file}'. \"\n                            f\"Expected one of: {valid_nc_ext}\"\n                        )\n                    self.upload_netcdf(file)\n                    result.dataset = self.current_ds\n\n                elif action == \"upload_csv\":\n                    if file is None:\n                        raise ValueError(\n                            \"Action 'upload_csv' requires argument 'csv_file', \"\n                            \"but none was provided.\"\n                        )\n\n                    # Validate CSV extension\n                    valid_csv_ext = (\".csv\", \".csv.gz\")\n                    if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                        raise ValueError(\n                            f\"Invalid file format for upload_csv: '{file}'. \"\n                            f\"Expected one of: {valid_csv_ext}\"\n                        )\n\n                    self.upload_csv(file)\n                    result.dataset = self.current_ds\n\n                elif action == \"extract\":\n                    if self.cfg.dataset is None:\n                        raise ValueError(\n                            \"Action 'extract' cannot run because no dataset provider is set \"\n                            \"(cfg.dataset is None).\"\n                        )\n                    self.extract()\n                    result.dataset = self.current_ds\n\n                elif action == \"calc_index\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before computing an index.\"\n                        )\n                    self.calc_index()\n                    result.index_ds = self.current_ds\n\n                elif action == \"to_csv\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before converting to a DataFrame.\"\n                        )\n                    self.to_dataframe()\n                    result.dataframe = self.current_df\n                    result.filename = self.to_csv()\n\n                elif action == \"to_nc\":\n                    if self.current_ds is None:\n                        raise ValueError(\n                            \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                            \"Upload or extract a dataset before saving to NetCDF.\"\n                        )\n                    result.filename = self.to_nc()\n\n                elif action == \"impute\":\n                    if self.current_ds is None:\n                        raise ValueError(\"Action 'impute' requires a dataset, but no dataset is available.\")\n                    self.impute()\n                    result.dataset = self.current_ds\n                    result.impute_ds = getattr(self, \"impute_ds\", None)\n\n                else:\n                    raise ValueError(f\"Unknown action '{action}'\")\n                self.logger.info(\"Completed action: %s\", action)\n            except Exception:\n                self.logger.exception(\"Action '%s' failed\", action)\n                raise\n\n        return result\n\n    # ----------------------------\n    # Exploration helpers using cfg.dsinfo\n    # ----------------------------\n    def get_datasets(self) -&gt; List[str]:\n        \"\"\"Return the list of dataset provider names available in configuration.\n\n        Returns:\n            List[str]: Names of available dataset providers from ``cfg.dsinfo``.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n        return list(self.cfg.dsinfo.keys())\n\n    def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n        \"\"\"Return the list of variables available for a dataset.\n\n        Args:\n            dataset (str, optional): Dataset name to query. Defaults to ``cfg.dataset``.\n\n        Returns:\n            List[str]: List of variable names.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n\n        dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n        if dataset_name is None:\n            raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n        dsinfo = self.cfg.dsinfo.get(dataset_name)\n        if not dsinfo or \"variables\" not in dsinfo:\n            raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n        return list(dsinfo[\"variables\"].keys())\n\n    def get_varinfo(self, var: str) -&gt; dict:\n        \"\"\"Get metadata for a variable from varinfo.\n\n        Args:\n            var (str): Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n        Returns:\n            dict: Metadata dictionary containing cf_name, long_name, units, etc.\n\n        Raises:\n            ValueError: If varinfo is not loaded or variable not found.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            raise ValueError(\"Configuration or varinfo not loaded\")\n\n        if var not in self.cfg.varinfo:\n            raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n        return self.cfg.varinfo[var]\n\n\n    def get_actions(self) -&gt; dict:\n        \"\"\"Return a dictionary of workflow actions with their outputs and descriptions.\n\n        Supports ``actionsinfo`` in mapping style or list style and returns a consistent mapping of action name to description/output.\n\n        Returns:\n            dict: Mapping action name -&gt; {'output': ..., 'description': ...}\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n            raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n        actions_map = getattr(self.cfg, \"actionsinfo\")\n\n        # If 'actions' key exists, fallback to list style\n        if \"actions\" in actions_map:\n            actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                        for a in actions_map[\"actions\"]}\n\n        return actions_map\n    def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n        \"\"\"Fetch climate extreme indices from ``cfg.extinfo`` that involve the given variables.\n\n        Args:\n            variables (list[str]): Variables to filter indices by (if ``None``, uses ``cfg.variables``).\n            require_all (bool): If True, return indices that require all provided variables; otherwise return indices if any variable matches.\n\n        Returns:\n            dict: Mapping index_name -&gt; index_definition.\n        \"\"\"\n        cfg = self.cfg\n        variables = variables or cfg.variables \n        if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n            raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n        indices_def = cfg.extinfo.get(\"indices\", {})\n        if not indices_def:\n            return {}\n\n        matched_indices = {}\n        for idx_name, idx_info in indices_def.items():\n            idx_vars = idx_info.get(\"variables\", [])\n            if require_all:\n                if all(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n            else:\n                if any(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n\n        return matched_indices\n\n    # ----------------------------\n    # Imputation\n    # ----------------------------\n    @update_ds(attr_name='impute_ds')\n    def impute(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"Impute missing values using the configured imputation method.\n\n        Args:\n            ds (xr.Dataset, optional): Dataset to impute. If None, uses\n                ``self.current_ds``.\n\n        Returns:\n            xr.Dataset | None: The imputed dataset (also sets\n                ``self.current_ds`` and ``self.impute_ds``). Returns ``None``\n                if no imputation method is configured.\n\n        Raises:\n            ValueError: If ``ds`` is ``None`` and ``self.current_ds`` is not set.\n        \"\"\"\n        cfg = self.cfg\n        impute_cfg = cfg.imputeinfo\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.impute is None:\n            self.logger.warning(\"No imputation method selected.\")\n            return None\n        # select variables (optional)\n        # variables = cfg.get(\"variables\", None)\n        # if variables:\n        #     missing = [v for v in variables if v not in self.current_ds.data_vars]\n        #     if missing:\n        #         raise ValueError(f\"Variables not present in dataset: {missing}\")\n        #     ds_in = self.current_ds[variables]\n        # else:\n        #     ds_in = self.current_ds\n\n        method = cfg.impute\n        normalize = impute_cfg[method].get(\"normalize\", True)\n        time_dim = cfg.dsinfo[cfg.dataset].get(\"time_dim\", \"time\")\n        lat_dim = cfg.dsinfo[cfg.dataset].get(\"lat_dim\", \"lat\")\n        lon_dim = cfg.dsinfo[cfg.dataset].get(\"lon_dim\", \"lon\")\n        # epochs = impute_cfg[method].get(\"epochs\", 300)\n\n        # run imputer (Imputer expects dims (time, lat, lon))\n        imputer = Imputer(\n            ds,\n            time_dim=time_dim,\n            lat_dim=lat_dim,\n            lon_dim=lon_dim,\n            method=method,\n            normalize=normalize,\n        )\n        recovered = imputer.impute()\n\n        # merge imputed variables back into original dataset if we operated on a subset\n\n        ds_out = recovered\n\n        # Return dataset (decorator will set current_ds and impute_ds and generate filenames)\n        return ds_out\n\n    def get_impute_methods(self) -&gt; Dict[str, dict]:\n        \"\"\"Return mapping of available imputation methods from config.\n\n        Returns:\n            Dict[str, dict]: Mapping of method name -&gt; config (empty dict if none configured).\n        \"\"\"\n        if not hasattr(self.cfg, \"imputeinfo\") or not self.cfg.imputeinfo:\n            return {}\n        return dict(self.cfg.imputeinfo)\n\n    def configure_logging(self, level=logging.INFO, handler: logging.Handler = None):\n        \"\"\"Configure logging for this extractor instance.\n\n        Args:\n            level (int, optional): Logging level (default: ``logging.INFO``).\n            handler (logging.Handler, optional): Handler to add; if ``None``, a default StreamHandler is created.\n        \"\"\"\n        if handler is None:\n            handler = logging.StreamHandler()\n            handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n        # Avoid adding duplicate handlers\n        if not any(isinstance(h, handler.__class__) for h in self.logger.handlers):\n            self.logger.addHandler(handler)\n        self.logger.setLevel(level)\n        # also set module logger level\n        logger.setLevel(level)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.__init__","title":"<code>__init__(self, cfg_name='config', conf_path=None, overrides=None)</code>  <code>special</code>","text":"<p>Initialize the workflow manager and load configuration.</p> <p>Parameters:</p> Name Type Description Default <code>cfg_name</code> <code>str</code> <p>Name of the Hydra configuration (default: \"config\").</p> <code>'config'</code> <code>conf_path</code> <code>str</code> <p>Optional config path override.</p> <code>None</code> <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply to the configuration.</p> <code>None</code> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n    \"\"\"Initialize the workflow manager and load configuration.\n\n    Args:\n        cfg_name (str): Name of the Hydra configuration (default: \"config\").\n        conf_path (str, optional): Optional config path override.\n        overrides (list[str], optional): Hydra overrides to apply to the configuration.\n    \"\"\"\n    self.cfg_name = cfg_name\n    self.conf_path = conf_path\n    self.cfg: Optional[DictConfig] = None\n\n    # Stage datasets\n    self.ds = None\n    self.current_ds = None\n    self.index_ds = None\n    self.impute_ds = None\n    self.bias_corrected_ds = None\n\n    # Stage DataFrames\n    self.raw_df = None\n    self.current_df = None\n    self.index_df = None\n    self.impute_df = None\n    self.bias_corrected_df = None\n    self.df = None  # alias for current_df\n\n    # filenames\n    self.filename = None\n    self.filetype = None\n\n    # Automatically load config on init\n    self.load_config(overrides)\n    self.cfg = self.preprocess_aoi(self.cfg)\n\n    # instance logger for this extractor\n    self.logger = logging.getLogger(f\"{__name__}.{self.__class__.__name__}\")\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.calc_index","title":"<code>calc_index(self, ds=None)</code>","text":"<p>Calculate the configured extreme index using xclim indices.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to operate on. If <code>None</code>, <code>self.current_ds</code> is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The computed index as an xarray Dataset (also sets <code>self.index_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='index_ds')\ndef calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n    \"\"\"Calculate the configured extreme index using xclim indices.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to operate on. If ``None``, ``self.current_ds`` is used.\n\n    Returns:\n        xr.Dataset: The computed index as an xarray Dataset (also sets ``self.index_ds``).\n    \"\"\"\n    cfg = self.cfg\n\n    # Use provided ds or fallback\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    if cfg.index is None:\n        self.logger.info(\"No index selected.\")\n        return None\n\n    if \"time\" in ds.coords:\n        years = pd.to_datetime(ds.time.values).year\n        n_years = len(pd.unique(years))\n        if n_years &lt; 30:\n            warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n    indices = extreme_index(cfg, ds)\n    index_ds = indices.calculate(cfg.index).compute()\n    index_ds = index_ds.to_dataset(name=cfg.index)\n\n    return index_ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.configure_logging","title":"<code>configure_logging(self, level=20, handler=None)</code>","text":"<p>Configure logging for this extractor instance.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>Logging level (default: <code>logging.INFO</code>).</p> <code>20</code> <code>handler</code> <code>logging.Handler</code> <p>Handler to add; if <code>None</code>, a default StreamHandler is created.</p> <code>None</code> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def configure_logging(self, level=logging.INFO, handler: logging.Handler = None):\n    \"\"\"Configure logging for this extractor instance.\n\n    Args:\n        level (int, optional): Logging level (default: ``logging.INFO``).\n        handler (logging.Handler, optional): Handler to add; if ``None``, a default StreamHandler is created.\n    \"\"\"\n    if handler is None:\n        handler = logging.StreamHandler()\n        handler.setFormatter(logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"))\n    # Avoid adding duplicate handlers\n    if not any(isinstance(h, handler.__class__) for h in self.logger.handlers):\n        self.logger.addHandler(handler)\n    self.logger.setLevel(level)\n    # also set module logger level\n    logger.setLevel(level)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.extract","title":"<code>extract(self)</code>","text":"<p>Extract data from the configured provider using <code>self.cfg</code>.</p> <p>Uses provider-specific classes (e.g., <code>CMIP</code>, <code>DWD</code>, <code>MSWX</code>, <code>HYRAS</code>, <code>POWER</code>) to fetch, load and extract datasets. When extraction completes, units are converted to those declared in <code>cfg.varinfo</code>, the dataset is computed, and filenames are generated from the configuration.</p> <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The extracted and computed dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef extract(self) -&gt; xr.Dataset:\n    \"\"\"Extract data from the configured provider using ``self.cfg``.\n\n    Uses provider-specific classes (e.g., ``CMIP``, ``DWD``, ``MSWX``, ``HYRAS``, ``POWER``)\n    to fetch, load and extract datasets. When extraction completes, units are converted to those declared in ``cfg.varinfo``, the dataset is computed, and filenames are generated from the configuration.\n\n    Returns:\n        xr.Dataset: The extracted and computed dataset (also sets ``self.current_ds``).\n    \"\"\"\n    cfg = self.cfg\n    extract_kwargs = {}\n\n    if cfg.lat is not None and cfg.lon is not None:\n        extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n        if cfg.dataset == \"dwd\":\n            extract_kwargs[\"buffer_km\"] = 30\n    elif cfg.region is not None:\n        extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n    elif cfg.shapefile is not None:\n        extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n    ds = None\n    dataset_upper = cfg.dataset.upper()\n\n    if dataset_upper == \"MSWX\":\n        ds_vars = []\n        for var in cfg.variables:\n            mswx = climdata.MSWX(cfg)\n            mswx.extract(**extract_kwargs)\n            mswx.load(var)\n            ds_vars.append(mswx.dataset)\n        ds = xr.merge(ds_vars)\n        self.dataset_class = mswx\n    elif dataset_upper == \"CMIP\":\n        cmip = climdata.CMIP(cfg)\n        cmip.fetch()\n        cmip.load()\n        cmip.extract(**extract_kwargs)\n        ds = cmip.ds\n        self.dataset_class = cmip\n    elif dataset_upper == \"POWER\":\n        power = climdata.POWER(cfg)\n        power.fetch()\n        power.load()\n        ds = power.ds\n        self.dataset_class = power\n    elif dataset_upper == \"DWD\":\n        ds_vars = []\n        for var in cfg.variables:\n            dwd = climdata.DWD(cfg)\n            ds_var = dwd.extract(variable=var, **extract_kwargs)\n            ds_vars.append(ds_var)\n        ds = xr.merge(ds_vars)\n        self.dataset_class = dwd\n    elif dataset_upper == \"HYRAS\":\n        hyras = climdata.HYRAS(cfg)\n        ds_vars = []\n        for var in cfg.variables:\n            hyras.extract(**extract_kwargs)\n            ds_vars.append(hyras.load(var, chunking={'time':\"auto\"})[[var]])\n        ds = xr.merge(ds_vars, compat=\"override\")\n        self.dataset_class = hyras\n    elif dataset_upper == \"W5E5\":\n        w5e5 = climdata.W5E5(cfg)\n        w5e5.fetch()  # Download from ISIMIP\n        w5e5.load()   # Load into xarray\n        w5e5.extract(**extract_kwargs)\n        ds = w5e5.ds\n        self.dataset_class = w5e5\n    elif dataset_upper == \"CMIP_W5E5\":\n        cmip_w5e5 = climdata.CMIPW5E5(cfg)\n        cmip_w5e5.fetch()  # Download CMIP6 data from ISIMIP\n        cmip_w5e5.load()   # Load into xarray\n        cmip_w5e5.extract(**extract_kwargs)\n        ds = cmip_w5e5.ds\n        self.dataset_class = cmip_w5e5\n    elif dataset_upper == \"NEXGDDP\":\n        nexgddp = climdata.NEXGDDP(cfg)\n        nexgddp.fetch()  # Download NEX-GDDP-CMIP6 data from NASA THREDDS\n        nexgddp.load()   # Load into xarray\n        nexgddp.extract(**extract_kwargs)\n        ds = nexgddp.ds\n        self.dataset_class = nexgddp\n    for var in ds.data_vars:\n        ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n    # ds = ds.compute()\n\n    return ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_actions","title":"<code>get_actions(self)</code>","text":"<p>Return a dictionary of workflow actions with their outputs and descriptions.</p> <p>Supports <code>actionsinfo</code> in mapping style or list style and returns a consistent mapping of action name to description/output.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping action name -&gt; {'output': ..., 'description': ...}</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_actions(self) -&gt; dict:\n    \"\"\"Return a dictionary of workflow actions with their outputs and descriptions.\n\n    Supports ``actionsinfo`` in mapping style or list style and returns a consistent mapping of action name to description/output.\n\n    Returns:\n        dict: Mapping action name -&gt; {'output': ..., 'description': ...}\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n        raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n    actions_map = getattr(self.cfg, \"actionsinfo\")\n\n    # If 'actions' key exists, fallback to list style\n    if \"actions\" in actions_map:\n        actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                    for a in actions_map[\"actions\"]}\n\n    return actions_map\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_datasets","title":"<code>get_datasets(self)</code>","text":"<p>Return the list of dataset provider names available in configuration.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>Names of available dataset providers from <code>cfg.dsinfo</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_datasets(self) -&gt; List[str]:\n    \"\"\"Return the list of dataset provider names available in configuration.\n\n    Returns:\n        List[str]: Names of available dataset providers from ``cfg.dsinfo``.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n    return list(self.cfg.dsinfo.keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_impute_methods","title":"<code>get_impute_methods(self)</code>","text":"<p>Return mapping of available imputation methods from config.</p> <p>Returns:</p> Type Description <code>Dict[str, dict]</code> <p>Mapping of method name -&gt; config (empty dict if none configured).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_impute_methods(self) -&gt; Dict[str, dict]:\n    \"\"\"Return mapping of available imputation methods from config.\n\n    Returns:\n        Dict[str, dict]: Mapping of method name -&gt; config (empty dict if none configured).\n    \"\"\"\n    if not hasattr(self.cfg, \"imputeinfo\") or not self.cfg.imputeinfo:\n        return {}\n    return dict(self.cfg.imputeinfo)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_indices","title":"<code>get_indices(self, variables, require_all=True)</code>","text":"<p>Fetch climate extreme indices from <code>cfg.extinfo</code> that involve the given variables.</p> <p>Parameters:</p> Name Type Description Default <code>variables</code> <code>list[str]</code> <p>Variables to filter indices by (if <code>None</code>, uses <code>cfg.variables</code>).</p> required <code>require_all</code> <code>bool</code> <p>If True, return indices that require all provided variables; otherwise return indices if any variable matches.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>Mapping index_name -&gt; index_definition.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n    \"\"\"Fetch climate extreme indices from ``cfg.extinfo`` that involve the given variables.\n\n    Args:\n        variables (list[str]): Variables to filter indices by (if ``None``, uses ``cfg.variables``).\n        require_all (bool): If True, return indices that require all provided variables; otherwise return indices if any variable matches.\n\n    Returns:\n        dict: Mapping index_name -&gt; index_definition.\n    \"\"\"\n    cfg = self.cfg\n    variables = variables or cfg.variables \n    if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n        raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n    indices_def = cfg.extinfo.get(\"indices\", {})\n    if not indices_def:\n        return {}\n\n    matched_indices = {}\n    for idx_name, idx_info in indices_def.items():\n        idx_vars = idx_info.get(\"variables\", [])\n        if require_all:\n            if all(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n        else:\n            if any(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n\n    return matched_indices\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_variables","title":"<code>get_variables(self, dataset=None)</code>","text":"<p>Return the list of variables available for a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Dataset name to query. Defaults to <code>cfg.dataset</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of variable names.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n    \"\"\"Return the list of variables available for a dataset.\n\n    Args:\n        dataset (str, optional): Dataset name to query. Defaults to ``cfg.dataset``.\n\n    Returns:\n        List[str]: List of variable names.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n\n    dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n    if dataset_name is None:\n        raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n    dsinfo = self.cfg.dsinfo.get(dataset_name)\n    if not dsinfo or \"variables\" not in dsinfo:\n        raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n    return list(dsinfo[\"variables\"].keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo","title":"<code>get_varinfo(self, var)</code>","text":"<p>Get metadata for a variable from varinfo.</p> <p>Parameters:</p> Name Type Description Default <code>var</code> <code>str</code> <p>Name of the variable, e.g., 'tas', 'tasmax', 'pr'.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Metadata dictionary containing cf_name, long_name, units, etc.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If varinfo is not loaded or variable not found.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_varinfo(self, var: str) -&gt; dict:\n    \"\"\"Get metadata for a variable from varinfo.\n\n    Args:\n        var (str): Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n    Returns:\n        dict: Metadata dictionary containing cf_name, long_name, units, etc.\n\n    Raises:\n        ValueError: If varinfo is not loaded or variable not found.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        raise ValueError(\"Configuration or varinfo not loaded\")\n\n    if var not in self.cfg.varinfo:\n        raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n    return self.cfg.varinfo[var]\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.impute","title":"<code>impute(self, ds=None)</code>","text":"<p>Impute missing values using the configured imputation method.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to impute. If None, uses <code>self.current_ds</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>xr.Dataset | None</code> <p>The imputed dataset (also sets     <code>self.current_ds</code> and <code>self.impute_ds</code>). Returns <code>None</code>     if no imputation method is configured.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>If <code>ds</code> is <code>None</code> and <code>self.current_ds</code> is not set.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='impute_ds')\ndef impute(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n    \"\"\"Impute missing values using the configured imputation method.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to impute. If None, uses\n            ``self.current_ds``.\n\n    Returns:\n        xr.Dataset | None: The imputed dataset (also sets\n            ``self.current_ds`` and ``self.impute_ds``). Returns ``None``\n            if no imputation method is configured.\n\n    Raises:\n        ValueError: If ``ds`` is ``None`` and ``self.current_ds`` is not set.\n    \"\"\"\n    cfg = self.cfg\n    impute_cfg = cfg.imputeinfo\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    if cfg.impute is None:\n        self.logger.warning(\"No imputation method selected.\")\n        return None\n    # select variables (optional)\n    # variables = cfg.get(\"variables\", None)\n    # if variables:\n    #     missing = [v for v in variables if v not in self.current_ds.data_vars]\n    #     if missing:\n    #         raise ValueError(f\"Variables not present in dataset: {missing}\")\n    #     ds_in = self.current_ds[variables]\n    # else:\n    #     ds_in = self.current_ds\n\n    method = cfg.impute\n    normalize = impute_cfg[method].get(\"normalize\", True)\n    time_dim = cfg.dsinfo[cfg.dataset].get(\"time_dim\", \"time\")\n    lat_dim = cfg.dsinfo[cfg.dataset].get(\"lat_dim\", \"lat\")\n    lon_dim = cfg.dsinfo[cfg.dataset].get(\"lon_dim\", \"lon\")\n    # epochs = impute_cfg[method].get(\"epochs\", 300)\n\n    # run imputer (Imputer expects dims (time, lat, lon))\n    imputer = Imputer(\n        ds,\n        time_dim=time_dim,\n        lat_dim=lat_dim,\n        lon_dim=lon_dim,\n        method=method,\n        normalize=normalize,\n    )\n    recovered = imputer.impute()\n\n    # merge imputed variables back into original dataset if we operated on a subset\n\n    ds_out = recovered\n\n    # Return dataset (decorator will set current_ds and impute_ds and generate filenames)\n    return ds_out\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.load_config","title":"<code>load_config(self, overrides=None)</code>","text":"<p>Load and compose the Hydra configuration.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply when composing the configuration.</p> <code>None</code> <p>Returns:</p> Type Description <code>DictConfig</code> <p>Composed Hydra configuration object stored on <code>self.cfg</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n    \"\"\"Load and compose the Hydra configuration.\n\n    Args:\n        overrides (list[str], optional): Hydra overrides to apply when composing the configuration.\n\n    Returns:\n        DictConfig: Composed Hydra configuration object stored on ``self.cfg``.\n    \"\"\"\n    overrides = overrides or []\n    conf_dir = _ensure_local_conf()\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    if not GlobalHydra.instance().is_initialized():\n        hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        hydra_ctx = None\n\n    if hydra_ctx:\n        with hydra_ctx:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    else:\n        self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    return self.cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.preprocess_aoi","title":"<code>preprocess_aoi(self, cfg)</code>","text":"<p>Process an 'aoi' specification in the configuration.</p> <p>Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>DictConfig</code> <p>Configuration object with optional <code>aoi</code> entry.</p> required <p>Returns:</p> Type Description <code>DictConfig</code> <p>The modified configuration. When a Point is provided, <code>cfg.lat</code> and <code>cfg.lon</code> are set; when a Polygon is provided, <code>cfg.bounds</code> is set and <code>cfg.region</code> is set to \"custom\".</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"Process an 'aoi' specification in the configuration.\n\n    Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon).\n\n    Args:\n        cfg (DictConfig): Configuration object with optional ``aoi`` entry.\n\n    Returns:\n        DictConfig: The modified configuration. When a Point is provided, ``cfg.lat`` and ``cfg.lon`` are set; when a Polygon is provided, ``cfg.bounds`` is set and ``cfg.region`` is set to \"custom\".\n    \"\"\"\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid AOI JSON string\")\n\n    aoi = cfg.aoi\n\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n    if isinstance(geom, Point):\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                 \"lon_min\": minx, \"lon_max\": maxx}}\n        cfg.region = \"custom\"\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n    return cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.run_workflow","title":"<code>run_workflow(self, overrides=None, actions=None, file=None)</code>","text":"<p>Execute a sequence of workflow actions.</p> <p>Parameters:</p> Name Type Description Default <code>overrides</code> <code>list[str]</code> <p>Hydra overrides to apply (not all actions will use these).</p> <code>None</code> <code>actions</code> <code>list[str]</code> <p>Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.</p> <code>None</code> <code>file</code> <code>str</code> <p>File path used for upload actions when required.</p> <code>None</code> <p>Returns:</p> Type Description <code>WorkflowResult</code> <p>Named result container with populated fields for dataframe/dataset/filenames.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def run_workflow(self, overrides: Optional[List[str]] = None,\n                 actions: Optional[List[str]] = None,\n                 file: Optional[str] = None) -&gt; WorkflowResult:\n    \"\"\"Execute a sequence of workflow actions.\n\n    Args:\n        overrides (list[str], optional): Hydra overrides to apply (not all actions will use these).\n        actions (list[str], optional): Ordered list of actions to perform. Supported actions include: 'upload_netcdf', 'upload_csv', 'extract', 'calc_index', 'to_dataframe', 'to_csv', 'to_nc'.\n        file (str, optional): File path used for upload actions when required.\n\n    Returns:\n        WorkflowResult: Named result container with populated fields for dataframe/dataset/filenames.\n    \"\"\"\n    actions = actions or [\"extract\", \"calc_index\", \"to_csv\", \"to_nc\"]\n    result = WorkflowResult(cfg=self.cfg)\n    for action in actions:\n        self.logger.info(\"Starting action: %s\", action)\n        try:\n            if action == \"upload_netcdf\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                        \"but none was provided.\"\n                    )\n                    # Validate extension\n                valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_netcdf: '{file}'. \"\n                        f\"Expected one of: {valid_nc_ext}\"\n                    )\n                self.upload_netcdf(file)\n                result.dataset = self.current_ds\n\n            elif action == \"upload_csv\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_csv' requires argument 'csv_file', \"\n                        \"but none was provided.\"\n                    )\n\n                # Validate CSV extension\n                valid_csv_ext = (\".csv\", \".csv.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_csv: '{file}'. \"\n                        f\"Expected one of: {valid_csv_ext}\"\n                    )\n\n                self.upload_csv(file)\n                result.dataset = self.current_ds\n\n            elif action == \"extract\":\n                if self.cfg.dataset is None:\n                    raise ValueError(\n                        \"Action 'extract' cannot run because no dataset provider is set \"\n                        \"(cfg.dataset is None).\"\n                    )\n                self.extract()\n                result.dataset = self.current_ds\n\n            elif action == \"calc_index\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before computing an index.\"\n                    )\n                self.calc_index()\n                result.index_ds = self.current_ds\n\n            elif action == \"to_csv\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before converting to a DataFrame.\"\n                    )\n                self.to_dataframe()\n                result.dataframe = self.current_df\n                result.filename = self.to_csv()\n\n            elif action == \"to_nc\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before saving to NetCDF.\"\n                    )\n                result.filename = self.to_nc()\n\n            elif action == \"impute\":\n                if self.current_ds is None:\n                    raise ValueError(\"Action 'impute' requires a dataset, but no dataset is available.\")\n                self.impute()\n                result.dataset = self.current_ds\n                result.impute_ds = getattr(self, \"impute_ds\", None)\n\n            else:\n                raise ValueError(f\"Unknown action '{action}'\")\n            self.logger.info(\"Completed action: %s\", action)\n        except Exception:\n            self.logger.exception(\"Action '%s' failed\", action)\n            raise\n\n    return result\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_csv","title":"<code>to_csv(self, df=None, filename=None)</code>","text":"<p>Save a DataFrame to CSV.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>DataFrame to save. Defaults to <code>self.current_df</code>.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Output filename. Defaults to <code>self.filename_csv</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the written CSV file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"Save a DataFrame to CSV.\n\n    Args:\n        df (pd.DataFrame, optional): DataFrame to save. Defaults to ``self.current_df``.\n        filename (str, optional): Output filename. Defaults to ``self.filename_csv``.\n\n    Returns:\n        str: The path of the written CSV file.\n    \"\"\"\n    df = df if df is not None else self.current_df\n\n    filename = filename or getattr(self, \"filename_csv\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_csv is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    df.to_csv(filename, index=False)\n    self.filename_csv = str(path)\n    self.current_filename = str(path)\n\n    # print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n    self.logger.info(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n    return filename\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_dataframe","title":"<code>to_dataframe(self, ds=None)</code>","text":"<p>Convert a dataset to a long-form pandas DataFrame.</p> <p>The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.</p> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to convert. If <code>None</code>, uses <code>self.current_ds</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>Long-form DataFrame (also sets <code>self.current_df</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_df()\ndef to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n    \"\"\"Convert a dataset to a long-form pandas DataFrame.\n\n    The output contains columns: time, lat, lon (or latitude/longitude), variable, value, units, source.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to convert. If ``None``, uses ``self.current_ds``.\n\n    Returns:\n        pd.DataFrame: Long-form DataFrame (also sets ``self.current_df``).\n    \"\"\"\n    ds = ds or self.current_ds\n    if ds is None:\n        raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n    df = ds.to_dataframe().reset_index()\n\n    id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n    value_vars = [v for v in ds.data_vars if v in df.columns]\n\n    if not value_vars:\n        raise ValueError(\"No variables in dataset available to melt into long format\")\n\n    df_long = df.melt(\n        id_vars=id_vars,\n        value_vars=value_vars,\n        var_name=\"variable\",\n        value_name=\"value\"\n    )\n\n    df_long[\"units\"] = df_long[\"variable\"].apply(\n        lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n    )\n    if getattr(self.cfg, \"dataset\") == 'cmip':\n        df_long[\"source_id\"] = getattr(self.cfg, \"source_id\")\n    df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n    df_long = df_long.drop_duplicates()\n    self._gen_fn_cfg()\n    return df_long\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_nc","title":"<code>to_nc(self, ds=None, filename=None)</code>","text":"<p>Save an xarray Dataset to NetCDF.</p> <p>Notes</p> <ul> <li>If <code>ds</code> is <code>None</code>: save <code>current_ds</code>.</li> <li>If <code>filename</code> is <code>None</code>: use <code>self.filename_nc</code>.</li> <li>Creates directories if needed and updates <code>self.filename_nc</code> and <code>self.current_filename</code>.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>ds</code> <code>xr.Dataset</code> <p>Dataset to save. If <code>None</code>, uses <code>self.current_ds</code>.</p> <code>None</code> <code>filename</code> <code>str</code> <p>Output filename. Defaults to <code>self.filename_nc</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the written NetCDF file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"Save an xarray Dataset to NetCDF.\n\n    Notes:\n        - If ``ds`` is ``None``: save ``current_ds``.\n        - If ``filename`` is ``None``: use ``self.filename_nc``.\n        - Creates directories if needed and updates ``self.filename_nc`` and ``self.current_filename``.\n\n    Args:\n        ds (xr.Dataset, optional): Dataset to save. If ``None``, uses ``self.current_ds``.\n        filename (str, optional): Output filename. Defaults to ``self.filename_nc``.\n\n    Returns:\n        str: The path of the written NetCDF file.\n    \"\"\"\n\n    # -------------------------------\n    # 1. Determine dataset to save\n    # -------------------------------\n    ds = ds or getattr(self, \"current_ds\", None)\n    if ds is None:\n        raise ValueError(\"No dataset available to save\")\n\n    # -------------------------------\n    # 2. Determine filename\n    # -------------------------------\n    filename = filename or getattr(self, \"filename_nc\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_nc is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------\n    # 3. Save to NetCDF\n    # -------------------------------\n    ds.to_netcdf(path)\n\n    # -------------------------------\n    # 4. Track filenames\n    # -------------------------------\n    self.filename_nc = str(path)\n    self.current_filename = str(path)\n\n    # print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n    self.logger.info(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n    return str(path)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.upload_csv","title":"<code>upload_csv(self, csv_file)</code>","text":"<p>Load a long-form CSV into an xarray.Dataset.</p> <p>The CSV must contain <code>time</code> and <code>lat</code>/<code>latitude</code>, <code>lon</code>/<code>longitude</code>, <code>variable</code>, <code>value</code>. Units may be supplied in a <code>units</code> column and an optional <code>source</code> column is recognized.</p> <p>Parameters:</p> Name Type Description Default <code>csv_file</code> <code>str</code> <p>Path to the CSV file to load.</p> required <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The converted dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n    \"\"\"Load a long-form CSV into an xarray.Dataset.\n\n    The CSV must contain ``time`` and ``lat``/``latitude``, ``lon``/``longitude``, ``variable``, ``value``. Units may be supplied in a ``units`` column and an optional ``source`` column is recognized.\n\n    Args:\n        csv_file (str): Path to the CSV file to load.\n\n    Returns:\n        xr.Dataset: The converted dataset (also sets ``self.current_ds``).\n    \"\"\"\n    if not os.path.exists(csv_file):\n        raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n    df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n    lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n    lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n    if lat_col is None or lon_col is None:\n        raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n    id_vars = [\"time\", lat_col, lon_col]\n    df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n    ds = df_wide.set_index(id_vars).to_xarray()\n\n    # Attach units from CSV\n    for var in ds.data_vars:\n        units_series = df[df[\"variable\"] == var][\"units\"]\n        ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n    # Global source attribute\n    if \"source\" in df.columns:\n        source_series = df[\"source\"].dropna().unique()\n        if len(source_series) &gt; 0:\n            ds.attrs[\"source\"] = source_series[0]\n\n    # Update cfg variables &amp; varinfo\n    if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n        self.cfg.variables = list(ds.data_vars)\n    if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n    self._gen_fn(ds)\n    return ds\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.upload_netcdf","title":"<code>upload_netcdf(self, nc_file)</code>","text":"<p>Load a NetCDF file into an xarray.Dataset and update file metadata.</p> <p>Parameters:</p> Name Type Description Default <code>nc_file</code> <code>str</code> <p>Path to the NetCDF file to open.</p> required <p>Returns:</p> Type Description <code>xr.Dataset</code> <p>The loaded dataset (also sets <code>self.current_ds</code>).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@update_ds(attr_name='ds')\ndef upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n    \"\"\"Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n    Args:\n        nc_file (str): Path to the NetCDF file to open.\n\n    Returns:\n        xr.Dataset: The loaded dataset (also sets ``self.current_ds``).\n    \"\"\"\n    if not os.path.exists(nc_file):\n        raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n    ds = xr.open_dataset(nc_file)\n\n    # Update cfg variables &amp; varinfo\n    if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n        self.cfg.variables = list(ds.data_vars)\n    if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                            for v in ds.data_vars}\n    self._gen_fn(ds)\n    return ds\n</code></pre>"},{"location":"bcsd_guide/","title":"Bias Correction and Statistical Downscaling (BCSD)","text":"<p>This module provides tools for bias correction and statistical downscaling of climate model data using the ISIMIP3BASD methodology.</p>"},{"location":"bcsd_guide/#overview","title":"Overview","text":"<p>The BCSD workflow has two main steps:</p> <ol> <li>Bias Correction (BC): Adjusts systematic biases in climate model outputs using quantile mapping</li> <li>Statistical Downscaling (SD): Increases spatial resolution using fine-scale observations</li> </ol>"},{"location":"bcsd_guide/#installation","title":"Installation","text":""},{"location":"bcsd_guide/#step-1-install-climdata-dependencies","title":"Step 1: Install climdata dependencies","text":"<p>The BCSD module requires additional dependencies:</p> <pre><code>pip install iris dask scipy\n</code></pre>"},{"location":"bcsd_guide/#step-2-download-isimip3basd-code","title":"Step 2: Download ISIMIP3BASD code","text":"<p>The original ISIMIP3BASD code is not included due to licensing. Download it from: https://github.com/ISI-MIP/isimip3basd</p> <p>Place these files in <code>climdata/_vendor/isimip3basd/</code>: - <code>bias_adjustment.py</code> - <code>statistical_downscaling.py</code> - <code>utility_functions.py</code></p>"},{"location":"bcsd_guide/#quick-start","title":"Quick Start","text":"<pre><code>from climdata.sdba import BCSD\n\n# Initialize BCSD for temperature\nbcsd = BCSD(\n    variable='tas',\n    regridding_tool='xesmf',  # or 'cdo'\n    regridding_method='conservative'\n)\n\n# Run complete workflow - obs_hist_coarse automatically derived!\nresult = bcsd.run(\n    obs_fine=obs_025deg,             # Historical obs at fine resolution\n    sim_hist_coarse=gcm_hist_1deg,  # Historical GCM\n    sim_fut_coarse=gcm_fut_1deg,    # Future GCM\n    output_path='./outputs/tas_bcsd.nc'\n)\n</code></pre> <p>Key Feature: No need to manually create coarse observations! The module automatically regrids <code>obs_fine</code> to match the GCM grid using your choice of xESMF (Python) or CDO (command-line).</p>"},{"location":"bcsd_guide/#usage-examples","title":"Usage Examples","text":""},{"location":"bcsd_guide/#example-1-temperature-bias-correction-and-downscaling","title":"Example 1: Temperature Bias Correction and Downscaling","text":"<pre><code>from climdata.sdba import BCSD\n\n# Automatic configuration for temperature\nbcsd = BCSD(\n    variable='tas',\n    regridding_tool='xesmf',  # Use Python-based xESMF\n    regridding_method='conservative',  # Conservative for aggregation\n    bias_correction_kwargs={\n        'n_processes': 4  # Use 4 CPU cores\n    }\n)\n\n# Only need to provide 3 datasets (fine obs auto-regridded to coarse!)\nresult = bcsd.run(\n    obs_fine=obs_fine,\n    sim_hist_coarse=sim_hist_coarse,\n    sim_fut_coarse=sim_fut_coarse\n)\n</code></pre>"},{"location":"bcsd_guide/#example-2-using-cdo-for-regridding","title":"Example 2: Using CDO for Regridding","text":"<pre><code>from climdata.sdba import BCSD\n\n# Use CDO instead of xESMF\nbcsd = BCSD(\n    variable='tas',\n    regridding_tool='cdo',        # Use CDO command-line tool\n    cdo_method='remapcon',        # Conservative remapping\n    cdo_env='cdo_stable',         # Your conda env with CDO\n    weights_dir='./weights'       # Save weights for reuse\n)\n\nresult = bcsd.run(\n    obs_fine=obs_fine,\n    sim_hist_coarse=sim_hist_coarse,\n    sim_fut_4: Providing Your Own Coarse Observations\n\nIf you already have native coarse-resolution observations (not derived from fine obs):\n\n```python\n# Load your own coarse observations\nobs_hist_coarse = xr.open_dataset('./data/obs_coarse_native.nc')\n\nbcsd = BCSD(variable='tas')\n\nresult = bcsd.run(\n    obs_fine=obs_fine,\n    sim_hist_coarse=sim_hist_coarse,\n    sim_fut_coarse=sim_fut_coarse,\n    obs_hist_coarse=obs_hist_coarse  # Provide your own!\n)\n</code></pre>"},{"location":"bcsd_guide/#example-5oarsesim_fut_coarse","title":"Example 5oarse=sim_fut_coarse","text":"<p>) <pre><code>### Example 3: Precipitation with Custom Settings\n\n```python\nfrom climdata.sdba import BCSD\n\nbcsd = BCSD(\n    variable='pr',\n    bias_correction_kwargs={\n        'distribution': 'gamma',\n        'trend_preservation': 'mixed',\n        'lower_bound': 0,\n        'lower_threshold': 0.1,\n        'n_quantiles': 100\n    }\n)\n\nresult = bcsd.run(...)\n</code></pre></p>"},{"location":"bcsd_guide/#example-3-bias-correction-only","title":"Example 3: Bias Correction Only","text":"<p>6 <pre><code>from climdata.sdba import BiasCorrection\n\nbc = BiasCorrection(\n    variable='tas',\n    distribution='normal',\n    trend_preservation='additive',\n    detrend=True\n)\n\ncorrected = bc.correct(\n    obs_hist=obs,\n    sim_hist=gcm_hist,\n    sim_fut=gcm_fut\n)\n</code></pre></p>"},{"location":"bcsd_guide/#example-4-downscaling-only","title":"Example 4: Downscaling Only","text":"<pre><code>from climdata.sdba import StatisticalDownscaling\n\nsd = StatisticalDownscaling(\n    variable='tas',\n    n_iterations=20\n)\n\ndownscaled = sd.downscale(\n    obs_fine=obs_fine,\n    sim_coarse=sim_coarse_already_corrected\n)\n```Automatic Coarse Observation Derivation\n\nWhen `obs_hist_coarse` is not provided, the module automatically regrids `obs_fine` to match the GCM grid:\n\n**xESMF (Python):**\n- Conservative remapping (area-weighted, mass-preserving)\n- Bilinear interpolation (faster, less accurate)\n- Nearest neighbor\n- Weights cached and reused for efficiency\n\n**CDO (Command-line):**\n- `remapcon`: Conservative remapping (recommended)\n- `remapbil`: Bilinear interpolation\n- `remapdis`: Distance-weighted averaging\n- `remapnn`: Nearest neighbor\n\nBoth methods ensure the coarse observations properly match the GCM grid structure.\n\n### \n\n## Variable-Specific Configurations\n\nThe module includes optimal default configurations for common variables:\n\n| Variable | Distribution | Trend Preservation | Bounds | Detrend |\n|----------|--------------|-------------------|--------|---------|\n| `tas` (Temperature) | Normal | Additive | None | Yes |\n| `pr` (Precipitation) | Gamma | Mixed | [0, \u221e) | No |\n| `rsds` (Solar radiation) | Beta | Bounded | [0, max] | No |\n| `hurs` (Humidity) | Beta | Bounded | [0, 100] | No |\n| `sfcWind` (Wind) | Weibull | Mixed | [0, \u221e) | No |\n\nThese defaults are based on Lange (2019) and can be overridden.\n\n## Method Details\n\n### Bias Correction\n\nUses trend-preserving quantile mapping:\n\n1. **Build CDFs** for each day-of-year (\u00b115 day window)\n2. **Map quantiles** from model to observations\n3. **Preserve trends** using additive/multiplicative/mixed methods\n4. **Optional detrending** for temperature variables\n\nTrend preservation methods:\n- **Additive**: `corrected = obs_hist_quantile + (sim_fut - sim_hist)`\n- **Multiplicative**: `corrected = obs_hist_quantile \u00d7 (sim_fut / sim_hist)`\n- **Mixed**: Combination based on threshold exceedance\n- **Bounded**: Special handling for variables with physical bounds\n\n### Statistical Downscaling\n\nUses modified MBCn (Multivariate Bias Correction) algorithm:\n\n1. **Bilinear interpolation** to fine resolution (initial guess)\n2. **Iterative quantile mapping** (default 20 iterations)\n3. **Preserve spatial sums** within each coarse grid cell\n4. **Match fine-scale patterns** from observations\n\n## Complete Workflow with climdata\n\n```python\nfrom omegaconf import DictConfig\nfrom climdata.datasets.CMIP_W5E5 import CMIPW5E5\nfrom climdata.datasets.W5E5 import W5E5\nfrom climdata.sdba import BCSD\n\n# 1. Download CMIP6 historical (coarse)\ncfg_hist = DictConfig({\n    'variables': ['tas'],\n    'time_range': {'start_date': '1980-01-01', 'end_date': '2014-12-31'},\n    'experiment_id': 'historical',\n    'source_id': 'gfdl-esm4',\n    'data_dir': './data'\n})\ncmip_hist = CMIPW5E5(cfg_hist)\ncmip_hist.fetch()\ncmip_hist.load()\n\n# 2. Download CMIP6 future (coarse)\ncfg_fut = DictConfig({\n    'variables': ['tas'],\n    'time_range': {'start_date': '2015-01-01', 'end_date': '2050-12-31'},\n    'experiment_id': 'ssp585',\n    'source_id': 'gfdl-esm4',\n    'data_dir': './data'\n})\ncmip_fut = CMIPW5E5(cfg_fut)\ncmip_fut.fetch()\ncmip_futfine=w5e5.ds,                # Fine obs (auto-regridded to coarse)\n    sim_hist_coarse=cmip_hist.ds,\n    sim_fut_coarse=cmip_fut.ds,\n    output_path='./outputs/tas_ssp585_bcsd.nc'\n)\n</code></pre>"},{"location":"bcsd_guide/#regridding-options","title":"Regridding Options","text":""},{"location":"bcsd_guide/#xesmf-python-based","title":"xESMF (Python-based)","text":"<p>ProCache regridding weights: Use <code>weights_dir</code> parameter 3. Use conservative regridding: Best for obs aggregation (mass-preserving) 4. Process by region: Extract smaller regions before BCSD 5. Save intermediate results: Use <code>save_intermediate=True</code> 6. Chunk datasets: Ensure data is well-chunked in time dimension 7. Choose regridding tool wisely:    - xESMF: Better for repeated operations (cached weights)    - CDO: Better for one-time large operations - Good integration with xarray</p> <p>Cons: - Requires xESMF installation: <code>pip install xesmf</code> - May need additional dependencies (ESMF library)</p> <p>Usage: <pre><code>bcsd = BCSD(\n    variable='tas',\n    regridding_tool='xesmf',\n    regridding_method='conservative',  # or 'bilinear', 'nearest'\n    weights_dir='./weights'  # Cache weights here\n)\n</code></pre></p>"},{"location":"bcsd_guide/#cdo-command-line","title":"CDO (Command-line)","text":"<p>Pros: - Very mature and reliable - Excellent for large datasets - Multiple specialized methods</p> <p>Cons: - Requires CDO installation - Subprocess overhead - May be slower for many small operations</p> <p>Usage: <pre><code>bcsd = BCSD(\n    variable='tas',\n    regridding_tool='cdo',\n    cdo_method='remapcon',  # or 'remapbil', 'remapdis', 'remapnn'\n    cdo_env='cdo_stable'    # Conda env with CDO\n)\n</code></pre></p> <p>Install CDO: <pre><code>conda create -n cdo_stable -c conda-forge cdo   'time_range': {'start_date': '1980-01-01', 'end_date': '2014-12-31'},\n    'data_dir': './data'\n})\nw5e5 = W5E5(cfg_obs)\nw5e5.fetch()\nw5e5.load()\n\n# 4. Create coarse observations (aggregate fine obs)\nobs_coarse = w5e5.ds.coarsen(lat=2, lon=2, boundary='trim').mean()\n\n# 5. Run BCSD\nbcsd = BCSD(variable='tas')\nresult = bcsd.run(\n    obs_hist_coarse=obs_coarse,\n    obs_fine=w5e5.ds,\n    sim_hist_coarse=cmip_hist.ds,\n    sim_fut_coarse=cmip_fut.ds,\n    output_path='./outputs/tas_ssp585_bcsd.nc'\n)\n</code></pre></p>"},{"location":"bcsd_guide/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use parallel processing: Set <code>n_processes=4</code> or higher</li> <li>Process by region: Extract smaller regions before BCSD</li> <li>Save intermediate results: Use <code>save_intermediate=True</code></li> <li>Chunk datasets: Ensure data is well-chunked in time dimension</li> </ol>"},{"location":"bcsd_guide/#reference","title":"Reference","text":"<p>Lange, S. (2019). Trend-preserving bias adjustment and statistical downscaling with ISIMIP3BASD (v1.0). Geoscientific Model Development, 12(7), 3055-3070. https://doi.org/10.5194/gmd-12-3055-2019</p>"},{"location":"bcsd_guide/#license","title":"License","text":"<p>The ISIMIP3BASD code is licensed under GNU Affero General Public License v3.0. See <code>climdata/_vendor/isimip3basd/README.md</code> for details.</p>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"Common Concepts in climdata","text":"<p>This page describes common terminology, configuration patterns, and reusable components in the <code>climdata</code> package.</p>"},{"location":"common/#configuration-files","title":"Configuration Files","text":"<ul> <li>All configuration is managed via Hydra and YAML files in the <code>conf/</code> directory.</li> <li>See <code>config.yaml</code> for the main entry point.</li> </ul>"},{"location":"common/#standard-variable-names","title":"Standard Variable Names","text":"<ul> <li>Variables follow CF conventions (see <code>variables.yaml</code>).</li> <li>Example: <code>tas</code> for air temperature, <code>pr</code> for precipitation.</li> </ul>"},{"location":"common/#output-schema","title":"Output Schema","text":"<p>All outputs are standardized to the following columns:</p> Column Description latitude Latitude of observation/grid longitude Longitude of observation/grid time Timestamp source Data source/provider variable Variable name value Observed or modeled value units Units of measurement"},{"location":"common/#regions-and-bounds","title":"Regions and Bounds","text":"<ul> <li>Regions are defined in <code>config.yaml</code> under <code>bounds</code>.</li> <li>Example: <code>europe</code>, <code>global</code>.</li> </ul>"},{"location":"common/#usage-patterns","title":"Usage Patterns","text":"<ul> <li>Use <code>climdata.load_config()</code> to load configuration.</li> <li>Use <code>climdata.DWD(cfg)</code> or <code>climdata.MSWX(cfg)</code> for dataset access.</li> </ul> <p>Add more shared concepts as your documentation grows.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>climdata could always use more documentation, whether as part of the official climdata docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up climdata for local development.</p> <ol> <li> <p>Fork the climdata repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/climdata.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv climdata\n$ cd climdata/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 climdata tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/Kaushikreddym/climdata/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>It's recommended to create and activate a conda environment first, then install via pip:</p> <pre><code># create &amp; activate conda environment (recommended)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install climdata from PyPI\npip install climdata\n</code></pre> <p>This is the preferred method to install climdata, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install climdata from sources, create/activate a conda environment and then install from the repository:</p> <pre><code># create &amp; activate conda environment (optional)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install from GitHub (editable install if desired)\npip install git+https://github.com/Kaushikreddym/climdata\n# or for editable development install:\n# git clone https://github.com/Kaushikreddym/climdata\n# cd climdata\n# pip install -e .\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Quick examples to get started with the ClimData workflow utilities.</p>"},{"location":"usage/#quickstart","title":"Quickstart","text":"<p>Install into a conda env (recommended) and then pip: <pre><code>conda create -n climdata python=3.11 -y\nconda activate climdata\npip install climdata\n# or from source:\n# pip install -e .\n</code></pre></p>"},{"location":"usage/#minimal-example","title":"Minimal example","text":"<pre><code>from climdata import ClimData\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Extract dataset (updates extractor.current_ds)\nds = extractor.extract()\n\n# Compute configured extreme index (updates extractor.index_ds)\nindex_ds = extractor.calc_index(ds)\n\n# Convert to long-form DataFrame (updates extractor.current_df)\ndf = extractor.to_dataframe(index_ds)\n\n# Save DataFrame to CSV\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"usage/#single-call-workflow","title":"Single-call workflow","text":"<p>Use the high-level runner to chain common steps: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n# result contains produced dataset/dataframe and filenames\nprint(result.dataframe.head())\nprint(\"Saved to:\", result.filename)\n</code></pre></p>"},{"location":"usage/#uploading-existing-files","title":"Uploading existing files","text":"<ul> <li>Load NetCDF: extractor.upload_netcdf(\"path/to/file.nc\")</li> <li>Load long-form CSV: extractor.upload_csv(\"path/to/file.csv\")</li> </ul>"},{"location":"usage/#introspection-helpers","title":"Introspection helpers","text":"<ul> <li>extractor.get_datasets()</li> <li>extractor.get_variables(dataset_name)</li> <li>extractor.get_varinfo(varname)</li> <li>extractor.get_actions()</li> </ul>"},{"location":"usage/#notes","title":"Notes","text":"<ul> <li>See <code>docs/index.md</code> for installation details and full examples.</li> <li>For provider-specific options (MSWX, CMIP, POWER, DWD, HYRAS) consult the configuration files under <code>conf/</code> and the API docs.</li> </ul>"},{"location":"examples/climdata_cli/","title":"Climdata cli","text":"In\u00a0[\u00a0]: Copied! <pre>import climdata\nimport xarray as xr\nimport xclim\nimport pandas as pd\nfrom climdata.utils.config import _ensure_local_conf\nfrom omegaconf import DictConfig\nimport hydra\nfrom hydra.core.global_hydra import GlobalHydra\nimport sys\n</pre> import climdata import xarray as xr import xclim import pandas as pd from climdata.utils.config import _ensure_local_conf from omegaconf import DictConfig import hydra from hydra.core.global_hydra import GlobalHydra import sys <p>Example usage:</p> <p>Run the CLI with overrides:</p> <p>python climdata_cli.py  dataset=mswx  lat=52.507  lon=13.137  time_range.start_date=2000-01-01  time_range.end_date=2000-12-31  dsinfo.mswx.params.google_service_account=/home/muduchuru/.climdata_conf/service.json  data_dir=/beegfs/muduchuru/data/  variables=['tas']</p> <p>All Hydra overrides follow the format key=value.</p> In\u00a0[\u00a0]: Copied! <pre>_ensure_local_conf()\n@hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    overrides = sys.argv[1:]\n\n    # Extract data\n    cfg, filename, ds = climdata.extract_data(overrides=overrides)\n</pre> _ensure_local_conf() @hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None) def main(cfg: DictConfig) -&gt; None:     overrides = sys.argv[1:]      # Extract data     cfg, filename, ds = climdata.extract_data(overrides=overrides) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"examples/climdata_cli/#uncomment-the-below-snippet-for-parallel-processing","title":"uncomment the below snippet for parallel processing\u00b6","text":"<p>import dask from dask.distributed import Client</p>"},{"location":"examples/climdata_cli/#configure-dask","title":"Configure Dask\u00b6","text":"<p>client = Client( n_workers=20,        # or match number of physical cores threads_per_worker=2, memory_limit=\"10GB\"  # per worker (8 * 10GB = 80GB total) ) from multiprocessing import freeze_support</p>"},{"location":"examples/extremes/","title":"Extremes","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2014_v6-0_de.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) Out[\u00a0]: <pre>&lt;xarray.Dataset&gt; Size: 158kB\nDimensions:      (time: 9496, dayofyear: 366)\nCoordinates:\n  * time         (time) datetime64[ns] 76kB 1989-01-01 1989-01-02 ... 2014-12-31\n    lon          float64 8B 13.25\n    lat          float64 8B 52.9\n    x            float32 4B 4.54e+06\n    y            float32 4B 3.314e+06\n    percentiles  int64 8B 10\n  * dayofyear    (dayofyear) int64 3kB 1 2 3 4 5 6 7 ... 361 362 363 364 365 366\nData variables:\n    tasmin       (time) float64 76kB -0.5 3.2 -1.5 -3.1 ... -9.0 -7.9 -4.2 0.9\n    tasmin_per   (dayofyear) float64 3kB -11.02 -10.47 -10.85 ... -10.89 -11.02\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_1989_v6-0_de.nc</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 9496</li><li>dayofyear: 366</li></ul></li><li>Coordinates: (7)<ul><li>time(time)datetime64[ns]1989-01-01 ... 2014-12-31standard_name :timelong_name :Mid Of Twentyfour Hour Time Interval [UTC]bounds :time_bndsaxis :T<pre>array(['1989-01-01T00:00:00.000000000', '1989-01-02T00:00:00.000000000',\n       '1989-01-03T00:00:00.000000000', ..., '2014-12-29T00:00:00.000000000',\n       '2014-12-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lon()float6413.25standard_name :longitudelong_name :Longitude Of Cell Centerunits :degrees_east_CoordinateAxisType :Lon<pre>array(13.24777272)</pre></li><li>lat()float6452.9standard_name :latitudelong_name :Latitude Of Cell Centerunits :degrees_north_CoordinateAxisType :Lat<pre>array(52.89523559)</pre></li><li>x()float324.54e+06standard_name :projection_x_coordinatelong_name :X Coordinate Of Projection for Cell Centerunits :maxis :Xbounds :x_bnds<pre>array(4539500., dtype=float32)</pre></li><li>y()float323.314e+06standard_name :projection_y_coordinatelong_name :Y Coordinate Of Projection for Cell Centerunits :maxis :Ybounds :y_bnds<pre>array(3314500., dtype=float32)</pre></li><li>percentiles()int6410<pre>array(10)</pre></li><li>dayofyear(dayofyear)int641 2 3 4 5 6 ... 362 363 364 365 366<pre>array([  1,   2,   3, ..., 364, 365, 366])</pre></li></ul></li><li>Data variables: (2)<ul><li>tasmin(time)float64-0.5 3.2 -1.5 ... -7.9 -4.2 0.9standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]<pre>array([-0.50000001,  3.20000005, -1.50000002, ..., -7.90000012,\n       -4.20000006,  0.90000001])</pre></li><li>tasmin_per(dayofyear)float64-11.02 -10.47 ... -10.89 -11.02standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]climatology_bounds :['1989-01-01', '2014-12-31']window :5alpha :0.3333333333333333beta :0.3333333333333333history :[2025-11-27 15:54:07] per: percentile_doy(arr=tasmin, window=5, per=10, alpha=0.3333333333333333, beta=0.3333333333333333, copy=True) - xclim version: 0.56.0<pre>array([-11.01666683, -10.47482207, -10.8512513 , -10.99879468,\n       -11.00000016, -11.00000016, -10.85574445,  -9.87251156,\n        -9.50774443,  -8.09875811,  -6.32885854,  -5.71747954,\n        -5.82250237,  -5.7045115 ,  -6.25775352,  -6.490959  ,\n        -6.5000001 ,  -6.84641106,  -7.30381746,  -7.96494989,\n        -9.24452069, -10.47275815, -11.27005496, -12.8002285 ,\n       -13.1709317 , -12.97885864, -12.59801845, -10.42470335,\n        -9.61626498,  -9.56333348, -10.19662116, -10.68646591,\n       -10.72666683, -10.90858464, -11.56750702, -10.92812802,\n       -10.40264856, -10.6019545 , -10.11934262,  -8.50294077,\n        -8.5363015 ,  -8.56333346,  -8.56333346,  -7.57527865,\n        -6.84530604,  -6.38050238,  -6.15187224,  -6.12666676,\n        -6.61591791,  -6.20229233,  -6.12666676,  -6.35320557,\n        -7.24753435,  -7.28172614,  -7.73480377,  -8.27246588,\n        -8.01470332,  -7.30920559,  -6.61525124,  -5.34973524,\n        -5.12666674,  -5.02116902,  -5.738822  ,  -6.2512969 ,\n        -6.32666676,  -6.32666676,  -6.2229042 ,  -5.48697725,\n        -5.16392702,  -5.12666674,  -5.18593615,  -5.14093158,\n        -4.54869413,  -3.68133339,  -3.09871237,  -2.36369867,\n        -2.51671237,  -3.46794526,  -4.09315075,  -4.20000006,\n...\n        -0.5400822 ,  -0.77536987,  -1.10000002,  -1.14054796,\n        -1.42533335,  -1.80305026,  -1.30000002,  -1.30000002,\n        -1.30000002,  -1.30000002,  -1.37158906,  -1.69000003,\n        -1.69000003,  -1.6742192 ,  -1.61093153,  -1.63389044,\n        -1.50838358,  -1.60000002,  -1.61023747,  -1.63578998,\n        -1.47854797,  -1.36808221,  -1.05027399,  -0.68567124,\n        -0.24356165,  -0.51424658,  -0.74857535,  -1.68205482,\n        -1.88412788,  -2.51039273,  -3.06333338,  -3.04610963,\n        -2.92666671,  -2.98894982,  -3.50459366,  -3.99056627,\n        -4.28574436,  -5.06488592,  -5.91528776,  -6.10202749,\n        -5.76193616,  -4.61417358,  -4.49648409,  -4.4633334 ,\n        -4.46664847,  -4.53419185,  -4.9701188 ,  -5.88401835,\n        -6.35995443,  -5.31666675,  -5.31740647,  -5.32666675,\n        -5.36890419,  -5.99966219,  -6.87729691,  -8.05017364,\n        -8.42510515,  -8.55182661,  -8.283233  ,  -7.00666677,\n        -7.02032887,  -7.30000011,  -7.28794531,  -7.00244759,\n        -6.90117819,  -7.05763481,  -8.03612797,  -7.0433791 ,\n        -6.69986311,  -7.10490422,  -7.77000012,  -7.78200012,\n        -8.50945218,  -9.20581749, -10.63634719, -10.99941569,\n       -10.8936714 , -11.01666683])</pre></li></ul></li><li>Indexes: (2)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1989-01-01', '1989-01-02', '1989-01-03', '1989-01-04',\n               '1989-01-05', '1989-01-06', '1989-01-07', '1989-01-08',\n               '1989-01-09', '1989-01-10',\n               ...\n               '2014-12-22', '2014-12-23', '2014-12-24', '2014-12-25',\n               '2014-12-26', '2014-12-27', '2014-12-28', '2014-12-29',\n               '2014-12-30', '2014-12-31'],\n              dtype='datetime64[ns]', name='time', length=9496, freq=None))</pre></li><li>dayofyearPandasIndex<pre>PandasIndex(Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       357, 358, 359, 360, 361, 362, 363, 364, 365, 366],\n      dtype='int64', name='dayofyear', length=366))</pre></li></ul></li><li>Attributes: (21)source :surface observationsinstitution :Deutscher Wetterdienst (DWD)Conventions :CF-1.11title :gridded_temperature_dataset_(HYRAS-DE TASMIN)realization :v6-0project_id :HYRASlevel_type :surfacefrequency :dayhorizontal_resolution :1_kmreferences :https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_minauthor :Climate Monitoring (KU21)contact :klimaanalyse@dwd.decreation_date :created at 2024-09-01 11:11:16variable_id :tasminunique_dataset_id :DWD_HYRAS_tasmin_v6-0_1989_day_0066D4B2A8license :The HYRAS data, produced by DWD, is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0; (see https://creativecommons.org/licenses/).ConventionsURL :http://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.htmlrealm :atmosproduct :observationsinput_data_status :checkedfilename :tasmin_hyras_1_1989_v6-0_de.nc</li></ul> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin','tas']\",\n            f\"time_range.start_date=2013-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin','tas']\",             f\"time_range.start_date=2013-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n</pre> In\u00a0[6]: Copied! <pre>import json\nimport climdata\n# geojson = {\n#   \"type\": \"FeatureCollection\",\n#   \"features\": [\n#     {\n#       \"type\": \"Feature\",\n#       \"properties\": {},\n#       \"geometry\": {\n#         \"coordinates\": [\n#           13.246667038198012,\n#           52.891982026993958\n#         ],\n#         \"type\": \"Point\"\n#       }\n#     }\n#   ]\n# }\n\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json import climdata # geojson = { #   \"type\": \"FeatureCollection\", #   \"features\": [ #     { #       \"type\": \"Feature\", #       \"properties\": {}, #       \"geometry\": { #         \"coordinates\": [ #           13.246667038198012, #           52.891982026993958 #         ], #         \"type\": \"Point\" #       } #     } #   ] # }  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"data_dir=/beegfs/muduchuru/data\", #optional              \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\nSaved index to: cmip_tn10p_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n</pre>"},{"location":"examples/wrapper/","title":"Wrapper","text":"In\u00a0[1]: Copied! <pre>import logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s:%(name)s:%(message)s\"\n)\n</pre> import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s:%(name)s:%(message)s\" )  In\u00a0[\u00a0]: Copied! <pre>from climdata import ClimData\noverrides = [\n    \"dataset=hyras\",  # Choose the MSWX dataset for extraction\n    \"lat=52\",\n    \"lon=13\",\n    f\"time_range.start_date=2024-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2024-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr,'tas']\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n]\nextractor = ClimData(overrides=overrides)\nds = extractor.extract()\n</pre> from climdata import ClimData overrides = [     \"dataset=hyras\",  # Choose the MSWX dataset for extraction     \"lat=52\",     \"lon=13\",     f\"time_range.start_date=2024-01-01\",  # Start date for data extraction     f\"time_range.end_date=2024-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr,'tas']\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files ] extractor = ClimData(overrides=overrides) ds = extractor.extract() <pre>INFO:numexpr.utils:Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\nINFO:numexpr.utils:Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\nINFO:numexpr.utils:NumExpr defaulting to 16 threads.\nWARNING:pint.util:Redefining 'percent' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '%' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'year' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'yr' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'C' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'd' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'h' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_north' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_east' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '[speed]' (&lt;class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'&gt;)\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2024_v6-0_de.nc\n</pre> <pre>&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2024_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2024_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2024_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2024_v6-0_de.nc\n</pre> In\u00a0[2]: Copied! <pre>import climdata\nfrom hydra import initialize, compose\nfrom omegaconf import OmegaConf\nclimdata.utils.config._ensure_local_conf()\noverrides = None\nwith initialize(config_path='conf', version_base=None):\n    cfg = compose(config_name='config', overrides=overrides or [])\nextractor_CMIP = climdata.CMIP(cfg)\n</pre> import climdata from hydra import initialize, compose from omegaconf import OmegaConf climdata.utils.config._ensure_local_conf() overrides = None with initialize(config_path='conf', version_base=None):     cfg = compose(config_name='config', overrides=overrides or []) extractor_CMIP = climdata.CMIP(cfg) <pre>INFO:numexpr.utils:Note: detected 80 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\nINFO:numexpr.utils:Note: NumExpr detected 80 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 16.\nINFO:numexpr.utils:NumExpr defaulting to 16 threads.\nWARNING:pint.util:Redefining 'percent' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '%' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'year' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'yr' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'C' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'd' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'h' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_north' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees_east' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining 'degrees' (&lt;class 'pint.delegates.txt_defparser.plain.UnitDefinition'&gt;)\nWARNING:pint.util:Redefining '[speed]' (&lt;class 'pint.delegates.txt_defparser.plain.DerivedDimensionDefinition'&gt;)\n</pre> In\u00a0[11]: Copied! <pre>print(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('ssp245'))\nprint(extractor_CMIP.get_variables(experiment_id='ssp119',source_id='CAMS-CSM1-0'))\n</pre> print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('ssp245')) print(extractor_CMIP.get_variables(experiment_id='ssp119',source_id='CAMS-CSM1-0')) <pre>['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>INFO:climdata.datasets.CMIPCloud:46 models found for experiment 'ssp245'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n['pr', 'tas', 'tasmax']\n</pre> In\u00a0[1]: Copied! <pre>import json\n\ngeojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}\n\nimport climdata\nmswx = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n)\n</pre> import json  geojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}  import climdata mswx = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] ) <pre>Saved NetCDF to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n</pre> In\u00a0[34]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name='tasmin'\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name='tasmin' data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  In\u00a0[22]: Copied! <pre>import climdata\nhyras = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"table_id=day\",\n            \"data_dir=./data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",\n            \"region=custom\",\n    ]\n)\n</pre> import climdata hyras = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"table_id=day\",             \"data_dir=./data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",             \"region=custom\",     ] ) <pre>../../examples/conf\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2020_v6-0_de.nc\n\u2714\ufe0f  Exists locally: ./data/hyras/TASMIN/tasmin_hyras_1_2020_v6-0_de.nc\n\ud83d\udce6 Extracted curvilinear box with shape: FrozenMappingWarningOnValuesAccess({'time': 366, 'bnds': 2, 'x': 145, 'y': 110})\n\u2705 Saved output to hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\n</pre> In\u00a0[\u00a0]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name = list(ds.data_vars)[0]  # replace with actual variable if needed\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name = list(ds.data_vars)[0]  # replace with actual variable if needed data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  <pre>&lt;xarray.Dataset&gt; Size: 47MB\nDimensions:  (time: 366, y: 110, x: 145)\nCoordinates:\n  * time     (time) datetime64[ns] 3kB 2020-01-01T12:00:00 ... 2020-12-31T12:...\n    lon      (y, x) float64 128kB ...\n    lat      (y, x) float64 128kB ...\n  * x        (x) float32 580B 4.178e+06 4.178e+06 ... 4.32e+06 4.322e+06\n  * y        (y) float32 440B 2.99e+06 2.99e+06 ... 3.098e+06 3.098e+06\nData variables:\n    tasmin   (time, y, x) float64 47MB ...\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_2020_v6-0_de.nc\n</pre>"},{"location":"examples/wrapper_workflow/","title":"Workflow Notebook","text":"In\u00a0[1]: Copied! <pre>from climdata import ClimData\nimport pandas as pd\nimport xarray as xr\n\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s | %(message)s\",\n    force=True,\n)\n</pre> from climdata import ClimData import pandas as pd import xarray as xr  import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s | %(message)s\",     force=True, ) In\u00a0[2]: Copied! <pre>extractor = ClimData()\ndatasets = extractor.get_datasets()\nprint(datasets)\n</pre> extractor = ClimData() datasets = extractor.get_datasets() print(datasets) <pre>['dwd', 'mswx', 'hyras', 'cmip', 'power', 'w5e5', 'cmip_w5e5', 'nexgddp']\n</pre> In\u00a0[3]: Copied! <pre>variables = extractor.get_variables('w5e5')\nprint(variables)\n\n# for CMIP\nimport climdata\nextractor_CMIP = climdata.CMIP(extractor.cfg)\nprint(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('ssp245'))\nprint(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))\n</pre> variables = extractor.get_variables('w5e5') print(variables)  # for CMIP import climdata extractor_CMIP = climdata.CMIP(extractor.cfg) print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('ssp245')) print(extractor_CMIP.get_variables(experiment_id='ssp245',source_id='ACCESS-CM2'))  <pre>['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n\u26a0\ufe0f  Warning: Requested time range 1989-2020 extends beyond\n   the typical Historical period (1850-2014).\n   Data availability may be limited.\n['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | 46 models found for experiment 'ssp245'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'BCC-CSM2-MR', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-WACCM', 'CIESM', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-1', 'EC-Earth3', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-H', 'HadGEM3-GC31-LL', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM6A-LR', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorESM2-LM', 'NorESM2-MM', 'TaiESM1', 'UKESM1-0-LL']\n['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n</pre> In\u00a0[4]: Copied! <pre>variables = extractor.get_variables('w5e5')\nprint(variables)\nprint(\"*\"*70)\nvarinfo = extractor.get_varinfo('rlds')\nprint(varinfo)\n</pre> variables = extractor.get_variables('w5e5') print(variables) print(\"*\"*70) varinfo = extractor.get_varinfo('rlds') print(varinfo) <pre>['tas', 'tasmax', 'tasmin', 'pr', 'rsds', 'rlds', 'hurs', 'sfcWind', 'ps', 'huss']\n**********************************************************************\n{'cf_name': 'surface_downwelling_longwave_flux_in_air', 'long_name': 'Surface downwelling longwave radiation', 'units': 'W m-2'}\n</pre> In\u00a0[13]: Copied! <pre>actions = extractor.get_actions()\nprint(actions.keys())\n</pre> actions = extractor.get_actions() print(actions.keys()) <pre>dict_keys(['extract', 'calc_index', 'impute', 'to_nc', 'to_csv', 'upload_netcdf', 'upload_csv'])\n</pre> In\u00a0[6]: Copied! <pre>indices = extractor.get_indices(['tasmin', 'tasmax'])\nprint(indices.keys())\n\nimpute_methods = extractor.get_impute_methods()\nprint(impute_methods.keys())\n</pre> indices = extractor.get_indices(['tasmin', 'tasmax']) print(indices.keys())  impute_methods = extractor.get_impute_methods() print(impute_methods.keys()) <pre>dict_keys(['heat_wave_index', 'heat_wave_frequency', 'heat_wave_max_length', 'heat_wave_total_length', 'hot_spell_frequency', 'hot_spell_max_length', 'hot_spell_total_length', 'hot_spell_max_magnitude', 'ice_days', 'isothermality', 'maximum_consecutive_frost_days', 'maximum_consecutive_frost_free_days', 'maximum_consecutive_tx_days'])\ndict_keys(['BRITS', 'XGBOOST', 'CDRec', 'SoftImpute'])\n</pre> In\u00a0[7]: Copied! <pre>import json\n\n# -----------------------------\n# Step 1: Define the area of interest (AOI)\n# -----------------------------\n# The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude].\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          24.246667038198012,  # longitude\n          12.891982026993958   # latitude\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\n# -----------------------------\n# Step 2: Define configuration overrides\n# -----------------------------\n# Overrides are strings used by Hydra to modify default configurations at runtime.\noverrides = [\n    \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above\n    f\"time_range.start_date=2004-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n    \"index=tn10p\",  # Climate extreme index to calculate\n    \"impute=BRITS\"\n]\n\n# -----------------------------\n# Step 3: Define the workflow sequence\n# -----------------------------\nseq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]\n\n# -----------------------------\n# Step 4: Initialize the ClimData extractor\n# -----------------------------\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Step 5: Run the Multi-Step workflow\n# -----------------------------\nresult = extractor.run_workflow(\n    actions=seq,\n)\n</pre> import json  # ----------------------------- # Step 1: Define the area of interest (AOI) # ----------------------------- # The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude]. geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           24.246667038198012,  # longitude           12.891982026993958   # latitude         ],         \"type\": \"Point\"       }     }   ] }   # ----------------------------- # Step 2: Define configuration overrides # ----------------------------- # Overrides are strings used by Hydra to modify default configurations at runtime. overrides = [     \"dataset=cmip\",  # Choose the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above     f\"time_range.start_date=2004-01-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download     \"index=tn10p\",  # Climate extreme index to calculate     \"impute=BRITS\" ]  # ----------------------------- # Step 3: Define the workflow sequence # ----------------------------- seq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]  # ----------------------------- # Step 4: Initialize the ClimData extractor # ----------------------------- extractor = ClimData(overrides=overrides)  # ----------------------------- # Step 5: Run the Multi-Step workflow # ----------------------------- result = extractor.run_workflow(     actions=seq, ) <pre>INFO | Starting action: extract\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | Completed action: extract\nINFO | Starting action: impute\nINFO | No missing data found. Imputation not required.\nINFO | Completed action: impute\nINFO | Starting action: calc_index\n/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires \u226530 years, got 11\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\nINFO | Completed action: calc_index\nINFO | Starting action: to_nc\n&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\nINFO | Dataset saved to NetCDF file: cmip_tn10p_LAT12.891982026993958_LON24.246667038198012_2004-01-01_2014-12-31.nc\nINFO | Completed action: to_nc\n</pre> In\u00a0[8]: Copied! <pre>import json\n\n# -----------------------------\n# Define the area of interest (AOI)\n# -----------------------------\n# This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012\ngeojson = {\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"properties\": {},\n            \"geometry\": {\n                \"coordinates\": [24.246667038198012, 12.891982026993958],\n                \"type\": \"Point\"\n            }\n        }\n    ]\n}\n\n# -----------------------------\n# Define configuration overrides\n# -----------------------------\n# These strings override the default hydra config at runtime\noverrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    # Optional Google service account if needed for MSWX access\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    \"index=tn10p\",  # Extreme climate index to calculate\n]\n\n# -----------------------------\n# Initialize the ClimData extractor\n# -----------------------------\n# This loads the configuration with overrides and prepares the object\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Extract climate data\n# -----------------------------\n# Returns an xarray.Dataset for the selected variables, AOI, and time range\nds = extractor.extract()\n\n# -----------------------------\n# Compute the climate index\n# -----------------------------\n# Takes the extracted dataset and calculates the extreme index \"tn10p\"\n# Returns a new xarray.Dataset containing only the index\nds_index = extractor.calc_index(ds)\n\n# -----------------------------\n# Convert the index dataset to a long-form pandas DataFrame\n# -----------------------------\n# Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\")\ndf_index = extractor.to_dataframe(ds_index)\n\n# -----------------------------\n# Save the DataFrame to CSV\n# -----------------------------\n# This will write the index values to \"index.csv\" in the current working directory\nextractor.to_csv(df_index, filename=\"index.csv\")\n</pre> import json  # ----------------------------- # Define the area of interest (AOI) # ----------------------------- # This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012 geojson = {     \"type\": \"FeatureCollection\",     \"features\": [         {             \"type\": \"Feature\",             \"properties\": {},             \"geometry\": {                 \"coordinates\": [24.246667038198012, 12.891982026993958],                 \"type\": \"Point\"             }         }     ] }  # ----------------------------- # Define configuration overrides # ----------------------------- # These strings override the default hydra config at runtime overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     # Optional Google service account if needed for MSWX access     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     \"index=tn10p\",  # Extreme climate index to calculate ]  # ----------------------------- # Initialize the ClimData extractor # ----------------------------- # This loads the configuration with overrides and prepares the object extractor = ClimData(overrides=overrides)  # ----------------------------- # Extract climate data # ----------------------------- # Returns an xarray.Dataset for the selected variables, AOI, and time range ds = extractor.extract()  # ----------------------------- # Compute the climate index # ----------------------------- # Takes the extracted dataset and calculates the extreme index \"tn10p\" # Returns a new xarray.Dataset containing only the index ds_index = extractor.calc_index(ds)  # ----------------------------- # Convert the index dataset to a long-form pandas DataFrame # ----------------------------- # Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\") df_index = extractor.to_dataframe(ds_index)  # ----------------------------- # Save the DataFrame to CSV # ----------------------------- # This will write the index values to \"index.csv\" in the current working directory extractor.to_csv(df_index, filename=\"index.csv\")  <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n\u2705 All 31 pr files already exist locally.\n</pre> <pre>/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index tn10p usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\nINFO | DataFrame saved to CSV file: index.csv\n</pre> Out[8]: <pre>'index.csv'</pre> In\u00a0[9]: Copied! <pre>print(extractor.current_filename)\n# print(extractor_point.filename_nc)\n</pre> print(extractor.current_filename) # print(extractor_point.filename_nc) <pre>index.csv\n</pre> In\u00a0[10]: Copied! <pre>box_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\n\nextractor_box = ClimData(overrides=box_overrides)\nresult_box = extractor_box.run_workflow(actions=[\"extract\", \"to_csv\"])\n</pre> box_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ]  extractor_box = ClimData(overrides=box_overrides) result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_csv\"])  <pre>INFO | Starting action: extract\n</pre> <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> <pre>INFO | Completed action: extract\nINFO | Starting action: to_csv\nINFO | DataFrame saved to CSV file: mswx_tasmin_tasmax_LAT_34.0_71.0_LON_-25.0_45.0_2014-12-01_2014-12-31.csv\nINFO | Completed action: to_csv\n</pre> In\u00a0[11]: Copied! <pre>lat_berlin, lon_berlin = [52.5,13.4]\nidx_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"lat={lat_berlin}\", # Select the region\n    f\"lon={lon_berlin}\",\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    \"index=heat_wave_max_length\"\n]\n\n\nextractor_idx = ClimData(overrides=idx_overrides)\nresult_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_csv\"])\nresult_idx.dataframe.head()\n</pre> lat_berlin, lon_berlin = [52.5,13.4] idx_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"lat={lat_berlin}\", # Select the region     f\"lon={lon_berlin}\",     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     \"index=heat_wave_max_length\" ]   extractor_idx = ClimData(overrides=idx_overrides) result_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_csv\"]) result_idx.dataframe.head() <pre>INFO | Starting action: extract\n</pre> <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> <pre>INFO | Completed action: extract\nINFO | Starting action: calc_index\n/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:632: UserWarning: Index heat_wave_max_length usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\nINFO | Completed action: calc_index\nINFO | Starting action: to_csv\nINFO | DataFrame saved to CSV file: mswx_heat_wave_max_length_LAT_52.5_LON_13.4_2014-12-01_2014-12-31.csv\nINFO | Completed action: to_csv\n</pre> Out[11]: time lat lon variable value units source 0 2014-01-01 52.549999 13.350003 heat_wave_max_length 0.0 d mswx In\u00a0[12]: Copied! <pre>try:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"calc_index\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"to_csv\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"upload_netcdf\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n</pre> try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"calc_index\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"to_csv\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"upload_netcdf\"]) except Exception as e:     print(\"Error:\", e) <pre>INFO | Starting action: calc_index\nERROR | Action 'calc_index' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 829, in run_workflow\n    raise ValueError(\nValueError: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n</pre> <pre>Error: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\n</pre> <pre>INFO | Starting action: to_csv\nERROR | Action 'to_csv' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 838, in run_workflow\n    raise ValueError(\nValueError: Action 'to_dataframe' requires a dataset, but no dataset is available. Upload or extract a dataset before converting to a DataFrame.\n</pre> <pre>Error: Action 'to_dataframe' requires a dataset, but no dataset is available. Upload or extract a dataset before converting to a DataFrame.\n</pre> <pre>INFO | Starting action: upload_netcdf\nERROR | Action 'upload_netcdf' failed\nTraceback (most recent call last):\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 786, in run_workflow\n    raise ValueError(\nValueError: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n</pre> <pre>Error: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n</pre>"},{"location":"examples/wrapper_workflow/#climdata-tutorial","title":"ClimData Tutorial\u00b6","text":"<p>This notebook demonstrates usage of the <code>ClimData</code> class for climate data extraction, extreme index computation, and workflow management. Includes examples for point-based and box-based extraction, variable exploration, and error handling.</p>"},{"location":"examples/wrapper_workflow/#1-imports","title":"1\ufe0f\u20e3 Imports\u00b6","text":""},{"location":"examples/wrapper_workflow/#2-explore-available-datasets","title":"2\ufe0f\u20e3 Explore available datasets\u00b6","text":""},{"location":"examples/wrapper_workflow/#3-explore-variables-for-a-dataset","title":"3\ufe0f\u20e3 Explore variables for a dataset\u00b6","text":""},{"location":"examples/wrapper_workflow/#4-explore-metadata-for-a-variable","title":"4\ufe0f\u20e3 Explore metadata for a variable\u00b6","text":""},{"location":"examples/wrapper_workflow/#5-explore-available-workflow-actions","title":"5\ufe0f\u20e3 Explore available workflow actions\u00b6","text":""},{"location":"examples/wrapper_workflow/#6-point-extraction-workflow","title":"6\ufe0f\u20e3 Point extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#output-filenames","title":"Output filenames\u00b6","text":""},{"location":"examples/wrapper_workflow/#7-box-extraction-workflow","title":"7\ufe0f\u20e3 Box extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#8-compute-extreme-index-only","title":"8\ufe0f\u20e3 Compute extreme index only\u00b6","text":""},{"location":"examples/wrapper_workflow/#9-error-examples","title":"9\ufe0f\u20e3 Error examples\u00b6","text":""},{"location":"examples/advanced/cmip_obs_comparison/","title":"CMIP validation","text":"In\u00a0[1]: Copied! <pre>import climdata\nimport pandas as pd\nimport xarray as xr\nimport cftime\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s | %(message)s\",\n    force=True,\n)\n\nextractor = climdata.ClimData()\n\nextractor_CMIP = climdata.CMIP(extractor.cfg)\nprint(extractor_CMIP.get_experiment_ids())\nprint(extractor_CMIP.get_source_ids('historical'))\nprint(extractor_CMIP.get_variables(experiment_id='historical',source_id=['GISS-E2-1-H','CanESM5','GFDL-ESM4']))\n</pre> import climdata import pandas as pd import xarray as xr import cftime import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s | %(message)s\",     force=True, )  extractor = climdata.ClimData()  extractor_CMIP = climdata.CMIP(extractor.cfg) print(extractor_CMIP.get_experiment_ids()) print(extractor_CMIP.get_source_ids('historical')) print(extractor_CMIP.get_variables(experiment_id='historical',source_id=['GISS-E2-1-H','CanESM5','GFDL-ESM4']))  <pre>['historical', 'ssp119', 'ssp126', 'ssp245', 'ssp370', 'ssp434', 'ssp460', 'ssp585']\n</pre> <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\nINFO | 65 models found for experiment 'historical'\n</pre> <pre>['ACCESS-CM2', 'ACCESS-ESM1-5', 'AWI-CM-1-1-MR', 'AWI-ESM-1-1-LR', 'BCC-CSM2-MR', 'BCC-ESM1', 'CAMS-CSM1-0', 'CAS-ESM2-0', 'CESM2', 'CESM2-FV2', 'CESM2-WACCM', 'CESM2-WACCM-FV2', 'CIESM', 'CMCC-CM2-HR4', 'CMCC-CM2-SR5', 'CMCC-ESM2', 'CNRM-CM6-1', 'CNRM-CM6-1-HR', 'CNRM-ESM2-1', 'CanESM5', 'CanESM5-CanOE', 'E3SM-1-0', 'E3SM-1-1', 'E3SM-1-1-ECA', 'EC-Earth3', 'EC-Earth3-AerChem', 'EC-Earth3-CC', 'EC-Earth3-Veg', 'EC-Earth3-Veg-LR', 'EC-Earth3P-VHR', 'FGOALS-f3-L', 'FGOALS-g3', 'FIO-ESM-2-0', 'GFDL-CM4', 'GFDL-ESM4', 'GISS-E2-1-G', 'GISS-E2-1-G-CC', 'GISS-E2-1-H', 'GISS-E2-2-H', 'HadGEM3-GC31-LL', 'HadGEM3-GC31-MM', 'ICON-ESM-LR', 'IITM-ESM', 'INM-CM4-8', 'INM-CM5-0', 'IPSL-CM5A2-INCA', 'IPSL-CM6A-LR', 'IPSL-CM6A-LR-INCA', 'KACE-1-0-G', 'KIOST-ESM', 'MCM-UA-1-0', 'MIROC-ES2H', 'MIROC-ES2L', 'MIROC6', 'MPI-ESM-1-2-HAM', 'MPI-ESM1-2-HR', 'MPI-ESM1-2-LR', 'MRI-ESM2-0', 'NESM3', 'NorCPM1', 'NorESM2-LM', 'NorESM2-MM', 'SAM0-UNICON', 'TaiESM1', 'UKESM1-0-LL']\n['hurs', 'pr', 'sfcWind', 'tas', 'tasmax', 'tasmin']\n</pre> In\u00a0[2]: Copied! <pre>from climdata import ClimData\noverrides = [\n    \"dataset=hyras\",\n    \"lat=52\",\n    \"lon=13\",\n    # \"region=germany\",\n    f\"time_range.start_date=1989-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr,tas]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n]\nextractor = ClimData(overrides=overrides)\nds_obs = extractor.extract()\ndf_obs = extractor.to_dataframe()\n</pre> from climdata import ClimData overrides = [     \"dataset=hyras\",     \"lat=52\",     \"lon=13\",     # \"region=germany\",     f\"time_range.start_date=1989-01-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr,tas]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files ] extractor = ClimData(overrides=overrides) ds_obs = extractor.extract() df_obs = extractor.to_dataframe() <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2014_v6-0_de.nc\n</pre> <pre>&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n</pre> <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_max/tasmax_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMAX/tasmax_hyras_1_2014_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/precipitation/pr_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/PR/pr_hyras_1_2014_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_mean/tas_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TAS/tas_hyras_1_2014_v6-0_de.nc\n</pre> In\u00a0[3]: Copied! <pre>ds_cmip = []\ndf_cmip = []\nimport cftime\nimport xarray as xr\n\nfor source_id in ['ACCESS-CM2','CanESM5','GFDL-ESM4']:\n    overrides = [\n        \"dataset=cmip\",  # Choose the MSWX dataset for extraction\n        \"lat=52\",\n        \"lon=13\",\n        f\"time_range.start_date=1989-01-01\",  # Start date for data extraction\n        f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n        f\"experiment_id=historical\",\n        f\"source_id='{source_id}'\",\n        f\"table_id=Amon\",\n        \"variables=['tasmin','tasmax','pr','tas']\",       # Variables to extract: min/max temp and precipitation\n        \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    ]\n    extractor = ClimData(overrides=overrides)\n    ds = extractor.extract()\n    df_cmip.append(extractor.to_dataframe())\ndf_cmip = pd.concat(df_cmip,axis=0)\n</pre> ds_cmip = [] df_cmip = [] import cftime import xarray as xr  for source_id in ['ACCESS-CM2','CanESM5','GFDL-ESM4']:     overrides = [         \"dataset=cmip\",  # Choose the MSWX dataset for extraction         \"lat=52\",         \"lon=13\",         f\"time_range.start_date=1989-01-01\",  # Start date for data extraction         f\"time_range.end_date=2014-12-31\",    # End date for data extraction         f\"experiment_id=historical\",         f\"source_id='{source_id}'\",         f\"table_id=Amon\",         \"variables=['tasmin','tasmax','pr','tas']\",       # Variables to extract: min/max temp and precipitation         \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     ]     extractor = ClimData(overrides=overrides)     ds = extractor.extract()     df_cmip.append(extractor.to_dataframe()) df_cmip = pd.concat(df_cmip,axis=0) <pre>/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/intake_esm/core.py:475: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n  cat = self.__class__({'esmcat': self.esmcat.dict(), 'df': esmcat_results})\n</pre> In\u00a0[4]: Copied! <pre>import pandas as pd\nimport numpy as np\ndef cftime_to_datetime(df, time_col='time'):\n    \"\"\"\n    Convert cftime.Datetime* to pd.Timestamp where possible.\n    \"\"\"\n    import cftime\n    times = df[time_col].values\n    new_times = []\n    for t in times:\n        if isinstance(t, (cftime.DatetimeNoLeap, cftime.DatetimeJulian, cftime.DatetimeProlepticGregorian)):\n            # convert to pandas Timestamp; keep year, month, day\n            # ignore hour/min/sec for safety\n            new_times.append(pd.Timestamp(f\"{t.year:04d}-{t.month:02d}-{t.day:02d}\"))\n        else:\n            new_times.append(pd.Timestamp(t))\n    df[time_col] = new_times\n    return df\ndef compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None):\n    \"\"\"\n    Compute RMSE, CORR, MAPE, MNB for CMIP models vs observations with optional time resampling.\n    \n    Parameters\n    ----------\n    df_cmip : pd.DataFrame\n        Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source_id','source']\n    df_obs : pd.DataFrame\n        Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source']\n    variables : list or None\n        List of variable names to compute metrics. If None, all variables in df_obs are used.\n    time_resample : str or None\n        Optional pandas resample frequency: 'D' (daily), 'MS' (month start), 'YS' (year start).\n        If None, no temporal aggregation is performed.\n    \n    Returns\n    -------\n    pd.DataFrame\n        Metrics per variable and source_id with columns ['variable','source_id','RMSE','CORR','MAPE','MNB']\n    \"\"\"\n    \n    if variables is None:\n        variables = df_obs['variable'].unique()\n    \n    # Ensure time is datetime\n    df_cmip = cftime_to_datetime(df_cmip, time_col='time')\n    df_obs  = cftime_to_datetime(df_obs, time_col='time')\n        \n    # 1\ufe0f\u20e3 Aggregate spatially: mean over lat/lon\n    df_cmip_mean = (\n        df_cmip.groupby(['time','variable','source_id'])['value']\n        .mean()\n        .reset_index()\n    )\n    df_obs_mean = (\n        df_obs.groupby(['time','variable'])['value']\n        .mean()\n        .reset_index()\n    )\n    \n    # 2\ufe0f\u20e3 Resample in time if requested\n    if time_resample is not None:\n        # CMIP\n        df_cmip_mean = (\n            df_cmip_mean.set_index('time')\n            .groupby(['variable','source_id'])\n            .resample(time_resample)['value']\n            .mean()\n            .reset_index()\n        )\n        # Observations\n        df_obs_mean = (\n            df_obs_mean.set_index('time')\n            .groupby('variable')\n            .resample(time_resample)['value']\n            .mean()\n            .reset_index()\n        )\n    \n    records = []\n    \n    # 3\ufe0f\u20e3 Loop over variables and source_ids\n    for var in variables:\n        o = df_obs_mean[df_obs_mean['variable']==var]\n        \n        for src in df_cmip_mean['source_id'].unique():\n            m = df_cmip_mean[(df_cmip_mean['variable']==var) &amp; (df_cmip_mean['source_id']==src)]\n            \n            # Merge on time\n            df_combined = pd.merge(\n                o[['time','value']], \n                m[['time','value']], \n                on='time', \n                suffixes=('_obs', f'_{src}')\n            )\n            \n            obs_vals = df_combined[f'value_obs'].values\n            mod_vals = df_combined[f'value_{src}'].values\n\n            # Small jitter to avoid divide-by-zero in MAPE\n            epsilon = obs_vals.std() * 0.001\n            obs_safe = obs_vals + epsilon\n\n            rmse = np.sqrt(np.mean((mod_vals - obs_vals)**2))\n            corr = np.corrcoef(obs_vals, mod_vals)[0,1]\n            mape = np.mean(np.abs((mod_vals - obs_safe)/obs_safe)) * 100\n            mnb  = np.mean(mod_vals - obs_vals) / np.mean(obs_safe) * 100  # mean normalized bias\n            \n            records.append({\n                'variable': var,\n                'source_id': src,\n                'Model Mean': mod_vals.mean(),\n                'Observation Mean': obs_vals.mean(),\n                'RMSE': rmse,\n                'CORR': corr,\n                'MAPE': mape,\n                'MNB': mnb\n            })\n    \n    df_metrics = pd.DataFrame(records)\n    return df_metrics\n</pre> import pandas as pd import numpy as np def cftime_to_datetime(df, time_col='time'):     \"\"\"     Convert cftime.Datetime* to pd.Timestamp where possible.     \"\"\"     import cftime     times = df[time_col].values     new_times = []     for t in times:         if isinstance(t, (cftime.DatetimeNoLeap, cftime.DatetimeJulian, cftime.DatetimeProlepticGregorian)):             # convert to pandas Timestamp; keep year, month, day             # ignore hour/min/sec for safety             new_times.append(pd.Timestamp(f\"{t.year:04d}-{t.month:02d}-{t.day:02d}\"))         else:             new_times.append(pd.Timestamp(t))     df[time_col] = new_times     return df def compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None):     \"\"\"     Compute RMSE, CORR, MAPE, MNB for CMIP models vs observations with optional time resampling.          Parameters     ----------     df_cmip : pd.DataFrame         Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source_id','source']     df_obs : pd.DataFrame         Long-form DataFrame with columns ['time','lat','lon','variable','value','units','source']     variables : list or None         List of variable names to compute metrics. If None, all variables in df_obs are used.     time_resample : str or None         Optional pandas resample frequency: 'D' (daily), 'MS' (month start), 'YS' (year start).         If None, no temporal aggregation is performed.          Returns     -------     pd.DataFrame         Metrics per variable and source_id with columns ['variable','source_id','RMSE','CORR','MAPE','MNB']     \"\"\"          if variables is None:         variables = df_obs['variable'].unique()          # Ensure time is datetime     df_cmip = cftime_to_datetime(df_cmip, time_col='time')     df_obs  = cftime_to_datetime(df_obs, time_col='time')              # 1\ufe0f\u20e3 Aggregate spatially: mean over lat/lon     df_cmip_mean = (         df_cmip.groupby(['time','variable','source_id'])['value']         .mean()         .reset_index()     )     df_obs_mean = (         df_obs.groupby(['time','variable'])['value']         .mean()         .reset_index()     )          # 2\ufe0f\u20e3 Resample in time if requested     if time_resample is not None:         # CMIP         df_cmip_mean = (             df_cmip_mean.set_index('time')             .groupby(['variable','source_id'])             .resample(time_resample)['value']             .mean()             .reset_index()         )         # Observations         df_obs_mean = (             df_obs_mean.set_index('time')             .groupby('variable')             .resample(time_resample)['value']             .mean()             .reset_index()         )          records = []          # 3\ufe0f\u20e3 Loop over variables and source_ids     for var in variables:         o = df_obs_mean[df_obs_mean['variable']==var]                  for src in df_cmip_mean['source_id'].unique():             m = df_cmip_mean[(df_cmip_mean['variable']==var) &amp; (df_cmip_mean['source_id']==src)]                          # Merge on time             df_combined = pd.merge(                 o[['time','value']],                  m[['time','value']],                  on='time',                  suffixes=('_obs', f'_{src}')             )                          obs_vals = df_combined[f'value_obs'].values             mod_vals = df_combined[f'value_{src}'].values              # Small jitter to avoid divide-by-zero in MAPE             epsilon = obs_vals.std() * 0.001             obs_safe = obs_vals + epsilon              rmse = np.sqrt(np.mean((mod_vals - obs_vals)**2))             corr = np.corrcoef(obs_vals, mod_vals)[0,1]             mape = np.mean(np.abs((mod_vals - obs_safe)/obs_safe)) * 100             mnb  = np.mean(mod_vals - obs_vals) / np.mean(obs_safe) * 100  # mean normalized bias                          records.append({                 'variable': var,                 'source_id': src,                 'Model Mean': mod_vals.mean(),                 'Observation Mean': obs_vals.mean(),                 'RMSE': rmse,                 'CORR': corr,                 'MAPE': mape,                 'MNB': mnb             })          df_metrics = pd.DataFrame(records)     return df_metrics  In\u00a0[9]: Copied! <pre>from bokeh.models import HoverTool\nimport hvplot.pandas\n\nvar = 'pr'\n# Rename columns to remove spaces\nmetrics_df = compute_cmip_metrics(df_cmip, df_obs, time_resample='MS')\n\nmetrics_df = metrics_df.rename(columns={\n    'Model Mean': 'Model_Mean',\n    'Observation Mean': 'Obs_Mean'\n})\n\n# Update your HoverTool to match\ncustom_hover = HoverTool(\n    tooltips=[\n        (\"Source ID\", \"@source_id\"),\n        (\"Model Mean\", \"@Model_Mean\"),\n        (\"Obs Mean\", \"@Obs_Mean\"), # No braces needed now\n        (\"RMSE\", \"@RMSE\"),\n        (\"Pearson r\", \"@CORR\"),\n    ],\n    mode='vline' \n)\n\n# Update your scatter call\npoints = metrics_df[metrics_df['variable']==var].hvplot.scatter(\n    x='source_id', \n    y='Model_Mean', \n    hover_cols=['RMSE','CORR', 'MAPE', 'MNB', 'Obs_Mean'], # Use new name\n    size=15,\n    tools=[custom_hover]\n)\n\n# 3. Create the box plot (disable its default hover so they don't clash)\nbox_plot = df_cmip[df_cmip['variable']==var].hvplot.box(\n    y='value',\n    by=['source_id'],\n    ylabel=var,\n    hover=False # We want the 'points' hover to take the lead\n)\n\ntop_plot = (box_plot * points).opts(\n    title=f\"Distribution: {var}\",\n    width=800, \n    height=400\n)\n\n# 2. New Time Series Plot (Bottom Panel)\n# We filter df_cmip for the variable, then plot time vs value\nts_plot = df_cmip[df_cmip['variable'] == var].hvplot.line(\n    x='time',      # Ensure this matches your date/time column name\n    y='value',\n    by='source_id',\n    ylabel=var,\n    title=f\"{var} Time Series by Model\",\n    width=800,\n    height=300,\n    alpha=0.6      # Slight transparency helps if lines overlap\n)\n\n# Optional: Add the observation line to the time series for reference\nif 'df_obs' in locals():\n    obs_ts = df_obs[df_obs['variable'] == var].hvplot.line(\n        x='time', \n        y='value', \n        color='black', \n        line_width=2, \n        label='Observations'\n    )\n    ts_plot = ts_plot * obs_ts\n\n# 3. Combine and stack vertically\n# The '+' operator creates a layout, '.cols(1)' forces the vertical stack\nlayout = (top_plot + ts_plot).cols(1)\nlayout\n</pre> from bokeh.models import HoverTool import hvplot.pandas  var = 'pr' # Rename columns to remove spaces metrics_df = compute_cmip_metrics(df_cmip, df_obs, time_resample='MS')  metrics_df = metrics_df.rename(columns={     'Model Mean': 'Model_Mean',     'Observation Mean': 'Obs_Mean' })  # Update your HoverTool to match custom_hover = HoverTool(     tooltips=[         (\"Source ID\", \"@source_id\"),         (\"Model Mean\", \"@Model_Mean\"),         (\"Obs Mean\", \"@Obs_Mean\"), # No braces needed now         (\"RMSE\", \"@RMSE\"),         (\"Pearson r\", \"@CORR\"),     ],     mode='vline'  )  # Update your scatter call points = metrics_df[metrics_df['variable']==var].hvplot.scatter(     x='source_id',      y='Model_Mean',      hover_cols=['RMSE','CORR', 'MAPE', 'MNB', 'Obs_Mean'], # Use new name     size=15,     tools=[custom_hover] )  # 3. Create the box plot (disable its default hover so they don't clash) box_plot = df_cmip[df_cmip['variable']==var].hvplot.box(     y='value',     by=['source_id'],     ylabel=var,     hover=False # We want the 'points' hover to take the lead )  top_plot = (box_plot * points).opts(     title=f\"Distribution: {var}\",     width=800,      height=400 )  # 2. New Time Series Plot (Bottom Panel) # We filter df_cmip for the variable, then plot time vs value ts_plot = df_cmip[df_cmip['variable'] == var].hvplot.line(     x='time',      # Ensure this matches your date/time column name     y='value',     by='source_id',     ylabel=var,     title=f\"{var} Time Series by Model\",     width=800,     height=300,     alpha=0.6      # Slight transparency helps if lines overlap )  # Optional: Add the observation line to the time series for reference if 'df_obs' in locals():     obs_ts = df_obs[df_obs['variable'] == var].hvplot.line(         x='time',          y='value',          color='black',          line_width=2,          label='Observations'     )     ts_plot = ts_plot * obs_ts  # 3. Combine and stack vertically # The '+' operator creates a layout, '.cols(1)' forces the vertical stack layout = (top_plot + ts_plot).cols(1) layout Out[9]: In\u00a0[6]: Copied! <pre>import holoviews as hv\nhv.save(layout, f'distribution_{var}.html')\n</pre> import holoviews as hv hv.save(layout, f'distribution_{var}.html') In\u00a0[8]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Polygon\nfrom matplotlib.colors import Normalize\nimport matplotlib.cm as cm\n\nrmse = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"RMSE\")\ncorr = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"CORR\")\n\nmodels = rmse.index.tolist()\nvariables = rmse.columns.tolist()\n\n# -------------------------------------------------------\n# Colormap and normalization\n# -------------------------------------------------------\ncmap_rmse = plt.cm.jet\nrmse_norm = Normalize(vmin=0, vmax=10)\ncmap_corr = plt.cm.Blues\ncorr_norm = Normalize(vmin=-1, vmax=1)\n\n# -------------------------------------------------------\n# Plot\n# -------------------------------------------------------\nfig, ax = plt.subplots(figsize=(12, 8))\n\nfor i, model in enumerate(models):\n    for j, var in enumerate(variables):\n\n        x0, x1 = j, j + 1\n        y0, y1 = i, i + 1\n\n        # Upper-left triangle (RMSE)\n        tri_rmse = Polygon(\n            [[x0, y1], [x0, y0], [x1, y1]],\n            closed=True,\n            facecolor=cmap_rmse(rmse_norm(rmse.loc[model, var])),\n            edgecolor=\"none\"\n        )\n        ax.add_patch(tri_rmse)\n\n        # Lower-right triangle (CORR)\n        tri_corr = Polygon(\n            [[x1, y0], [x0, y0], [x1, y1]],\n            closed=True,\n            facecolor=cmap_corr(corr_norm(corr.loc[model, var])),\n            edgecolor=\"none\"\n        )\n        ax.add_patch(tri_corr)\n\n# -------------------------------------------------------\n# Axes formatting\n# -------------------------------------------------------\nax.set_xticks(np.arange(len(variables)) + 0.5)\nax.set_xticklabels(variables, fontsize=14)\n\nax.set_yticks(np.arange(len(models)) + 0.5)\nax.set_yticklabels(models, fontsize=14)\n\nax.set_xlim(0, len(variables))\nax.set_ylim(0, len(models))\nax.invert_yaxis()\nax.set_aspect(\"equal\")\n\nax.set_xlabel(\"Variables\", fontsize=16)\nax.set_ylabel(\"\", fontsize=16)\nax.set_title(\"Portrait Plot (RMSE &amp; CORR)\", fontsize=18)\n\nax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n\n# -------------------------------------------------------\n# Colorbars (larger, better spacing)\n# -------------------------------------------------------\ncbar_rmse = fig.colorbar(\n    cm.ScalarMappable(norm=rmse_norm, cmap=cmap_rmse),\n    ax=ax,\n    fraction=0.05,\n    pad=0.02,\n    shrink=0.8,\n    aspect=20\n)\ncbar_rmse.set_label(\"Root Mean Sq Error\", fontsize=14)\ncbar_rmse.ax.tick_params(labelsize=12)\n\ncbar_corr = fig.colorbar(\n    cm.ScalarMappable(norm=corr_norm, cmap=cmap_corr),\n    ax=ax,\n    fraction=0.15,\n    pad=0.02,\n    shrink=0.8,\n    aspect=20\n)\ncbar_corr.set_label(\"Pearson Correlation Coefficient\", fontsize=14)\ncbar_corr.ax.tick_params(labelsize=12)\n\nplt.tight_layout()\nplt.show()\n</pre> import pandas as pd import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Polygon from matplotlib.colors import Normalize import matplotlib.cm as cm  rmse = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"RMSE\") corr = metrics_df.pivot(index=\"source_id\", columns=\"variable\", values=\"CORR\")  models = rmse.index.tolist() variables = rmse.columns.tolist()  # ------------------------------------------------------- # Colormap and normalization # ------------------------------------------------------- cmap_rmse = plt.cm.jet rmse_norm = Normalize(vmin=0, vmax=10) cmap_corr = plt.cm.Blues corr_norm = Normalize(vmin=-1, vmax=1)  # ------------------------------------------------------- # Plot # ------------------------------------------------------- fig, ax = plt.subplots(figsize=(12, 8))  for i, model in enumerate(models):     for j, var in enumerate(variables):          x0, x1 = j, j + 1         y0, y1 = i, i + 1          # Upper-left triangle (RMSE)         tri_rmse = Polygon(             [[x0, y1], [x0, y0], [x1, y1]],             closed=True,             facecolor=cmap_rmse(rmse_norm(rmse.loc[model, var])),             edgecolor=\"none\"         )         ax.add_patch(tri_rmse)          # Lower-right triangle (CORR)         tri_corr = Polygon(             [[x1, y0], [x0, y0], [x1, y1]],             closed=True,             facecolor=cmap_corr(corr_norm(corr.loc[model, var])),             edgecolor=\"none\"         )         ax.add_patch(tri_corr)  # ------------------------------------------------------- # Axes formatting # ------------------------------------------------------- ax.set_xticks(np.arange(len(variables)) + 0.5) ax.set_xticklabels(variables, fontsize=14)  ax.set_yticks(np.arange(len(models)) + 0.5) ax.set_yticklabels(models, fontsize=14)  ax.set_xlim(0, len(variables)) ax.set_ylim(0, len(models)) ax.invert_yaxis() ax.set_aspect(\"equal\")  ax.set_xlabel(\"Variables\", fontsize=16) ax.set_ylabel(\"\", fontsize=16) ax.set_title(\"Portrait Plot (RMSE &amp; CORR)\", fontsize=18)  ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)  # ------------------------------------------------------- # Colorbars (larger, better spacing) # ------------------------------------------------------- cbar_rmse = fig.colorbar(     cm.ScalarMappable(norm=rmse_norm, cmap=cmap_rmse),     ax=ax,     fraction=0.05,     pad=0.02,     shrink=0.8,     aspect=20 ) cbar_rmse.set_label(\"Root Mean Sq Error\", fontsize=14) cbar_rmse.ax.tick_params(labelsize=12)  cbar_corr = fig.colorbar(     cm.ScalarMappable(norm=corr_norm, cmap=cmap_corr),     ax=ax,     fraction=0.15,     pad=0.02,     shrink=0.8,     aspect=20 ) cbar_corr.set_label(\"Pearson Correlation Coefficient\", fontsize=14) cbar_corr.ax.tick_params(labelsize=12)  plt.tight_layout() plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/advanced/cmip_obs_comparison/#cmipobservations-comparison-tutorial","title":"CMIP\u2013Observations Comparison (Tutorial)\u00b6","text":"<p>Purpose: Compare CMIP model output with local observations at a single point. This notebook demonstrates extracting observational and CMIP data using <code>climdata</code>, computing standard performance metrics (RMSE, Pearson correlation, MAPE, MNB), and producing visual summaries (boxplots + time series + portrait plot).</p> <p>Prerequisites:</p> <ul> <li><code>climdata</code> package</li> <li>Additional python packages: <code>cftime</code>, <code>hvplot</code>, <code>holoviews</code>, <code>bokeh</code>, <code>matplotlib</code>, <code>numpy</code></li> </ul> <p>How to run:</p> <ol> <li>Run cells top-to-bottom in order. Edit the <code>overrides</code> blocks to change <code>dataset</code>, <code>lat</code>, <code>lon</code>, <code>time_range</code>, <code>variables</code>, and CMIP <code>source_id</code> list.</li> <li>Outputs: an interactive HTML per variable (e.g. <code>distribution_tas.html</code>) and static portrait plot shown inline.</li> </ol> <p>Notes &amp; tips:</p> <ul> <li>The metrics function supports optional time resampling (e.g., <code>time_resample='MS'</code> for monthly means).</li> </ul>"},{"location":"examples/advanced/cmip_obs_comparison/#setup-imports","title":"Setup &amp; imports \ud83d\udd27\u00b6","text":"<p>The first cell imports core packages used in this tutorial. If any imports fail, install requirements via:</p> <pre># plotting support\npip install hvplot holoviews bokeh\n</pre> <p>If you are working on a cluster, ensure the <code>data_dir</code> and any data access permissions are correct before running extraction cells.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#observational-data-extraction-step","title":"Observational data extraction (step)\u00b6","text":"<p>This cell demonstrates extracting local/observational data using <code>ClimData</code> with <code>overrides</code>.</p> <ul> <li><code>dataset</code>: choose dataset (e.g., <code>hyras</code> or <code>mswx</code>)</li> <li><code>lat</code>/<code>lon</code>: location for point extraction</li> <li><code>time_range.start_date</code> and <code>.end_date</code>: extraction period</li> <li><code>variables</code>: list of variable names to extract</li> </ul> <p>After running this cell you should have:</p> <ul> <li><code>ds_obs</code>: an xarray Dataset with observation variables</li> <li><code>df_obs</code>: a Pandas long-form DataFrame (<code>time</code>, <code>variable</code>, <code>value</code>, <code>units</code>, <code>source</code>)</li> </ul> <p>The notebook converts <code>time</code> values to <code>cftime.DatetimeNoLeap</code> where needed to support calendar-aware comparisons.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#cmip-models-extraction-step","title":"CMIP models extraction (step)\u00b6","text":"<p>This loop extracts CMIP model output for a list of <code>source_id</code> values and concatenates models along <code>source_id</code>:</p> <ul> <li><code>dataset=cmip</code>, choose <code>experiment_id</code> (e.g., <code>historical</code>)</li> <li><code>source_id</code>: set a single model name per loop iteration (e.g., <code>CanESM5</code>)</li> <li><code>table_id</code>: typically <code>Amon</code> for monthly output</li> <li>Result variables stored in <code>ds_cmip</code> (xarray, concatenated) and <code>df_cmip</code> (Pandas long-form concatenated DataFrame)</li> </ul> <p>Tip: narrow <code>time_range</code> to speed up downloads and processing during testing.</p>"},{"location":"examples/advanced/cmip_obs_comparison/#compute-cmip-vs-observations-metrics","title":"Compute CMIP-vs-Observations metrics \ud83e\uddee\u00b6","text":"<p>This cell defines <code>compute_cmip_metrics(df_cmip, df_obs, variables=None, time_resample=None)</code> which:</p> <ul> <li>Aggregates spatially (mean over lat/lon), optionally resamples time (e.g., monthly <code>'MS'</code>), and computes:<ul> <li>RMSE (Root Mean Square Error)</li> <li>CORR (Pearson correlation)</li> <li>MAPE (Mean Absolute Percentage Error)</li> <li>MNB (Mean Normalized Bias)</li> </ul> </li> </ul> <p>Output: a DataFrame <code>metrics_df</code> with one row per (<code>variable</code>,<code>source_id</code>) and columns like <code>RMSE</code>, <code>CORR</code>, <code>MAPE</code>, <code>MNB</code>, <code>Model Mean</code>, <code>Observation Mean</code>.</p> <p>Notes:</p> <ul> <li>The function converts <code>cftime</code> dates to <code>pandas.Timestamp</code> where possible for reliable joins and resampling.</li> <li>Missing or misaligned dates between model and observations are handled by an inner merge on <code>time</code> in this implementation.</li> </ul>"},{"location":"examples/advanced/cmip_obs_comparison/#visualization-outputs","title":"Visualization &amp; outputs \ud83d\udcca\u00b6","text":"<p>This section creates two linked visual summaries per selected variable:</p> <ul> <li>Top panel: a box plot of model value distributions by <code>source_id</code> combined with point markers showing model mean and hover details (RMSE, CORR, MAPE, MNB, Obs mean).</li> <li>Bottom panel: time series of each model (by <code>source_id</code>) with observations overplotted (if available).</li> </ul> <p>Files saved:</p> <ul> <li><code>distribution_&lt;variable&gt;.html</code> \u2014 interactive Holoviews/HvPlot HTML for the chosen variable.</li> <li>Inline portrait plot created with Matplotlib shows per-model RMSE (upper-left triangle) and correlation (lower-right triangle).</li> </ul> <p>Tip: Change <code>var</code> to iterate through variables and call <code>hv.save()</code> for each variable to persist results to disk.</p>"},{"location":"examples/datasets/cmip_nex_gddp/","title":"Cmip nex gddp","text":"In\u00a0[1]: Copied! <pre>from climdata import ClimData\nimport pandas as pd\nimport xarray as xr\n\nimport logging\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(levelname)s | %(message)s\",\n    force=True,\n)\n</pre> from climdata import ClimData import pandas as pd import xarray as xr  import logging  logging.basicConfig(     level=logging.INFO,     format=\"%(levelname)s | %(message)s\",     force=True, ) In\u00a0[2]: Copied! <pre>overrides = [\n    \"dataset=nexgddp\",  # Choose the MSWX dataset for extraction\n    \"lat=52\",\n    \"lon=13\",\n    f\"time_range.start_date=2004-01-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    \"source_id=GFDL-ESM4\",\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n    \"index=tn10p\",  # Climate extreme index to calculate\n    \"impute=BRITS\"\n]\n\n# -----------------------------\n# Step 3: Define the workflow sequence\n# -----------------------------\nseq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]\n\n# -----------------------------\n# Step 4: Initialize the ClimData extractor\n# -----------------------------\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Step 5: Run the Multi-Step workflow\n# -----------------------------\nresult = extractor.run_workflow(\n    actions=seq,\n)\n</pre> overrides = [     \"dataset=nexgddp\",  # Choose the MSWX dataset for extraction     \"lat=52\",     \"lon=13\",     f\"time_range.start_date=2004-01-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     \"source_id=GFDL-ESM4\",     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download     \"index=tn10p\",  # Climate extreme index to calculate     \"impute=BRITS\" ]  # ----------------------------- # Step 3: Define the workflow sequence # ----------------------------- seq = [\"extract\", \"impute\", \"calc_index\", \"to_nc\"]  # ----------------------------- # Step 4: Initialize the ClimData extractor # ----------------------------- extractor = ClimData(overrides=overrides)  # ----------------------------- # Step 5: Run the Multi-Step workflow # ----------------------------- result = extractor.run_workflow(     actions=seq, ) <pre>INFO | Starting action: extract\n</pre> <pre>\ud83d\udd0d Auto-discovering metadata for GFDL-ESM4/historical...\n   Checking available realizations...\n   \u2713 Found: r1i1p1f1 (grid: gr1)\n\u2713 Discovered grid_label: gr1\n\ud83d\udd0d Downloading NEX-GDDP-CMIP6 data from NASA THREDDS...\n   Model: GFDL-ESM4, Experiment: historical\n\n\ud83d\udce5 Fetching tasmin (Daily Minimum Near-Surface Air Temperature)...\n</pre> <pre>  Downloading tasmin:   9%|\u2589         | 1/11 [03:22&lt;33:43, 202.36s/it]</pre> <pre>  \u274c Failed to download tasmin_day_GFDL-ESM4_historical_r1i1p1f1_gr1_2004_v2.0.nc: ('Connection broken: IncompleteRead(17931946 bytes read, 230256302 more expected)', IncompleteRead(17931946 bytes read, 230256302 more expected))\n</pre> <pre>  Downloading tasmin:  18%|\u2588\u258a        | 2/11 [11:15&lt;54:13, 361.45s/it]</pre> <pre>  \u274c Failed to download tasmin_day_GFDL-ESM4_historical_r1i1p1f1_gr1_2005_v2.0.nc: ('Connection broken: IncompleteRead(107805362 bytes read, 141070476 more expected)', IncompleteRead(107805362 bytes read, 141070476 more expected))\n</pre> <pre>  Downloading tasmin:  27%|\u2588\u2588\u258b       | 3/11 [13:18&lt;35:29, 266.21s/it]\n</pre> <pre>Unexpected exception formatting exception. Falling back to standard exception\n</pre> <pre>Traceback (most recent call last):\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_323365/4280736918.py\", line 28, in &lt;module&gt;\n    result = extractor.run_workflow(\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 824, in run_workflow\n    self.extract()\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 48, in wrapper\n    ds = func(self, *args, **kwargs)\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py\", line 593, in extract\n    nexgddp.fetch()  # Download NEX-GDDP-CMIP6 data from NASA THREDDS\n  File \"/beegfs/muduchuru/pkgs_fnl/climdata/climdata/datasets/NEXGDDP.py\", line 483, in fetch\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/requests/models.py\", line 820, in generate\n    yield from self.raw.stream(chunk_size, decode_content=True)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/urllib3/response.py\", line 1066, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/urllib3/response.py\", line 955, in read\n    data = self._raw_read(amt)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/urllib3/response.py\", line 879, in _raw_read\n    data = self._fp_read(amt, read1=read1) if not fp_closed else b\"\"\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/urllib3/response.py\", line 862, in _fp_read\n    return self._fp.read(amt) if amt is not None else self._fp.read()\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/http/client.py\", line 466, in read\n    s = self.fp.read(amt)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/socket.py\", line 717, in readinto\n    return self._sock.recv_into(b)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/ssl.py\", line 1307, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/ssl.py\", line 1163, in read\n    return self._sslobj.read(len, buffer)\nKeyboardInterrupt\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2105, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1396, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1287, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1055, in format_exception_as_a_whole\n    frames.append(self.format_record(record))\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 955, in format_record\n    frame_info.lines, Colors, self.has_colors, lvals\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 778, in lines\n    return self._sd.lines\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n    pieces = self.included_pieces\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n    return only(\n  File \"/home/muduchuru/miniforge3/envs/sdba/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n</pre> In\u00a0[7]: Copied! <pre>from climdata.datasets.NEXGDDP import NEXGDDP\nnex = NEXGDDP(extractor.cfg)\n</pre> from climdata.datasets.NEXGDDP import NEXGDDP nex = NEXGDDP(extractor.cfg) In\u00a0[8]: Copied! <pre>nex._construct_download_url('pr','2005')\n</pre> nex._construct_download_url('pr','2005') Out[8]: <pre>('https://ds.nccs.nasa.gov/thredds/ncss/grid/AMES/NEX/GDDP-CMIP6/MRI-ESM2-0/historical/r1i1p1f1/pr/pr_day_MRI-ESM2-0_historical_r1i1p1f1_gn_2005_v2.0.nc?var=pr&amp;north=90&amp;south=-90&amp;east=180&amp;west=-180&amp;horizStride=1&amp;time_start=2005-01-01T12:00:00Z&amp;time_end=2005-12-31T12:00:00Z&amp;accept=netcdf3&amp;addLatLon=true',\n 'pr_day_MRI-ESM2-0_historical_r1i1p1f1_gn_2005_v2.0.nc')</pre>"},{"location":"examples/datasets/cmip_w5e5_example/","title":"CMIP Downscaled (0.5 deg)","text":"In\u00a0[1]: Copied! <pre>from climdata.utils.config import load_config\nfrom climdata.datasets.CMIP_W5E5 import CMIPW5E5\nfrom climdata import ClimData\n\n# Configure for Berlin, Germany\noverrides = [\n    \"dataset=cmip_w5e5\",                  # Select CMIP-W5E5 dataset\n    \"lat=52.52\",                          # Berlin latitude\n    \"lon=13.40\",                          # Berlin longitude\n    \"experiment_id=ssp585\",               # Climate scenario (ssp585 = high emissions)\n    \"source_id=gfdl-esm4\",                # CMIP6 model\n    \"time_range.start_date=2010-01-01\",   # Future period\n    \"time_range.end_date=2010-12-31\",\n    \"variables=[tas,tasmin,tasmax,pr]\",   # Temperature and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",                    # Local directory for downloaded files\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Initialize CMIP-W5E5 dataset\ncmip_w5e5 = CMIPW5E5(extractor.cfg)\n\n# Fetch data from ISIMIP repository\ncmip_w5e5.fetch()\n\n# Load into xarray\ncmip_w5e5.load()\n\n# Extract for point location\ncmip_w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))\n</pre> from climdata.utils.config import load_config from climdata.datasets.CMIP_W5E5 import CMIPW5E5 from climdata import ClimData  # Configure for Berlin, Germany overrides = [     \"dataset=cmip_w5e5\",                  # Select CMIP-W5E5 dataset     \"lat=52.52\",                          # Berlin latitude     \"lon=13.40\",                          # Berlin longitude     \"experiment_id=ssp585\",               # Climate scenario (ssp585 = high emissions)     \"source_id=gfdl-esm4\",                # CMIP6 model     \"time_range.start_date=2010-01-01\",   # Future period     \"time_range.end_date=2010-12-31\",     \"variables=[tas,tasmin,tasmax,pr]\",   # Temperature and precipitation     \"data_dir=/beegfs/muduchuru/data\",                    # Local directory for downloaded files ]  extractor = ClimData(overrides=overrides)  # Initialize CMIP-W5E5 dataset cmip_w5e5 = CMIPW5E5(extractor.cfg)  # Fetch data from ISIMIP repository cmip_w5e5.fetch()  # Load into xarray cmip_w5e5.load()  # Extract for point location cmip_w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat)) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[1], line 21\n     18 extractor = ClimData(overrides=overrides)\n     20 # Initialize CMIP-W5E5 dataset\n---&gt; 21 cmip_w5e5 = CMIPW5E5(extractor.cfg)\n     23 # Fetch data from ISIMIP repository\n     24 cmip_w5e5.fetch()\n\nFile /beegfs/muduchuru/pkgs_fnl/climdata/climdata/datasets/CMIP_W5E5.py:69, in CMIPW5E5.__init__(self, cfg)\n     63     raise ImportError(\n     64         \"isimip-client is required for CMIP-W5E5 data access. \"\n     65         \"Install it with: pip install isimip-client\"\n     66     )\n     68 # Validate time range for experiment\n---&gt; 69 self._validate_time_range()\n\nFile /beegfs/muduchuru/pkgs_fnl/climdata/climdata/datasets/CMIP_W5E5.py:114, in CMIPW5E5._validate_time_range(self)\n    112 # Check if requested period is outside valid range\n    113 if end_year &lt; valid_start or start_year &gt; valid_end:\n--&gt; 114     raise ValueError(\n    115         f\"\u274c Time range mismatch for experiment '{self.experiment_id}'!\\n\"\n    116         f\"   Requested: {start_year}-{end_year}\\n\"\n    117         f\"   Valid period for {period_name}: {valid_start}-{valid_end}\\n\"\n    118         f\"   \\n\"\n    119         f\"   Hint: Use 'historical' for years 1850-2014, and SSP scenarios (ssp126, ssp370, ssp585) for 2015-2100.\"\n    120     )\n    122 # Warn if requested period extends beyond valid range\n    123 if start_year &lt; valid_start or end_year &gt; valid_end:\n\nValueError: \u274c Time range mismatch for experiment 'ssp585'!\n   Requested: 2010-2010\n   Valid period for SSP scenario (ssp585): 2015-2100\n   \n   Hint: Use 'historical' for years 1850-2014, and SSP scenarios (ssp126, ssp370, ssp585) for 2015-2100.</pre> In\u00a0[2]: Copied! <pre># View the dataset\ncmip_w5e5.ds\n</pre> # View the dataset cmip_w5e5.ds Out[2]: <pre>&lt;xarray.Dataset&gt; Size: 9kB\nDimensions:  (time: 365)\nCoordinates:\n    lon      float64 8B 13.25\n    lat      float64 8B 52.75\n  * time     (time) datetime64[ns] 3kB 2050-01-01 2050-01-02 ... 2050-12-31\nData variables:\n    tas      (time) float32 1kB ...\n    tasmin   (time) float32 1kB ...\n    tasmax   (time) float32 1kB ...\n    pr       (time) float32 1kB ...\nAttributes:\n    institution:    Potsdam Institute for Climate Impact Research (PIK)\n    contact:        ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;htt...\n    references:     Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; a...\n    title:          ISIMIP3b bias-adjusted climate input data\n    project:        Inter-Sectoral Impact Model Intercomparison Project phase...\n    summary:        CMIP6 daily output data bias-adjusted and statistically d...\n    source:         CMIP6 gfdl-esm4 via ISIMIP\n    dataset:        CMIP6-W5E5\n    experiment_id:  ssp585\n    source_id:      gfdl-esm4\n    member_id:      r1i1p1f1\n    description:    CMIP6 ssp585 scenario from gfdl-esm4 in W5E5 format</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 365</li></ul></li><li>Coordinates: (3)<ul><li>lon()float6413.25standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(13.25)</pre></li><li>lat()float6452.75standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.75)</pre></li><li>time(time)datetime64[ns]2050-01-01 ... 2050-12-31standard_name :timelong_name :timeaxis :T<pre>array(['2050-01-01T00:00:00.000000000', '2050-01-02T00:00:00.000000000',\n       '2050-01-03T00:00:00.000000000', ..., '2050-12-29T00:00:00.000000000',\n       '2050-12-30T00:00:00.000000000', '2050-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (4)<ul><li>tas(time)float32...standard_name :air_temperaturelong_name :Near-Surface Air Temperatureunits :K<pre>[365 values with dtype=float32]</pre></li><li>tasmin(time)float32...standard_name :air_temperaturelong_name :Daily Minimum Near-Surface Air Temperatureunits :K<pre>[365 values with dtype=float32]</pre></li><li>tasmax(time)float32...standard_name :air_temperaturelong_name :Daily Maximum Near-Surface Air Temperatureunits :K<pre>[365 values with dtype=float32]</pre></li><li>pr(time)float32...standard_name :precipitation_fluxlong_name :Precipitationunits :kg m-2 s-1<pre>[365 values with dtype=float32]</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2050-01-01', '2050-01-02', '2050-01-03', '2050-01-04',\n               '2050-01-05', '2050-01-06', '2050-01-07', '2050-01-08',\n               '2050-01-09', '2050-01-10',\n               ...\n               '2050-12-22', '2050-12-23', '2050-12-24', '2050-12-25',\n               '2050-12-26', '2050-12-27', '2050-12-28', '2050-12-29',\n               '2050-12-30', '2050-12-31'],\n              dtype='datetime64[ns]', name='time', length=365, freq=None))</pre></li></ul></li><li>Attributes: (12)institution :Potsdam Institute for Climate Impact Research (PIK)contact :ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https://www.isimip.org&gt;references :Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; and Lange (2021) &lt;https://doi.org/10.5281/zenodo.4686991&gt; for ISIMIP3BASD; Cucchi et al. (2020) &lt;https://doi.org/10.5194/essd-2020-28&gt; and Lange et al. (2021) &lt;https://doi.org/10.48364/ISIMIP.342217&gt; for W5E5title :ISIMIP3b bias-adjusted climate input dataproject :Inter-Sectoral Impact Model Intercomparison Project phase 3b (ISIMIP3b)summary :CMIP6 daily output data bias-adjusted and statistically downscaled to 0.5 degree horizontal resolution using ISIMIP3BASD v2.5.0 and W5E5 v2.0source :CMIP6 gfdl-esm4 via ISIMIPdataset :CMIP6-W5E5experiment_id :ssp585source_id :gfdl-esm4member_id :r1i1p1f1description :CMIP6 ssp585 scenario from gfdl-esm4 in W5E5 format</li></ul> In\u00a0[4]: Copied! <pre>from climdata.datasets.CMIP_W5E5 import CMIPW5E5\nfrom climdata.utils.config import load_config\n\ncmip_w5e5 = CMIPW5E5(extractor.cfg)\n\n# Get available experiment IDs\nexperiments = cmip_w5e5.get_experiment_ids()\nprint(\"Available CMIP6 Experiments:\")\nfor exp in experiments:\n    print(f\"  - {exp}\")\n</pre> from climdata.datasets.CMIP_W5E5 import CMIPW5E5 from climdata.utils.config import load_config  cmip_w5e5 = CMIPW5E5(extractor.cfg)  # Get available experiment IDs experiments = cmip_w5e5.get_experiment_ids() print(\"Available CMIP6 Experiments:\") for exp in experiments:     print(f\"  - {exp}\") <pre>\ud83d\udd0d Fetching available experiment IDs from ISIMIP...\n\u2705 Found 1 experiment IDs: ['historical']\nAvailable CMIP6 Experiments:\n  - historical\n</pre> In\u00a0[5]: Copied! <pre># Get available models for a specific experiment\nmodels = cmip_w5e5.get_source_ids(experiment_id='ssp585')\nprint(\"\\nAvailable Models for SSP5-8.5:\")\nfor model in models:\n    print(f\"  - {model}\")\n</pre> # Get available models for a specific experiment models = cmip_w5e5.get_source_ids(experiment_id='ssp585') print(\"\\nAvailable Models for SSP5-8.5:\") for model in models:     print(f\"  - {model}\") <pre>\ud83d\udd0d Fetching available source IDs for experiment 'ssp585'...\n\u2705 Found 5 source IDs: ['gfdl-esm4', 'ipsl-cm6a-lr', 'mpi-esm1-2-hr', 'mri-esm2-0', 'ukesm1-0-ll']\n\nAvailable Models for SSP5-8.5:\n  - gfdl-esm4\n  - ipsl-cm6a-lr\n  - mpi-esm1-2-hr\n  - mri-esm2-0\n  - ukesm1-0-ll\n</pre> In\u00a0[9]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nfrom climdata import ClimData\nfrom climdata.datasets.CMIP_W5E5 import CMIPW5E5\n# Function to load data for a scenario\ndef load_scenario(scenario, model='gfdl-esm4'):\n    overrides = [\n        \"dataset=cmip_w5e5\",\n        \"lat=52.52\",\n        \"lon=13.40\",\n        f\"experiment_id={scenario}\",\n        f\"source_id={model}\",\n        \"time_range.start_date=2050-01-01\",\n        \"time_range.end_date=2050-12-31\",\n        \"variables=[tasmin]\",\n        \"data_dir=/beegfs/muduchuru/data\",\n    ]\n    extractor = ClimData(overrides=overrides)\n    cmip = CMIPW5E5(extractor.cfg)\n    cmip.fetch()\n    cmip.load()\n    cmip.extract(point=(extractor.cfg.lon, extractor.cfg.lat))\n    return cmip.ds\n\n# Load low and high emission scenarios\nds_low = load_scenario('ssp126')  # Low emissions\nds_high = load_scenario('ssp585') # High emissions\n\n# Plot comparison\nplt.figure(figsize=(12, 6))\nds_low['tasmin'].plot(label='SSP1-2.6 (Low emissions)', alpha=0.7)\nds_high['tasmin'].plot(label='SSP5-8.5 (High emissions)', alpha=0.7)\nplt.title('Future Temperature Projections for Berlin')\nplt.ylabel('Temperature (K)')\nplt.xlabel('Year')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt from climdata import ClimData from climdata.datasets.CMIP_W5E5 import CMIPW5E5 # Function to load data for a scenario def load_scenario(scenario, model='gfdl-esm4'):     overrides = [         \"dataset=cmip_w5e5\",         \"lat=52.52\",         \"lon=13.40\",         f\"experiment_id={scenario}\",         f\"source_id={model}\",         \"time_range.start_date=2050-01-01\",         \"time_range.end_date=2050-12-31\",         \"variables=[tasmin]\",         \"data_dir=/beegfs/muduchuru/data\",     ]     extractor = ClimData(overrides=overrides)     cmip = CMIPW5E5(extractor.cfg)     cmip.fetch()     cmip.load()     cmip.extract(point=(extractor.cfg.lon, extractor.cfg.lat))     return cmip.ds  # Load low and high emission scenarios ds_low = load_scenario('ssp126')  # Low emissions ds_high = load_scenario('ssp585') # High emissions  # Plot comparison plt.figure(figsize=(12, 6)) ds_low['tasmin'].plot(label='SSP1-2.6 (Low emissions)', alpha=0.7) ds_high['tasmin'].plot(label='SSP5-8.5 (High emissions)', alpha=0.7) plt.title('Future Temperature Projections for Berlin') plt.ylabel('Temperature (K)') plt.xlabel('Year') plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() <pre>\ud83d\udd0d Searching for CMIP6 datasets in ISIMIP repository...\n   Model: gfdl-esm4, Experiment: ssp126\n\n\ud83d\udce5 Fetching tasmin...\n\u2705 Found dataset: gfdl-esm4_r1i1p1f1_w5e5_ssp126_tasmin_global_daily\n  \u2713 Already exists: gfdl-esm4_r1i1p1f1_w5e5_ssp126_tasmin_global_daily_2041_2050.nc\n\n\u2705 Downloaded 1 files\n\ud83d\udcc2 Loading 1 CMIP6 files...\n  Loading tasmin from 1 file(s)...\n\u2705 Loaded dataset with 1 variables\n\ud83d\udd0d Searching for CMIP6 datasets in ISIMIP repository...\n   Model: gfdl-esm4, Experiment: ssp585\n\n\ud83d\udce5 Fetching tasmin...\n\u2705 Found dataset: gfdl-esm4_r1i1p1f1_w5e5_ssp585_tasmin_global_daily\n  \u2713 Already exists: gfdl-esm4_r1i1p1f1_w5e5_ssp585_tasmin_global_daily_2041_2050.nc\n\n\u2705 Downloaded 1 files\n\ud83d\udcc2 Loading 1 CMIP6 files...\n  Loading tasmin from 1 file(s)...\n\u2705 Loaded dataset with 1 variables\n</pre> In\u00a0[1]: Copied! <pre>from climdata.datasets.CMIP_W5E5 import CMIPW5E5\nfrom climdata import ClimData\n\n# Configure for Central Europe region\noverrides = [\n    \"dataset=cmip_w5e5\",\n    \"experiment_id=ssp126\",                # Middle-of-the-road scenario\n    \"source_id=gfdl-esm4\",               # UK Earth System Model\n    \"time_range.start_date=2050-01-01\",\n    \"time_range.end_date=2050-12-31\",\n    \"variables=[tas,pr]\",\n    \"data_dir=/beegfs/muduchuru/data\",\n]\n\nextractor = ClimData(overrides=overrides)\ncmip_w5e5 = CMIPW5E5(extractor.cfg)\n\n# Fetch and load\ncmip_w5e5.fetch()\ncmip_w5e5.load()\n\n# Extract for Central Europe bounding box\nbox = {\n    'lon_min': 5.0,   # Western boundary\n    'lon_max': 15.0,  # Eastern boundary\n    'lat_min': 47.0,  # Southern boundary\n    'lat_max': 55.0   # Northern boundary\n}\ncmip_w5e5.extract(box=box)\n\n# View spatial data\ncmip_w5e5.ds\n</pre> from climdata.datasets.CMIP_W5E5 import CMIPW5E5 from climdata import ClimData  # Configure for Central Europe region overrides = [     \"dataset=cmip_w5e5\",     \"experiment_id=ssp126\",                # Middle-of-the-road scenario     \"source_id=gfdl-esm4\",               # UK Earth System Model     \"time_range.start_date=2050-01-01\",     \"time_range.end_date=2050-12-31\",     \"variables=[tas,pr]\",     \"data_dir=/beegfs/muduchuru/data\", ]  extractor = ClimData(overrides=overrides) cmip_w5e5 = CMIPW5E5(extractor.cfg)  # Fetch and load cmip_w5e5.fetch() cmip_w5e5.load()  # Extract for Central Europe bounding box box = {     'lon_min': 5.0,   # Western boundary     'lon_max': 15.0,  # Eastern boundary     'lat_min': 47.0,  # Southern boundary     'lat_max': 55.0   # Northern boundary } cmip_w5e5.extract(box=box)  # View spatial data cmip_w5e5.ds <pre>\ud83d\udd0d Searching for CMIP6 datasets in ISIMIP repository...\n   Model: gfdl-esm4, Experiment: ssp126\n\n\ud83d\udce5 Fetching tas...\n\u2705 Found dataset: gfdl-esm4_r1i1p1f1_w5e5_ssp126_tas_global_daily\n  \u2713 Already exists: gfdl-esm4_r1i1p1f1_w5e5_ssp126_tas_global_daily_2041_2050.nc\n\n\ud83d\udce5 Fetching pr...\n\u2705 Found dataset: gfdl-esm4_r1i1p1f1_w5e5_ssp126_pr_global_daily\n  \u2713 Already exists: gfdl-esm4_r1i1p1f1_w5e5_ssp126_pr_global_daily_2041_2050.nc\n\n\u2705 Downloaded 2 files\n\ud83d\udcc2 Loading 2 CMIP6 files...\n  Loading tas from 1 file(s)...\n  Loading pr from 1 file(s)...\n\u2705 Loaded dataset with 2 variables\n</pre> Out[1]: <pre>&lt;xarray.Dataset&gt; Size: 938kB\nDimensions:  (lat: 16, time: 365, lon: 20)\nCoordinates:\n  * lat      (lat) float64 128B 47.25 47.75 48.25 48.75 ... 53.75 54.25 54.75\n  * time     (time) datetime64[ns] 3kB 2050-01-01 2050-01-02 ... 2050-12-31\n  * lon      (lon) float64 160B 5.25 5.75 6.25 6.75 ... 13.25 13.75 14.25 14.75\nData variables:\n    tas      (time, lat, lon) float32 467kB ...\n    pr       (time, lat, lon) float32 467kB ...\nAttributes:\n    institution:    Potsdam Institute for Climate Impact Research (PIK)\n    contact:        ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;htt...\n    references:     Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; a...\n    title:          ISIMIP3b bias-adjusted climate input data\n    project:        Inter-Sectoral Impact Model Intercomparison Project phase...\n    summary:        CMIP6 daily output data bias-adjusted and statistically d...\n    source:         CMIP6 gfdl-esm4 via ISIMIP\n    dataset:        CMIP6-W5E5\n    experiment_id:  ssp126\n    source_id:      gfdl-esm4\n    member_id:      r1i1p1f1\n    description:    CMIP6 ssp126 scenario from gfdl-esm4 in W5E5 format</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>lat: 16</li><li>time: 365</li><li>lon: 20</li></ul></li><li>Coordinates: (3)<ul><li>lat(lat)float6447.25 47.75 48.25 ... 54.25 54.75standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array([47.25, 47.75, 48.25, 48.75, 49.25, 49.75, 50.25, 50.75, 51.25, 51.75,\n       52.25, 52.75, 53.25, 53.75, 54.25, 54.75])</pre></li><li>time(time)datetime64[ns]2050-01-01 ... 2050-12-31standard_name :timelong_name :timeaxis :T<pre>array(['2050-01-01T00:00:00.000000000', '2050-01-02T00:00:00.000000000',\n       '2050-01-03T00:00:00.000000000', ..., '2050-12-29T00:00:00.000000000',\n       '2050-12-30T00:00:00.000000000', '2050-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lon(lon)float645.25 5.75 6.25 ... 14.25 14.75<pre>array([ 5.25,  5.75,  6.25,  6.75,  7.25,  7.75,  8.25,  8.75,  9.25,  9.75,\n       10.25, 10.75, 11.25, 11.75, 12.25, 12.75, 13.25, 13.75, 14.25, 14.75])</pre></li></ul></li><li>Data variables: (2)<ul><li>tas(time, lat, lon)float32...standard_name :air_temperaturelong_name :Near-Surface Air Temperatureunits :K<pre>[116800 values with dtype=float32]</pre></li><li>pr(time, lat, lon)float32...standard_name :precipitation_fluxlong_name :Precipitationunits :kg m-2 s-1<pre>[116800 values with dtype=float32]</pre></li></ul></li><li>Indexes: (3)<ul><li>latPandasIndex<pre>PandasIndex(Index([47.25, 47.75, 48.25, 48.75, 49.25, 49.75, 50.25, 50.75, 51.25, 51.75,\n       52.25, 52.75, 53.25, 53.75, 54.25, 54.75],\n      dtype='float64', name='lat'))</pre></li><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2050-01-01', '2050-01-02', '2050-01-03', '2050-01-04',\n               '2050-01-05', '2050-01-06', '2050-01-07', '2050-01-08',\n               '2050-01-09', '2050-01-10',\n               ...\n               '2050-12-22', '2050-12-23', '2050-12-24', '2050-12-25',\n               '2050-12-26', '2050-12-27', '2050-12-28', '2050-12-29',\n               '2050-12-30', '2050-12-31'],\n              dtype='datetime64[ns]', name='time', length=365, freq=None))</pre></li><li>lonPandasIndex<pre>PandasIndex(Index([ 5.25,  5.75,  6.25,  6.75,  7.25,  7.75,  8.25,  8.75,  9.25,  9.75,\n       10.25, 10.75, 11.25, 11.75, 12.25, 12.75, 13.25, 13.75, 14.25, 14.75],\n      dtype='float64', name='lon'))</pre></li></ul></li><li>Attributes: (12)institution :Potsdam Institute for Climate Impact Research (PIK)contact :ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https://www.isimip.org&gt;references :Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; and Lange (2021) &lt;https://doi.org/10.5281/zenodo.4686991&gt; for ISIMIP3BASD; Cucchi et al. (2020) &lt;https://doi.org/10.5194/essd-2020-28&gt; and Lange et al. (2021) &lt;https://doi.org/10.48364/ISIMIP.342217&gt; for W5E5title :ISIMIP3b bias-adjusted climate input dataproject :Inter-Sectoral Impact Model Intercomparison Project phase 3b (ISIMIP3b)summary :CMIP6 daily output data bias-adjusted and statistically downscaled to 0.5 degree horizontal resolution using ISIMIP3BASD v2.5.0 and W5E5 v2.0source :CMIP6 gfdl-esm4 via ISIMIPdataset :CMIP6-W5E5experiment_id :ssp126source_id :gfdl-esm4member_id :r1i1p1f1description :CMIP6 ssp126 scenario from gfdl-esm4 in W5E5 format</li></ul> In\u00a0[9]: Copied! <pre># Plot spatial map for a specific day\nimport matplotlib.pyplot as plt\nimport hvplot.xarray\n# Select first day\ncmip_w5e5.ds['tas'].isel(time=0).hvplot(figsize=(10, 8),coastline=True,geo=True)\n</pre> # Plot spatial map for a specific day import matplotlib.pyplot as plt import hvplot.xarray # Select first day cmip_w5e5.ds['tas'].isel(time=0).hvplot(figsize=(10, 8),coastline=True,geo=True) <pre>WARNING:param.main: hvPlot does not have the concept of a figure, and the figsize keyword will be ignored. The size of each subplot in a layout is set individually using the width and height options.\n</pre> Out[9]: In\u00a0[\u00a0]: Copied! <pre>from climdata.datasets.CMIP_W5E5 import CMIPW5E5\nfrom climdata import ClimData\n\n# Configure and extract\noverrides = [\n    \"dataset=cmip_w5e5\",\n    \"lat=52.52\",\n    \"lon=13.40\",\n    \"experiment_id=ssp370\",\n    \"source_id=mpi-esm1-2-hr\",\n    \"time_range.start_date=2080-01-01\",\n    \"time_range.end_date=2080-12-31\",\n    \"variables=[tas,pr,rsds]\",\n    \"data_dir=./data\",\n]\n\nextractor = ClimData(overrides=overrides)\ncmip_w5e5 = CMIPW5E5(extractor.cfg)\ncmip_w5e5.fetch()\ncmip_w5e5.load()\ncmip_w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))\n\n# Save as NetCDF\ncmip_w5e5.save_netcdf('berlin_ssp370_2080.nc')\n\n# Save as CSV\ncmip_w5e5.save_csv('berlin_ssp370_2080.csv')\n\nprint(\"\u2705 Data saved successfully!\")\n</pre> from climdata.datasets.CMIP_W5E5 import CMIPW5E5 from climdata import ClimData  # Configure and extract overrides = [     \"dataset=cmip_w5e5\",     \"lat=52.52\",     \"lon=13.40\",     \"experiment_id=ssp370\",     \"source_id=mpi-esm1-2-hr\",     \"time_range.start_date=2080-01-01\",     \"time_range.end_date=2080-12-31\",     \"variables=[tas,pr,rsds]\",     \"data_dir=./data\", ]  extractor = ClimData(overrides=overrides) cmip_w5e5 = CMIPW5E5(extractor.cfg) cmip_w5e5.fetch() cmip_w5e5.load() cmip_w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))  # Save as NetCDF cmip_w5e5.save_netcdf('berlin_ssp370_2080.nc')  # Save as CSV cmip_w5e5.save_csv('berlin_ssp370_2080.csv')  print(\"\u2705 Data saved successfully!\") In\u00a0[\u00a0]: Copied! <pre>from climdata.utils.wrapper_workflow import ClimateExtractor\n\n# Configure with overrides\noverrides = [\n    \"dataset=cmip_w5e5\",\n    \"lat=12.89\",\n    \"lon=24.25\",\n    \"experiment_id=ssp585\",\n    \"source_id=gfdl-esm4\",\n    \"time_range.start_date=2050-01-01\",\n    \"time_range.end_date=2050-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n    \"data_dir=./data\",\n]\n\n# Initialize extractor\nextractor = ClimateExtractor(overrides=overrides)\n\n# Extract data\nds = extractor.extract()\n\n# Convert to DataFrame\ndf = extractor.to_dataframe()\n\n# Save results\nresult = extractor.save(format='csv')\n\nprint(f\"Data extracted and saved to: {result.filename}\")\ndf.head()\n</pre> from climdata.utils.wrapper_workflow import ClimateExtractor  # Configure with overrides overrides = [     \"dataset=cmip_w5e5\",     \"lat=12.89\",     \"lon=24.25\",     \"experiment_id=ssp585\",     \"source_id=gfdl-esm4\",     \"time_range.start_date=2050-01-01\",     \"time_range.end_date=2050-12-31\",     \"variables=[tasmin,tasmax,pr]\",     \"data_dir=./data\", ]  # Initialize extractor extractor = ClimateExtractor(overrides=overrides)  # Extract data ds = extractor.extract()  # Convert to DataFrame df = extractor.to_dataframe()  # Save results result = extractor.save(format='csv')  print(f\"Data extracted and saved to: {result.filename}\") df.head() In\u00a0[\u00a0]: Copied! <pre>from climdata.utils.wrapper_workflow import ClimateExtractor\n\n# Configure with climate index\noverrides = [\n    \"dataset=cmip_w5e5\",\n    \"lat=52.52\",\n    \"lon=13.40\",\n    \"experiment_id=ssp585\",\n    \"source_id=gfdl-esm4\",\n    \"time_range.start_date=2050-01-01\",\n    \"time_range.end_date=2079-12-31\",  # 30 years for climate indices\n    \"variables=[tasmin,tasmax,pr]\",\n    \"index=tn10p\",  # Cold nights (10th percentile of minimum temperature)\n    \"data_dir=./data\",\n]\n\nextractor = ClimateExtractor(overrides=overrides)\n\n# Extract base data\nds = extractor.extract()\n\n# Calculate index\nindex_ds = extractor.calc_index()\n\n# View results\nprint(\"Climate index calculated:\")\nindex_ds\n</pre> from climdata.utils.wrapper_workflow import ClimateExtractor  # Configure with climate index overrides = [     \"dataset=cmip_w5e5\",     \"lat=52.52\",     \"lon=13.40\",     \"experiment_id=ssp585\",     \"source_id=gfdl-esm4\",     \"time_range.start_date=2050-01-01\",     \"time_range.end_date=2079-12-31\",  # 30 years for climate indices     \"variables=[tasmin,tasmax,pr]\",     \"index=tn10p\",  # Cold nights (10th percentile of minimum temperature)     \"data_dir=./data\", ]  extractor = ClimateExtractor(overrides=overrides)  # Extract base data ds = extractor.extract()  # Calculate index index_ds = extractor.calc_index()  # View results print(\"Climate index calculated:\") index_ds"},{"location":"examples/datasets/cmip_w5e5_example/#example-1-basic-usage-point-extraction","title":"Example 1: Basic Usage - Point Extraction\u00b6","text":"<p>Extract climate projection data for a specific location.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-2-explore-available-models-and-scenarios","title":"Example 2: Explore Available Models and Scenarios\u00b6","text":"<p>Discover what CMIP6 experiments and models are available.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-3-compare-multiple-scenarios","title":"Example 3: Compare Multiple Scenarios\u00b6","text":"<p>Compare low and high emission scenarios for the same location.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-4-regional-extraction-with-bounding-box","title":"Example 4: Regional Extraction with Bounding Box\u00b6","text":"<p>Extract data for a region instead of a single point.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-5-save-data-to-file","title":"Example 5: Save Data to File\u00b6","text":"<p>Export extracted data to NetCDF or CSV format.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-6-using-climateextractor-workflow","title":"Example 6: Using ClimateExtractor Workflow\u00b6","text":"<p>Use the high-level <code>ClimateExtractor</code> workflow for end-to-end processing.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#example-7-calculate-climate-indices","title":"Example 7: Calculate Climate Indices\u00b6","text":"<p>Compute extreme indices like frost days or heat waves from CMIP-W5E5 data.</p>"},{"location":"examples/datasets/cmip_w5e5_example/#key-features","title":"Key Features\u00b6","text":""},{"location":"examples/datasets/cmip_w5e5_example/#scenarios-available","title":"Scenarios Available\u00b6","text":"<ul> <li>historical: Historical period (typically 1850-2014)</li> <li>ssp126: Low emissions (sustainability pathway)</li> <li>ssp245: Middle-of-the-road (moderate emissions)</li> <li>ssp370: Medium-high emissions</li> <li>ssp585: High emissions (fossil-fueled development)</li> </ul>"},{"location":"examples/datasets/cmip_w5e5_example/#models-available-in-isimip3b","title":"Models Available in ISIMIP3b\u00b6","text":"<ul> <li>gfdl-esm4: NOAA Geophysical Fluid Dynamics Laboratory</li> <li>ipsl-cm6a-lr: Institut Pierre-Simon Laplace</li> <li>mpi-esm1-2-hr: Max Planck Institute for Meteorology</li> <li>mri-esm2-0: Meteorological Research Institute</li> <li>ukesm1-0-ll: UK Earth System Modeling</li> </ul>"},{"location":"examples/datasets/cmip_w5e5_example/#variables","title":"Variables\u00b6","text":"<p>Standard CMIP6 variables are available:</p> <ul> <li>tas: Near-surface air temperature (K)</li> <li>tasmin: Daily minimum temperature (K)</li> <li>tasmax: Daily maximum temperature (K)</li> <li>pr: Precipitation (kg m\u207b\u00b2 s\u207b\u00b9)</li> <li>rsds: Surface downwelling shortwave radiation (W m\u207b\u00b2)</li> <li>hurs: Near-surface relative humidity (%)</li> <li>sfcWind: Near-surface wind speed (m s\u207b\u00b9)</li> <li>ps: Surface air pressure (Pa)</li> </ul>"},{"location":"examples/datasets/cmip_w5e5_example/#notes","title":"Notes\u00b6","text":"<ol> <li>Data Size: CMIP6 files can be large. Start with short time periods for testing.</li> <li>ISIMIP Client: Requires <code>isimip-client</code> package for data access.</li> <li>Resolution: Data is provided at 0.5\u00b0 (~55 km) spatial resolution.</li> <li>Bias Adjustment: ISIMIP3b data is bias-adjusted to W5E5 observations.</li> <li>Attribution: When using this data, cite both CMIP6 and ISIMIP appropriately.</li> </ol>"},{"location":"examples/datasets/cmip_w5e5_example/#references","title":"References\u00b6","text":"<ul> <li>ISIMIP: https://www.isimip.org/</li> <li>CMIP6: https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6</li> <li>W5E5: https://doi.org/10.48364/ISIMIP.342217</li> </ul>"},{"location":"examples/datasets/w5e5_example/","title":"W5E5 0.5 deg reference dataset","text":"In\u00a0[4]: Copied! <pre>from climdata.utils.config import load_config\nfrom climdata.datasets.W5E5 import W5E5\nfrom climdata import ClimData\n# Configure\noverrides = [\n    \"dataset=w5e5\",  # Select the MSWX dataset for extraction\n    \"lat=52\",\n    \"lon=13\",\n    f\"time_range.start_date=2004-01-01\",  # Start date of extraction\n    f\"time_range.end_date=2004-12-31\",    # End date of extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Fetch, load, and extract\nw5e5 = W5E5(extractor.cfg)\nw5e5.fetch()  # Download from ISIMIP\nw5e5.load()   # Load into xarray\nw5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))\n</pre> from climdata.utils.config import load_config from climdata.datasets.W5E5 import W5E5 from climdata import ClimData # Configure overrides = [     \"dataset=w5e5\",  # Select the MSWX dataset for extraction     \"lat=52\",     \"lon=13\",     f\"time_range.start_date=2004-01-01\",  # Start date of extraction     f\"time_range.end_date=2004-12-31\",    # End date of extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ]  extractor = ClimData(overrides=overrides)  # Fetch, load, and extract w5e5 = W5E5(extractor.cfg) w5e5.fetch()  # Download from ISIMIP w5e5.load()   # Load into xarray w5e5.extract(point=(extractor.cfg.lon, extractor.cfg.lat))  <pre>\ud83d\udd0d Searching for W5E5 datasets in ISIMIP repository...\n\n\ud83d\udce5 Fetching tasmin...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_tasmin_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_tasmin_global_daily_2001_2010.nc\n\n\ud83d\udce5 Fetching tasmax...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_tasmax_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_tasmax_global_daily_2001_2010.nc\n\n\ud83d\udce5 Fetching pr...\n\u2705 Found dataset: 20crv3-w5e5_obsclim_pr_global_daily\n  \u2713 Already exists: 20crv3-w5e5_obsclim_pr_global_daily_2001_2010.nc\n\n\u2705 Downloaded 3 files\n\ud83d\udcc2 Loading 3 W5E5 files...\n  Loading tasmin from 1 file(s)...\n  Loading tasmax from 1 file(s)...\n  Loading pr from 1 file(s)...\n\u2705 Loaded dataset with 3 variables\n</pre> In\u00a0[5]: Copied! <pre>w5e5.ds\n</pre> w5e5.ds Out[5]: <pre>&lt;xarray.Dataset&gt; Size: 7kB\nDimensions:  (time: 366)\nCoordinates:\n    lon      float64 8B 13.25\n    lat      float64 8B 52.25\n  * time     (time) datetime64[ns] 3kB 2004-01-01 2004-01-02 ... 2004-12-31\nData variables:\n    tasmin   (time) float32 1kB ...\n    tasmax   (time) float32 1kB ...\n    pr       (time) float32 1kB ...\nAttributes:\n    title:        20CRv3-W5E5 observational climate input data for ISIMIP3a\n    institution:  Potsdam Institute for Climate Impact Research (PIK)\n    project:      Inter-Sectoral Impact Model Intercomparison Project phase 3...\n    contact:      ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https...\n    summary:      20CRv3, member 001, bias-adjusted to W5E5 v2.0 with ISIMIP3...\n    references:   Slivinski et al. (2019) &lt;https://doi.org/10.1002/qj.3598&gt; a...\n    source:       W5E5 via ISIMIP\n    dataset:      W5E5v2.0\n    description:  WFDE5 over land merged with ERA5 over ocean</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 366</li></ul></li><li>Coordinates: (3)<ul><li>lon()float6413.25standard_name :longitudelong_name :Longitudeunits :degrees_eastaxis :X<pre>array(13.25)</pre></li><li>lat()float6452.25standard_name :latitudelong_name :Latitudeunits :degrees_northaxis :Y<pre>array(52.25)</pre></li><li>time(time)datetime64[ns]2004-01-01 ... 2004-12-31standard_name :timelong_name :Timeaxis :T<pre>array(['2004-01-01T00:00:00.000000000', '2004-01-02T00:00:00.000000000',\n       '2004-01-03T00:00:00.000000000', ..., '2004-12-29T00:00:00.000000000',\n       '2004-12-30T00:00:00.000000000', '2004-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li></ul></li><li>Data variables: (3)<ul><li>tasmin(time)float32...standard_name :air_temperaturelong_name :Daily Minimum Near-Surface Air Temperatureunits :K<pre>[366 values with dtype=float32]</pre></li><li>tasmax(time)float32...standard_name :air_temperaturelong_name :Daily Maximum Near-Surface Air Temperatureunits :K<pre>[366 values with dtype=float32]</pre></li><li>pr(time)float32...standard_name :precipitation_fluxlong_name :Precipitationunits :kg m-2 s-1<pre>[366 values with dtype=float32]</pre></li></ul></li><li>Indexes: (1)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['2004-01-01', '2004-01-02', '2004-01-03', '2004-01-04',\n               '2004-01-05', '2004-01-06', '2004-01-07', '2004-01-08',\n               '2004-01-09', '2004-01-10',\n               ...\n               '2004-12-22', '2004-12-23', '2004-12-24', '2004-12-25',\n               '2004-12-26', '2004-12-27', '2004-12-28', '2004-12-29',\n               '2004-12-30', '2004-12-31'],\n              dtype='datetime64[ns]', name='time', length=366, freq=None))</pre></li></ul></li><li>Attributes: (9)title :20CRv3-W5E5 observational climate input data for ISIMIP3ainstitution :Potsdam Institute for Climate Impact Research (PIK)project :Inter-Sectoral Impact Model Intercomparison Project phase 3a (ISIMIP3a)contact :ISIMIP cross-sectoral science team &lt;info@isimip.org&gt; &lt;https://www.isimip.org&gt;summary :20CRv3, member 001, bias-adjusted to W5E5 v2.0 with ISIMIP3BASD v2.5.1 (for 1901-1978) combined with W5E5 v2.0 (for 1979-2019)references :Slivinski et al. (2019) &lt;https://doi.org/10.1002/qj.3598&gt; and Compo et al. (2011) &lt;https://doi.org/10.1002/qj.776&gt; for 20CRv3; Cucchi et al. (2020) &lt;https://doi.org/10.5194/essd-2020-28&gt; and Lange et al. (2021) &lt;https://doi.org/10.48364/ISIMIP.342217&gt; for W5E5; Lange (2019) &lt;https://doi.org/10.5194/gmd-12-3055-2019&gt; and Lange (2021) &lt;https://doi.org/10.5281/zenodo.5776126&gt; for ISIMIP3BASDsource :W5E5 via ISIMIPdataset :W5E5v2.0description :WFDE5 over land merged with ERA5 over ocean</li></ul>"},{"location":"examples/sdba/sdba/","title":"Sdba","text":"In\u00a0[1]: Copied! <pre>import xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\n# Import climdata BCSD modules\nfrom climdata.sdba import BCSD, BiasCorrection, StatisticalDownscaling, regrid_to_coarse\n\nprint(\"\u2713 All imports successful!\")\n</pre> import xarray as xr import numpy as np import matplotlib.pyplot as plt from datetime import datetime  # Import climdata BCSD modules from climdata.sdba import BCSD, BiasCorrection, StatisticalDownscaling, regrid_to_coarse  print(\"\u2713 All imports successful!\") <pre>\u2713 All imports successful!\n</pre> In\u00a0[2]: Copied! <pre># Time periods\nhist_start = '2004-01-01'\nhist_end = '2014-12-31'\nfut_start = '2015-01-01'\nfut_end = '2050-12-31'\n\nprint(f\"Historical period: {hist_start} to {hist_end}\")\nprint(f\"Future period: {fut_start} to {fut_end}\")\n</pre> # Time periods hist_start = '2004-01-01' hist_end = '2014-12-31' fut_start = '2015-01-01' fut_end = '2050-12-31'  print(f\"Historical period: {hist_start} to {hist_end}\") print(f\"Future period: {fut_start} to {fut_end}\") <pre>Historical period: 2004-01-01 to 2014-12-31\nFuture period: 2015-01-01 to 2050-12-31\n</pre> In\u00a0[3]: Copied! <pre>from climdata import ClimData\n\noptions = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tas]\",\n    f\"time_range.start_date={hist_start}\",  # Start date of extraction\n    f\"time_range.end_date={hist_end}\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\nmswx = ClimData(overrides=options)\n\nobs_fine_real = mswx.extract()\n\nprint(f\"\\n\u2713 MSWX data loaded!\")\nprint(f\"  Dimensions: {obs_fine_real.dims}\")\nprint(f\"  Variables: {list(obs_fine_real.data_vars)}\")\nprint(f\"  Resolution: ~0.1\u00b0 (~10 km)\")\nprint(f\"  Time range: {obs_fine_real.time.values[0]} to {obs_fine_real.time.values[-1]}\")\n</pre> from climdata import ClimData  options = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tas]\",     f\"time_range.start_date={hist_start}\",  # Start date of extraction     f\"time_range.end_date={hist_end}\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ] mswx = ClimData(overrides=options)  obs_fine_real = mswx.extract()  print(f\"\\n\u2713 MSWX data loaded!\") print(f\"  Dimensions: {obs_fine_real.dims}\") print(f\"  Variables: {list(obs_fine_real.data_vars)}\") print(f\"  Resolution: ~0.1\u00b0 (~10 km)\") print(f\"  Time range: {obs_fine_real.time.values[0]} to {obs_fine_real.time.values[-1]}\") <pre>\u2705 All 4018 tas files already exist locally.\n\n\u2713 MSWX data loaded!\n  Dimensions: FrozenMappingWarningOnValuesAccess({'time': 4018, 'lat': 370, 'lon': 450})\n  Variables: ['tas']\n  Resolution: ~0.1\u00b0 (~10 km)\n  Time range: 2004-01-01T00:00:00.000000000 to 2014-12-31T00:00:00.000000000\n</pre> In\u00a0[4]: Copied! <pre># Choose a CMIP6 model (using MPI-ESM1-2-HR as example)\ncmip_model = 'MPI-ESM1-2-HR'\n\noptions = [\n    \"dataset=cmip\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tas]\",\n    f\"time_range.start_date={hist_start}\",  # Start date of extraction\n    f\"time_range.end_date={hist_end}\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    f\"source_id={cmip_model}\",\n    f\"experiment_id=historical\",\n]\ncmip_hist = ClimData(overrides=options)\n\nsim_hist_coarse_real = cmip_hist.extract()\n\nprint(f\"\\n\u2713 Historical CMIP6 data loaded!\")\nprint(f\"  Model: {cmip_model}\")\nprint(f\"  Dimensions: {sim_hist_coarse_real.dims}\")\nprint(f\"  Resolution: ~1-2\u00b0 (model-dependent)\")\nprint(f\"  Time range: {sim_hist_coarse_real.time.values[0]} to {sim_hist_coarse_real.time.values[-1]}\")\n</pre> # Choose a CMIP6 model (using MPI-ESM1-2-HR as example) cmip_model = 'MPI-ESM1-2-HR'  options = [     \"dataset=cmip\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tas]\",     f\"time_range.start_date={hist_start}\",  # Start date of extraction     f\"time_range.end_date={hist_end}\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     f\"source_id={cmip_model}\",     f\"experiment_id=historical\", ] cmip_hist = ClimData(overrides=options)  sim_hist_coarse_real = cmip_hist.extract()  print(f\"\\n\u2713 Historical CMIP6 data loaded!\") print(f\"  Model: {cmip_model}\") print(f\"  Dimensions: {sim_hist_coarse_real.dims}\") print(f\"  Resolution: ~1-2\u00b0 (model-dependent)\") print(f\"  Time range: {sim_hist_coarse_real.time.values[0]} to {sim_hist_coarse_real.time.values[-1]}\") <pre>\n\u2713 Historical CMIP6 data loaded!\n  Model: MPI-ESM1-2-HR\n  Dimensions: FrozenMappingWarningOnValuesAccess({'lat': 40, 'bnds': 2, 'lon': 49, 'source_id': 1, 'time': 4018})\n  Resolution: ~1-2\u00b0 (model-dependent)\n  Time range: 2004-01-01T00:00:00.000000000 to 2014-12-31T00:00:00.000000000\n</pre> In\u00a0[5]: Copied! <pre># Choose a CMIP6 model (using MPI-ESM1-2-HR as example)\ncmip_model = 'MPI-ESM1-2-HR'\n\noptions = [\n    \"dataset=cmip\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tas]\",\n    f\"time_range.start_date={fut_start}\",  # Start date of extraction\n    f\"time_range.end_date={fut_end}\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    f\"source_id={cmip_model}\",\n    f\"experiment_id=ssp585\",\n]\ncmip_hist = ClimData(overrides=options)\n\nsim_fut_coarse_real = cmip_hist.extract()\n\nprint(f\"\\n\u2713 Future CMIP6 data loaded!\")\nprint(f\"  Scenario: SSP5-8.5\")\nprint(f\"  Dimensions: {sim_fut_coarse_real.dims}\")\nprint(f\"  Time range: {sim_fut_coarse_real.time.values[0]} to {sim_fut_coarse_real.time.values[-1]}\")\n</pre> # Choose a CMIP6 model (using MPI-ESM1-2-HR as example) cmip_model = 'MPI-ESM1-2-HR'  options = [     \"dataset=cmip\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tas]\",     f\"time_range.start_date={fut_start}\",  # Start date of extraction     f\"time_range.end_date={fut_end}\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     f\"source_id={cmip_model}\",     f\"experiment_id=ssp585\", ] cmip_hist = ClimData(overrides=options)  sim_fut_coarse_real = cmip_hist.extract()  print(f\"\\n\u2713 Future CMIP6 data loaded!\") print(f\"  Scenario: SSP5-8.5\") print(f\"  Dimensions: {sim_fut_coarse_real.dims}\") print(f\"  Time range: {sim_fut_coarse_real.time.values[0]} to {sim_fut_coarse_real.time.values[-1]}\")  <pre>\n\u2713 Future CMIP6 data loaded!\n  Scenario: SSP5-8.5\n  Dimensions: FrozenMappingWarningOnValuesAccess({'lat': 40, 'bnds': 2, 'lon': 49, 'source_id': 1, 'time': 13149})\n  Time range: 2015-01-01T00:00:00.000000000 to 2050-12-31T00:00:00.000000000\n</pre> In\u00a0[6]: Copied! <pre># Check data summary\nprint(\"Data Summary:\")\nprint(f\"\\nObservations (MSWX):\")\nprint(f\"  Variable: tas (temperature)\")\nprint(f\"  Shape: {obs_fine_real['tas'].shape}\")\nprint(f\"  Resolution: ~{(obs_fine_real.lat.values[1] - obs_fine_real.lat.values[0]):.2f}\u00b0\")\n\nprint(f\"\\nHistorical GCM (CMIP6):\")\nprint(f\"  Variable: tas\")\nprint(f\"  Shape: {sim_hist_coarse_real['tas'].shape}\")\nprint(f\"  Resolution: ~{(sim_hist_coarse_real.lat.values[1] - sim_hist_coarse_real.lat.values[0]):.2f}\u00b0\")\n\nprint(f\"\\nFuture GCM (CMIP6):\")\nprint(f\"  Variable: tas\")\nprint(f\"  Shape: {sim_fut_coarse_real['tas'].shape}\")\n\n# Check if grids are compatible for downscaling\nlat_ratio = len(obs_fine_real.lat) / len(sim_hist_coarse_real.lat)\nlon_ratio = len(obs_fine_real.lon) / len(sim_hist_coarse_real.lon)\n\nprint(f\"\\n\ud83d\udcca Downscaling factors:\")\nprint(f\"  Latitude: {lat_ratio:.2f}x\")\nprint(f\"  Longitude: {lon_ratio:.2f}x\")\n\nif lat_ratio == int(lat_ratio) and lon_ratio == int(lon_ratio):\n    print(\"  \u2713 Grids are compatible for ISIMIP3BASD downscaling!\")\nelse:\n    print(\"  \u26a0 Warning: Grids are not exact integer multiples.\")\n    print(\"    ISIMIP3BASD downscaling requires integer downscaling factors.\")\n    print(\"    Consider regridding to compatible resolutions first.\")\n</pre>  # Check data summary print(\"Data Summary:\") print(f\"\\nObservations (MSWX):\") print(f\"  Variable: tas (temperature)\") print(f\"  Shape: {obs_fine_real['tas'].shape}\") print(f\"  Resolution: ~{(obs_fine_real.lat.values[1] - obs_fine_real.lat.values[0]):.2f}\u00b0\")  print(f\"\\nHistorical GCM (CMIP6):\") print(f\"  Variable: tas\") print(f\"  Shape: {sim_hist_coarse_real['tas'].shape}\") print(f\"  Resolution: ~{(sim_hist_coarse_real.lat.values[1] - sim_hist_coarse_real.lat.values[0]):.2f}\u00b0\")  print(f\"\\nFuture GCM (CMIP6):\") print(f\"  Variable: tas\") print(f\"  Shape: {sim_fut_coarse_real['tas'].shape}\")  # Check if grids are compatible for downscaling lat_ratio = len(obs_fine_real.lat) / len(sim_hist_coarse_real.lat) lon_ratio = len(obs_fine_real.lon) / len(sim_hist_coarse_real.lon)  print(f\"\\n\ud83d\udcca Downscaling factors:\") print(f\"  Latitude: {lat_ratio:.2f}x\") print(f\"  Longitude: {lon_ratio:.2f}x\")  if lat_ratio == int(lat_ratio) and lon_ratio == int(lon_ratio):     print(\"  \u2713 Grids are compatible for ISIMIP3BASD downscaling!\") else:     print(\"  \u26a0 Warning: Grids are not exact integer multiples.\")     print(\"    ISIMIP3BASD downscaling requires integer downscaling factors.\")     print(\"    Consider regridding to compatible resolutions first.\") <pre>Data Summary:\n\nObservations (MSWX):\n  Variable: tas (temperature)\n  Shape: (4018, 370, 450)\n  Resolution: ~0.10\u00b0\n\nHistorical GCM (CMIP6):\n  Variable: tas\n  Shape: (1, 4018, 40, 49)\n  Resolution: ~0.94\u00b0\n\nFuture GCM (CMIP6):\n  Variable: tas\n  Shape: (1, 13149, 40, 49)\n\n\ud83d\udcca Downscaling factors:\n  Latitude: 9.25x\n  Longitude: 9.18x\n  \u26a0 Warning: Grids are not exact integer multiples.\n    ISIMIP3BASD downscaling requires integer downscaling factors.\n    Consider regridding to compatible resolutions first.\n</pre> In\u00a0[7]: Copied! <pre># Initialize BCSD for temperature\nbcsd = BCSD(\n    variable='tas',\n    regridding_tool='xesmf',  # or 'cdo' if xESMF not available\n    regridding_method='conservative',  # area-weighted conservative regridding\n    bias_correction_kwargs={\n        'n_processes': 4  # Use parallel processing\n    },\n    downscaling_kwargs={\n        'n_iterations': 20,  # MBCn iterations\n        'n_processes': 4\n    }\n)\n\nprint(\"BCSD pipeline configured!\")\n</pre> # Initialize BCSD for temperature bcsd = BCSD(     variable='tas',     regridding_tool='xesmf',  # or 'cdo' if xESMF not available     regridding_method='conservative',  # area-weighted conservative regridding     bias_correction_kwargs={         'n_processes': 4  # Use parallel processing     },     downscaling_kwargs={         'n_iterations': 20,  # MBCn iterations         'n_processes': 4     } )  print(\"BCSD pipeline configured!\") <pre>\ud83d\udd27 BiasCorrection initialized for tas\n   Distribution: normal\n   Trend preservation: additive\n   Detrend: True\n\ud83d\udd27 StatisticalDownscaling initialized for tas\n   Iterations: 20\n\n============================================================\nBCSD Pipeline initialized for tas\nRegridding: xesmf (conservative)\n============================================================\nBCSD pipeline configured!\n</pre> In\u00a0[8]: Copied! <pre># Reload the module to pick up code changes\nimport importlib\nimport climdata.sdba.bcsd\nimportlib.reload(climdata.sdba.bcsd)\nfrom climdata.sdba import BCSD, BiasCorrection, StatisticalDownscaling, regrid_to_coarse\n\n# Reinitialize BCSD\nbcsd = BCSD(\n    variable='tas',\n    regridding_tool='xesmf',\n    regridding_method='conservative',\n    bias_correction_kwargs={'n_processes': 4},\n    downscaling_kwargs={'n_iterations': 20, 'n_processes': 4}\n)\n\nprint(\"\u2713 Module reloaded and BCSD reconfigured!\")\n</pre> # Reload the module to pick up code changes import importlib import climdata.sdba.bcsd importlib.reload(climdata.sdba.bcsd) from climdata.sdba import BCSD, BiasCorrection, StatisticalDownscaling, regrid_to_coarse  # Reinitialize BCSD bcsd = BCSD(     variable='tas',     regridding_tool='xesmf',     regridding_method='conservative',     bias_correction_kwargs={'n_processes': 4},     downscaling_kwargs={'n_iterations': 20, 'n_processes': 4} )  print(\"\u2713 Module reloaded and BCSD reconfigured!\") <pre>\ud83d\udd27 BiasCorrection initialized for tas\n   Distribution: normal\n   Trend preservation: additive\n   Detrend: True\n\ud83d\udd27 StatisticalDownscaling initialized for tas\n   Iterations: 20\n\n============================================================\nBCSD Pipeline initialized for tas\nRegridding: xesmf (conservative)\n============================================================\n\u2713 Module reloaded and BCSD reconfigured!\n</pre> In\u00a0[\u00a0]: Copied! <pre># Test BCSD workflow step by step\n\n# Step 1: Regrid fine observations to coarse GCM grid\nprint(\"Step 1: Regridding observations to coarse resolution...\")\nobs_hist_coarse = regrid_to_coarse(\n    obs_fine_real,\n    sim_hist_coarse_real,\n    method='conservative',\n    regridding_tool='xesmf'\n)\nprint(f\"  \u2713 Coarse observations shape: {obs_hist_coarse['tas'].shape}\")\n\n# Ensure all datasets have exactly the same spatial grid\nprint(\"\\nStep 1b: Aligning spatial grids...\")\n\n# Remove extra dimensions from CMIP data (source_id dimension)\nif 'source_id' in sim_hist_coarse_real.dims:\n    sim_hist_coarse_real = sim_hist_coarse_real.squeeze('source_id', drop=True)\n    print(\"  \u2713 Removed source_id dimension from historical data\")\nif 'source_id' in sim_fut_coarse_real.dims:\n    sim_fut_coarse_real = sim_fut_coarse_real.squeeze('source_id', drop=True)\n    print(\"  \u2713 Removed source_id dimension from future data\")\n\n# Use sim_hist as the reference grid - ensure obs and sim_fut match it exactly\nobs_hist_coarse = obs_hist_coarse.sel(\n    lat=sim_hist_coarse_real.lat,\n    lon=sim_hist_coarse_real.lon,\n    method='nearest'\n)\n# Explicitly assign coordinates to ensure perfect match\nobs_hist_coarse = obs_hist_coarse.assign_coords({\n    'lat': sim_hist_coarse_real.lat,\n    'lon': sim_hist_coarse_real.lon\n})\n\nsim_fut_coarse_aligned = sim_fut_coarse_real.sel(\n    lat=sim_hist_coarse_real.lat,\n    lon=sim_hist_coarse_real.lon,\n    method='nearest'\n)\n# Explicitly assign coordinates\nsim_fut_coarse_aligned = sim_fut_coarse_aligned.assign_coords({\n    'lat': sim_hist_coarse_real.lat,\n    'lon': sim_hist_coarse_real.lon\n})\n\nprint(f\"  \u2713 Aligned shapes - obs: {obs_hist_coarse['tas'].shape}, hist: {sim_hist_coarse_real['tas'].shape}, fut: {sim_fut_coarse_aligned['tas'].shape}\")\nprint(f\"  \u2713 Coordinate match - lat: {(obs_hist_coarse.lat == sim_hist_coarse_real.lat).all().values}, lon: {(obs_hist_coarse.lon == sim_hist_coarse_real.lon).all().values}\")\n\n# Step 2: Bias correction at coarse resolution\nprint(\"\\nStep 2: Bias correction...\")\nbc = BiasCorrection(\n    variable='tas'\n)\nsim_fut_ba = bc.correct(\n    obs_hist=obs_hist_coarse,\n    sim_hist=sim_hist_coarse_real,\n    sim_fut=sim_fut_coarse_aligned\n)\nprint(f\"  \u2713 Bias-corrected shape: {sim_fut_ba['tas'].shape}\")\n</pre> # Test BCSD workflow step by step  # Step 1: Regrid fine observations to coarse GCM grid print(\"Step 1: Regridding observations to coarse resolution...\") obs_hist_coarse = regrid_to_coarse(     obs_fine_real,     sim_hist_coarse_real,     method='conservative',     regridding_tool='xesmf' ) print(f\"  \u2713 Coarse observations shape: {obs_hist_coarse['tas'].shape}\")  # Ensure all datasets have exactly the same spatial grid print(\"\\nStep 1b: Aligning spatial grids...\")  # Remove extra dimensions from CMIP data (source_id dimension) if 'source_id' in sim_hist_coarse_real.dims:     sim_hist_coarse_real = sim_hist_coarse_real.squeeze('source_id', drop=True)     print(\"  \u2713 Removed source_id dimension from historical data\") if 'source_id' in sim_fut_coarse_real.dims:     sim_fut_coarse_real = sim_fut_coarse_real.squeeze('source_id', drop=True)     print(\"  \u2713 Removed source_id dimension from future data\")  # Use sim_hist as the reference grid - ensure obs and sim_fut match it exactly obs_hist_coarse = obs_hist_coarse.sel(     lat=sim_hist_coarse_real.lat,     lon=sim_hist_coarse_real.lon,     method='nearest' ) # Explicitly assign coordinates to ensure perfect match obs_hist_coarse = obs_hist_coarse.assign_coords({     'lat': sim_hist_coarse_real.lat,     'lon': sim_hist_coarse_real.lon })  sim_fut_coarse_aligned = sim_fut_coarse_real.sel(     lat=sim_hist_coarse_real.lat,     lon=sim_hist_coarse_real.lon,     method='nearest' ) # Explicitly assign coordinates sim_fut_coarse_aligned = sim_fut_coarse_aligned.assign_coords({     'lat': sim_hist_coarse_real.lat,     'lon': sim_hist_coarse_real.lon })  print(f\"  \u2713 Aligned shapes - obs: {obs_hist_coarse['tas'].shape}, hist: {sim_hist_coarse_real['tas'].shape}, fut: {sim_fut_coarse_aligned['tas'].shape}\") print(f\"  \u2713 Coordinate match - lat: {(obs_hist_coarse.lat == sim_hist_coarse_real.lat).all().values}, lon: {(obs_hist_coarse.lon == sim_hist_coarse_real.lon).all().values}\")  # Step 2: Bias correction at coarse resolution print(\"\\nStep 2: Bias correction...\") bc = BiasCorrection(     variable='tas' ) sim_fut_ba = bc.correct(     obs_hist=obs_hist_coarse,     sim_hist=sim_hist_coarse_real,     sim_fut=sim_fut_coarse_aligned ) print(f\"  \u2713 Bias-corrected shape: {sim_fut_ba['tas'].shape}\")  <pre>Step 1: Regridding observations to coarse resolution...\n\ud83d\udd04 Regridding from fine to coarse resolution using xesmf...\n   Fine grid: FrozenMappingWarningOnValuesAccess({'time': 4018, 'lat': 370, 'lon': 450})\n   Target coarse grid: FrozenMappingWarningOnValuesAccess({'lat': 40, 'bnds': 2, 'lon': 49, 'source_id': 1, 'time': 4018})\n   Creating conservative regridder...\n   Regridding variable: tas\n   \u2705 Regridding complete!\n  \u2713 Coarse observations shape: (4018, 40, 49)\n\nStep 1b: Aligning spatial grids...\n  \u2713 Removed source_id dimension from historical data\n  \u2713 Removed source_id dimension from future data\n  \u2713 Aligned shapes - obs: (4018, 40, 49), hist: (4018, 40, 49), fut: (13149, 40, 49)\n  \u2713 Coordinate match - lat: True, lon: True\n\nStep 2: Bias correction...\n\ud83d\udd27 BiasCorrection initialized for tas\n   Distribution: normal\n   Trend preservation: additive\n   Detrend: True\n\n\ud83d\udd04 Starting bias correction for tas...\n   Obs hist period: 2004-01-01T00:00:00.000000000 to 2014-12-31T00:00:00.000000000\n   Sim hist period: 2004-01-01T00:00:00.000000000 to 2014-12-31T00:00:00.000000000\n   Sim fut period: 2015-01-01T00:00:00.000000000 to 2050-12-31T00:00:00.000000000\n   Converting xarray datasets to iris cubes...\n   Running ISIMIP3BASD bias adjustment...\n   (This may take a while for large datasets)\nadjusting at location ...\n(0, 0)\n(0, 1)\n(0, 2)\n(0, 3)\n(0, 4)\n(0, 5)\n(0, 6)\n(0, 7)\n(0, 8)\n(0, 9)\n(0, 10)\n(0, 11)\n(0, 12)\n(0, 13)\n(0, 14)\n(0, 15)\n(0, 16)\n(0, 17)\n(0, 18)\n(0, 19)\n(0, 20)\n(0, 21)\n(0, 22)\n(0, 23)\n(0, 24)\n(0, 25)\n(0, 26)\n(0, 27)\n(0, 28)\n(0, 29)\n(0, 30)\n(0, 31)\n(0, 32)\n(0, 33)\n(0, 34)\n(0, 35)\n(0, 36)\n(0, 37)\n(0, 38)\n(0, 39)\n(0, 40)\n(0, 41)\n(0, 42)\n(0, 43)\n(0, 44)\n(0, 45)\n(0, 46)\n(0, 47)\n(0, 48)\n(1, 0)\n(1, 1)\n(1, 2)\n(1, 3)\n(1, 4)\n(1, 5)\n(1, 6)\n(1, 7)\n(1, 8)\n(1, 9)\n(1, 10)\n(1, 11)\n(1, 12)\n(1, 13)\n(1, 14)\n(1, 15)\n(1, 16)\n(1, 17)\n(1, 18)\n(1, 19)\n(1, 20)\n(1, 21)\n(1, 22)\n(1, 23)\n(1, 24)\n(1, 25)\n(1, 26)\n(1, 27)\n(1, 28)\n(1, 29)\n(1, 30)\n(1, 31)\n(1, 32)\n(1, 33)\n(1, 34)\n(1, 35)\n(1, 36)\n(1, 37)\n(1, 38)\n(1, 39)\n(1, 40)\n(1, 41)\n(1, 42)\n(1, 43)\n(1, 44)\n(1, 45)\n(1, 46)\n(1, 47)\n(1, 48)\n(2, 0)\n(2, 1)\n(2, 2)\n(2, 3)\n(2, 4)\n(2, 5)\n(2, 6)\n(2, 7)\n(2, 8)\n(2, 9)\n(2, 10)\n(2, 11)\n(2, 12)\n(2, 13)\n(2, 14)\n(2, 15)\n(2, 16)\n(2, 17)\n(2, 18)\n(2, 19)\n(2, 20)\n(2, 21)\n(2, 22)\n(2, 23)\n(2, 24)\n(2, 25)\n(2, 26)\n(2, 27)\n(2, 28)\n(2, 29)\n(2, 30)\n(2, 31)\n(2, 32)\n(2, 33)\n(2, 34)\n(2, 35)\n(2, 36)\n(2, 37)\n(2, 38)\n(2, 39)\n(2, 40)\n(2, 41)\n(2, 42)\n(2, 43)\n(2, 44)\n(2, 45)\n(2, 46)\n(2, 47)\n(2, 48)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n(3, 4)\n(3, 5)\n(3, 6)\n(3, 7)\n(3, 8)\n(3, 9)\n(3, 10)\n(3, 11)\n(3, 12)\n(3, 13)\n(3, 14)\n(3, 15)\n(3, 16)\n(3, 17)\n(3, 18)\n(3, 19)\n(3, 20)\n(3, 21)\n(3, 22)\n(3, 23)\n(3, 24)\n(3, 25)\n(3, 26)\n(3, 27)\n(3, 28)\n(3, 29)\n(3, 30)\n(3, 31)\n(3, 32)\n(3, 33)\n(3, 34)\n(3, 35)\n(3, 36)\n(3, 37)\n(3, 38)\n(3, 39)\n(3, 40)\n(3, 41)\n(3, 42)\n(3, 43)\n(3, 44)\n(3, 45)\n(3, 46)\n(3, 47)\n(3, 48)\n(4, 0)\n(4, 1)\n(4, 2)\n(4, 3)\n(4, 4)\n(4, 5)\n(4, 6)\n(4, 7)\n(4, 8)\n(4, 9)\n(4, 10)\n(4, 11)\n(4, 12)\n(4, 13)\n(4, 14)\n(4, 15)\n(4, 16)\n(4, 17)\n(4, 18)\n(4, 19)\n(4, 20)\n(4, 21)\n(4, 22)\n(4, 23)\n(4, 24)\n(4, 25)\n(4, 26)\n(4, 27)\n(4, 28)\n(4, 29)\n(4, 30)\n(4, 31)\n(4, 32)\n(4, 33)\n(4, 34)\n(4, 35)\n(4, 36)\n(4, 37)\n(4, 38)\n(4, 39)\n(4, 40)\n(4, 41)\n(4, 42)\n(4, 43)\n(4, 44)\n(4, 45)\n(4, 46)\n(4, 47)\n(4, 48)\n(5, 0)\n(5, 1)\n(5, 2)\n(5, 3)\n(5, 4)\n(5, 5)\n(5, 6)\n(5, 7)\n(5, 8)\n(5, 9)\n(5, 10)\n(5, 11)\n(5, 12)\n(5, 13)\n(5, 14)\n(5, 15)\n(5, 16)\n(5, 17)\n(5, 18)\n(5, 19)\n(5, 20)\n(5, 21)\n(5, 22)\n(5, 23)\n(5, 24)\n(5, 25)\n(5, 26)\n(5, 27)\n(5, 28)\n(5, 29)\n(5, 30)\n(5, 31)\n(5, 32)\n(5, 33)\n(5, 34)\n(5, 35)\n(5, 36)\n(5, 37)\n(5, 38)\n(5, 39)\n(5, 40)\n(5, 41)\n(5, 42)\n(5, 43)\n(5, 44)\n(5, 45)\n(5, 46)\n(5, 47)\n(5, 48)\n(6, 0)\n(6, 1)\n(6, 2)\n(6, 3)\n(6, 4)\n(6, 5)\n(6, 6)\n(6, 7)\n(6, 8)\n(6, 9)\n(6, 10)\n(6, 11)\n(6, 12)\n(6, 13)\n(6, 14)\n(6, 15)\n(6, 16)\n(6, 17)\n(6, 18)\n(6, 19)\n(6, 20)\n(6, 21)\n(6, 22)\n(6, 23)\n(6, 24)\n(6, 25)\n(6, 26)\n(6, 27)\n(6, 28)\n(6, 29)\n(6, 30)\n(6, 31)\n(6, 32)\n(6, 33)\n(6, 34)\n(6, 35)\n(6, 36)\n(6, 37)\n(6, 38)\n(6, 39)\n(6, 40)\n(6, 41)\n(6, 42)\n(6, 43)\n(6, 44)\n(6, 45)\n(6, 46)\n(6, 47)\n(6, 48)\n(7, 0)\n(7, 1)\n(7, 2)\n(7, 3)\n(7, 4)\n(7, 5)\n(7, 6)\n(7, 7)\n(7, 8)\n(7, 9)\n(7, 10)\n(7, 11)\n(7, 12)\n(7, 13)\n(7, 14)\n(7, 15)\n(7, 16)\n(7, 17)\n(7, 18)\n(7, 19)\n(7, 20)\n(7, 21)\n(7, 22)\n(7, 23)\n(7, 24)\n(7, 25)\n(7, 26)\n(7, 27)\n(7, 28)\n(7, 29)\n(7, 30)\n(7, 31)\n(7, 32)\n(7, 33)\n(7, 34)\n(7, 35)\n(7, 36)\n(7, 37)\n(7, 38)\n(7, 39)\n(7, 40)\n(7, 41)\n(7, 42)\n(7, 43)\n(7, 44)\n(7, 45)\n(7, 46)\n(7, 47)\n(7, 48)\n(8, 0)\n(8, 1)\n(8, 2)\n(8, 3)\n(8, 4)\n(8, 5)\n(8, 6)\n(8, 7)\n(8, 8)\n(8, 9)\n(8, 10)\n(8, 11)\n(8, 12)\n(8, 13)\n(8, 14)\n(8, 15)\n(8, 16)\n(8, 17)\n(8, 18)\n(8, 19)\n(8, 20)\n(8, 21)\n(8, 22)\n(8, 23)\n(8, 24)\n(8, 25)\n(8, 26)\n(8, 27)\n(8, 28)\n(8, 29)\n(8, 30)\n(8, 31)\n(8, 32)\n(8, 33)\n(8, 34)\n(8, 35)\n(8, 36)\n(8, 37)\n(8, 38)\n(8, 39)\n(8, 40)\n(8, 41)\n(8, 42)\n(8, 43)\n(8, 44)\n(8, 45)\n(8, 46)\n(8, 47)\n(8, 48)\n(9, 0)\n(9, 1)\n(9, 2)\n(9, 3)\n(9, 4)\n(9, 5)\n(9, 6)\n(9, 7)\n(9, 8)\n(9, 9)\n(9, 10)\n(9, 11)\n(9, 12)\n(9, 13)\n(9, 14)\n(9, 15)\n(9, 16)\n(9, 17)\n(9, 18)\n(9, 19)\n(9, 20)\n(9, 21)\n(9, 22)\n(9, 23)\n(9, 24)\n(9, 25)\n(9, 26)\n(9, 27)\n(9, 28)\n(9, 29)\n(9, 30)\n(9, 31)\n(9, 32)\n(9, 33)\n(9, 34)\n(9, 35)\n(9, 36)\n(9, 37)\n(9, 38)\n(9, 39)\n(9, 40)\n(9, 41)\n(9, 42)\n(9, 43)\n(9, 44)\n(9, 45)\n(9, 46)\n(9, 47)\n(9, 48)\n(10, 0)\n(10, 1)\n(10, 2)\n(10, 3)\n(10, 4)\n(10, 5)\n(10, 6)\n(10, 7)\n(10, 8)\n(10, 9)\n(10, 10)\n(10, 11)\n(10, 12)\n(10, 13)\n(10, 14)\n(10, 15)\n(10, 16)\n(10, 17)\n(10, 18)\n(10, 19)\n(10, 20)\n(10, 21)\n(10, 22)\n(10, 23)\n(10, 24)\n(10, 25)\n(10, 26)\n(10, 27)\n(10, 28)\n(10, 29)\n(10, 30)\n(10, 31)\n(10, 32)\n(10, 33)\n(10, 34)\n(10, 35)\n(10, 36)\n(10, 37)\n(10, 38)\n(10, 39)\n(10, 40)\n(10, 41)\n(10, 42)\n(10, 43)\n(10, 44)\n(10, 45)\n(10, 46)\n(10, 47)\n(10, 48)\n(11, 0)\n(11, 1)\n(11, 2)\n(11, 3)\n(11, 4)\n(11, 5)\n(11, 6)\n(11, 7)\n(11, 8)\n(11, 9)\n(11, 10)\n(11, 11)\n(11, 12)\n(11, 13)\n(11, 14)\n(11, 15)\n(11, 16)\n(11, 17)\n(11, 18)\n(11, 19)\n(11, 20)\n(11, 21)\n(11, 22)\n(11, 23)\n(11, 24)\n(11, 25)\n(11, 26)\n(11, 27)\n(11, 28)\n(11, 29)\n(11, 30)\n(11, 31)\n(11, 32)\n(11, 33)\n(11, 34)\n(11, 35)\n(11, 36)\n(11, 37)\n(11, 38)\n(11, 39)\n(11, 40)\n(11, 41)\n(11, 42)\n(11, 43)\n(11, 44)\n(11, 45)\n(11, 46)\n(11, 47)\n(11, 48)\n(12, 0)\n(12, 1)\n(12, 2)\n(12, 3)\n(12, 4)\n(12, 5)\n(12, 6)\n(12, 7)\n(12, 8)\n(12, 9)\n(12, 10)\n(12, 11)\n(12, 12)\n(12, 13)\n(12, 14)\n(12, 15)\n(12, 16)\n(12, 17)\n(12, 18)\n(12, 19)\n(12, 20)\n(12, 21)\n(12, 22)\n(12, 23)\n(12, 24)\n(12, 25)\n(12, 26)\n(12, 27)\n(12, 28)\n(12, 29)\n(12, 30)\n(12, 31)\n(12, 32)\n(12, 33)\n(12, 34)\n(12, 35)\n(12, 36)\n(12, 37)\n(12, 38)\n(12, 39)\n(12, 40)\n(12, 41)\n(12, 42)\n(12, 43)\n(12, 44)\n(12, 45)\n(12, 46)\n(12, 47)\n(12, 48)\n(13, 0)\n(13, 1)\n(13, 2)\n(13, 3)\n(13, 4)\n(13, 5)\n(13, 6)\n(13, 7)\n(13, 8)\n(13, 9)\n(13, 10)\n(13, 11)\n(13, 12)\n(13, 13)\n(13, 14)\n(13, 15)\n(13, 16)\n(13, 17)\n(13, 18)\n(13, 19)\n(13, 20)\n(13, 21)\n(13, 22)\n(13, 23)\n(13, 24)\n(13, 25)\n(13, 26)\n(13, 27)\n(13, 28)\n(13, 29)\n(13, 30)\n(13, 31)\n(13, 32)\n(13, 33)\n(13, 34)\n(13, 35)\n(13, 36)\n(13, 37)\n(13, 38)\n(13, 39)\n(13, 40)\n(13, 41)\n(13, 42)\n(13, 43)\n(13, 44)\n(13, 45)\n(13, 46)\n(13, 47)\n(13, 48)\n(14, 0)\n(14, 1)\n(14, 2)\n(14, 3)\n(14, 4)\n(14, 5)\n(14, 6)\n(14, 7)\n(14, 8)\n(14, 9)\n(14, 10)\n(14, 11)\n(14, 12)\n(14, 13)\n(14, 14)\n(14, 15)\n(14, 16)\n(14, 17)\n(14, 18)\n(14, 19)\n(14, 20)\n(14, 21)\n(14, 22)\n(14, 23)\n(14, 24)\n(14, 25)\n(14, 26)\n(14, 27)\n(14, 28)\n(14, 29)\n(14, 30)\n(14, 31)\n(14, 32)\n(14, 33)\n(14, 34)\n(14, 35)\n(14, 36)\n(14, 37)\n(14, 38)\n(14, 39)\n(14, 40)\n(14, 41)\n(14, 42)\n(14, 43)\n(14, 44)\n(14, 45)\n(14, 46)\n(14, 47)\n(14, 48)\n(15, 0)\n(15, 1)\n(15, 2)\n(15, 3)\n(15, 4)\n(15, 5)\n(15, 6)\n(15, 7)\n(15, 8)\n(15, 9)\n(15, 10)\n(15, 11)\n(15, 12)\n(15, 13)\n(15, 14)\n(15, 15)\n(15, 16)\n(15, 17)\n(15, 18)\n(15, 19)\n(15, 20)\n(15, 21)\n(15, 22)\n(15, 23)\n(15, 24)\n(15, 25)\n(15, 26)\n(15, 27)\n(15, 28)\n(15, 29)\n(15, 30)\n(15, 31)\n(15, 32)\n(15, 33)\n(15, 34)\n(15, 35)\n(15, 36)\n(15, 37)\n(15, 38)\n(15, 39)\n(15, 40)\n(15, 41)\n(15, 42)\n(15, 43)\n(15, 44)\n(15, 45)\n(15, 46)\n(15, 47)\n(15, 48)\n(16, 0)\n(16, 1)\n(16, 2)\n(16, 3)\n(16, 4)\n(16, 5)\n(16, 6)\n(16, 7)\n(16, 8)\n(16, 9)\n(16, 10)\n(16, 11)\n(16, 12)\n(16, 13)\n(16, 14)\n(16, 15)\n(16, 16)\n(16, 17)\n(16, 18)\n(16, 19)\n(16, 20)\n(16, 21)\n(16, 22)\n(16, 23)\n(16, 24)\n(16, 25)\n(16, 26)\n(16, 27)\n(16, 28)\n(16, 29)\n(16, 30)\n(16, 31)\n(16, 32)\n(16, 33)\n(16, 34)\n(16, 35)\n(16, 36)\n(16, 37)\n(16, 38)\n(16, 39)\n(16, 40)\n(16, 41)\n(16, 42)\n(16, 43)\n(16, 44)\n(16, 45)\n(16, 46)\n(16, 47)\n(16, 48)\n(17, 0)\n(17, 1)\n(17, 2)\n(17, 3)\n(17, 4)\n(17, 5)\n(17, 6)\n(17, 7)\n(17, 8)\n(17, 9)\n(17, 10)\n(17, 11)\n(17, 12)\n(17, 13)\n(17, 14)\n(17, 15)\n(17, 16)\n(17, 17)\n(17, 18)\n(17, 19)\n(17, 20)\n(17, 21)\n(17, 22)\n(17, 23)\n(17, 24)\n(17, 25)\n(17, 26)\n(17, 27)\n(17, 28)\n(17, 29)\n(17, 30)\n(17, 31)\n(17, 32)\n(17, 33)\n(17, 34)\n(17, 35)\n(17, 36)\n(17, 37)\n(17, 38)\n(17, 39)\n(17, 40)\n(17, 41)\n(17, 42)\n(17, 43)\n(17, 44)\n(17, 45)\n(17, 46)\n(17, 47)\n(17, 48)\n(18, 0)\n(18, 1)\n(18, 2)\n(18, 3)\n(18, 4)\n(18, 5)\n(18, 6)\n(18, 7)\n(18, 8)\n(18, 9)\n(18, 10)\n(18, 11)\n(18, 12)\n(18, 13)\n(18, 14)\n(18, 15)\n(18, 16)\n(18, 17)\n(18, 18)\n(18, 19)\n(18, 20)\n(18, 21)\n(18, 22)\n(18, 23)\n(18, 24)\n(18, 25)\n(18, 26)\n(18, 27)\n(18, 28)\n(18, 29)\n(18, 30)\n(18, 31)\n(18, 32)\n(18, 33)\n(18, 34)\n(18, 35)\n(18, 36)\n(18, 37)\n(18, 38)\n(18, 39)\n(18, 40)\n(18, 41)\n(18, 42)\n(18, 43)\n(18, 44)\n(18, 45)\n(18, 46)\n(18, 47)\n(18, 48)\n(19, 0)\n(19, 1)\n(19, 2)\n(19, 3)\n(19, 4)\n(19, 5)\n(19, 6)\n(19, 7)\n(19, 8)\n(19, 9)\n(19, 10)\n(19, 11)\n(19, 12)\n(19, 13)\n(19, 14)\n(19, 15)\n(19, 16)\n(19, 17)\n(19, 18)\n(19, 19)\n(19, 20)\n(19, 21)\n(19, 22)\n(19, 23)\n(19, 24)\n(19, 25)\n(19, 26)\n(19, 27)\n(19, 28)\n(19, 29)\n(19, 30)\n(19, 31)\n(19, 32)\n(19, 33)\n(19, 34)\n(19, 35)\n(19, 36)\n(19, 37)\n(19, 38)\n(19, 39)\n(19, 40)\n(19, 41)\n(19, 42)\n(19, 43)\n(19, 44)\n(19, 45)\n(19, 46)\n(19, 47)\n(19, 48)\n(20, 0)\n(20, 1)\n(20, 2)\n(20, 3)\n(20, 4)\n(20, 5)\n(20, 6)\n(20, 7)\n(20, 8)\n(20, 9)\n(20, 10)\n(20, 11)\n(20, 12)\n(20, 13)\n(20, 14)\n(20, 15)\n(20, 16)\n(20, 17)\n(20, 18)\n(20, 19)\n(20, 20)\n(20, 21)\n(20, 22)\n(20, 23)\n(20, 24)\n(20, 25)\n(20, 26)\n(20, 27)\n(20, 28)\n(20, 29)\n(20, 30)\n(20, 31)\n(20, 32)\n(20, 33)\n(20, 34)\n(20, 35)\n(20, 36)\n(20, 37)\n(20, 38)\n(20, 39)\n(20, 40)\n(20, 41)\n(20, 42)\n(20, 43)\n(20, 44)\n(20, 45)\n(20, 46)\n(20, 47)\n(20, 48)\n(21, 0)\n(21, 1)\n(21, 2)\n(21, 3)\n(21, 4)\n(21, 5)\n(21, 6)\n(21, 7)\n(21, 8)\n(21, 9)\n(21, 10)\n(21, 11)\n(21, 12)\n(21, 13)\n(21, 14)\n(21, 15)\n(21, 16)\n(21, 17)\n(21, 18)\n(21, 19)\n(21, 20)\n(21, 21)\n(21, 22)\n(21, 23)\n(21, 24)\n(21, 25)\n(21, 26)\n(21, 27)\n(21, 28)\n(21, 29)\n(21, 30)\n(21, 31)\n(21, 32)\n(21, 33)\n(21, 34)\n(21, 35)\n(21, 36)\n(21, 37)\n(21, 38)\n(21, 39)\n(21, 40)\n(21, 41)\n(21, 42)\n(21, 43)\n(21, 44)\n(21, 45)\n(21, 46)\n(21, 47)\n(21, 48)\n(22, 0)\n(22, 1)\n(22, 2)\n(22, 3)\n(22, 4)\n(22, 5)\n(22, 6)\n(22, 7)\n(22, 8)\n(22, 9)\n(22, 10)\n(22, 11)\n(22, 12)\n(22, 13)\n(22, 14)\n(22, 15)\n(22, 16)\n(22, 17)\n(22, 18)\n(22, 19)\n(22, 20)\n(22, 21)\n(22, 22)\n(22, 23)\n(22, 24)\n(22, 25)\n(22, 26)\n(22, 27)\n(22, 28)\n(22, 29)\n(22, 30)\n(22, 31)\n(22, 32)\n(22, 33)\n(22, 34)\n(22, 35)\n(22, 36)\n(22, 37)\n(22, 38)\n(22, 39)\n(22, 40)\n(22, 41)\n(22, 42)\n(22, 43)\n(22, 44)\n(22, 45)\n(22, 46)\n(22, 47)\n(22, 48)\n(23, 0)\n(23, 1)\n(23, 2)\n(23, 3)\n(23, 4)\n(23, 5)\n(23, 6)\n(23, 7)\n(23, 8)\n(23, 9)\n(23, 10)\n(23, 11)\n(23, 12)\n(23, 13)\n(23, 14)\n(23, 15)\n(23, 16)\n(23, 17)\n(23, 18)\n(23, 19)\n(23, 20)\n(23, 21)\n(23, 22)\n(23, 23)\n(23, 24)\n(23, 25)\n(23, 26)\n(23, 27)\n(23, 28)\n(23, 29)\n(23, 30)\n(23, 31)\n(23, 32)\n(23, 33)\n(23, 34)\n(23, 35)\n(23, 36)\n(23, 37)\n(23, 38)\n(23, 39)\n(23, 40)\n(23, 41)\n(23, 42)\n(23, 43)\n(23, 44)\n(23, 45)\n(23, 46)\n(23, 47)\n(23, 48)\n(24, 0)\n(24, 1)\n(24, 2)\n(24, 3)\n(24, 4)\n(24, 5)\n(24, 6)\n(24, 7)\n(24, 8)\n(24, 9)\n(24, 10)\n(24, 11)\n(24, 12)\n(24, 13)\n(24, 14)\n(24, 15)\n(24, 16)\n(24, 17)\n(24, 18)\n(24, 19)\n(24, 20)\n(24, 21)\n(24, 22)\n(24, 23)\n(24, 24)\n(24, 25)\n(24, 26)\n(24, 27)\n(24, 28)\n(24, 29)\n(24, 30)\n(24, 31)\n(24, 32)\n(24, 33)\n(24, 34)\n(24, 35)\n(24, 36)\n(24, 37)\n(24, 38)\n(24, 39)\n(24, 40)\n(24, 41)\n(24, 42)\n(24, 43)\n(24, 44)\n(24, 45)\n(24, 46)\n(24, 47)\n(24, 48)\n(25, 0)\n(25, 1)\n(25, 2)\n(25, 3)\n(25, 4)\n(25, 5)\n(25, 6)\n(25, 7)\n(25, 8)\n(25, 9)\n(25, 10)\n(25, 11)\n(25, 12)\n(25, 13)\n(25, 14)\n(25, 15)\n(25, 16)\n(25, 17)\n(25, 18)\n(25, 19)\n(25, 20)\n(25, 21)\n(25, 22)\n(25, 23)\n(25, 24)\n(25, 25)\n(25, 26)\n(25, 27)\n(25, 28)\n(25, 29)\n(25, 30)\n(25, 31)\n(25, 32)\n(25, 33)\n(25, 34)\n(25, 35)\n(25, 36)\n(25, 37)\n(25, 38)\n(25, 39)\n(25, 40)\n(25, 41)\n(25, 42)\n(25, 43)\n(25, 44)\n(25, 45)\n(25, 46)\n(25, 47)\n(25, 48)\n(26, 0)\n(26, 1)\n(26, 2)\n(26, 3)\n(26, 4)\n(26, 5)\n(26, 6)\n(26, 7)\n(26, 8)\n(26, 9)\n(26, 10)\n(26, 11)\n(26, 12)\n(26, 13)\n(26, 14)\n(26, 15)\n(26, 16)\n(26, 17)\n(26, 18)\n(26, 19)\n(26, 20)\n(26, 21)\n(26, 22)\n(26, 23)\n(26, 24)\n(26, 25)\n(26, 26)\n(26, 27)\n(26, 28)\n(26, 29)\n(26, 30)\n(26, 31)\n(26, 32)\n(26, 33)\n(26, 34)\n(26, 35)\n(26, 36)\n(26, 37)\n(26, 38)\n(26, 39)\n(26, 40)\n(26, 41)\n(26, 42)\n(26, 43)\n(26, 44)\n(26, 45)\n(26, 46)\n(26, 47)\n(26, 48)\n(27, 0)\n(27, 1)\n(27, 2)\n(27, 3)\n(27, 4)\n(27, 5)\n(27, 6)\n(27, 7)\n(27, 8)\n(27, 9)\n(27, 10)\n(27, 11)\n(27, 12)\n(27, 13)\n(27, 14)\n(27, 15)\n(27, 16)\n(27, 17)\n(27, 18)\n(27, 19)\n(27, 20)\n(27, 21)\n(27, 22)\n(27, 23)\n(27, 24)\n(27, 25)\n(27, 26)\n(27, 27)\n(27, 28)\n(27, 29)\n(27, 30)\n(27, 31)\n(27, 32)\n(27, 33)\n(27, 34)\n(27, 35)\n(27, 36)\n(27, 37)\n(27, 38)\n(27, 39)\n(27, 40)\n(27, 41)\n(27, 42)\n(27, 43)\n(27, 44)\n(27, 45)\n(27, 46)\n(27, 47)\n(27, 48)\n(28, 0)\n(28, 1)\n(28, 2)\n(28, 3)\n(28, 4)\n(28, 5)\n(28, 6)\n(28, 7)\n(28, 8)\n(28, 9)\n(28, 10)\n(28, 11)\n(28, 12)\n(28, 13)\n(28, 14)\n(28, 15)\n(28, 16)\n(28, 17)\n(28, 18)\n(28, 19)\n(28, 20)\n(28, 21)\n(28, 22)\n(28, 23)\n(28, 24)\n(28, 25)\n(28, 26)\n(28, 27)\n(28, 28)\n(28, 29)\n(28, 30)\n(28, 31)\n(28, 32)\n(28, 33)\n(28, 34)\n(28, 35)\n(28, 36)\n(28, 37)\n(28, 38)\n(28, 39)\n(28, 40)\n(28, 41)\n(28, 42)\n(28, 43)\n(28, 44)\n(28, 45)\n(28, 46)\n(28, 47)\n(28, 48)\n(29, 0)\n(29, 1)\n(29, 2)\n(29, 3)\n(29, 4)\n(29, 5)\n(29, 6)\n(29, 7)\n(29, 8)\n(29, 9)\n(29, 10)\n(29, 11)\n(29, 12)\n(29, 13)\n(29, 14)\n(29, 15)\n(29, 16)\n(29, 17)\n(29, 18)\n(29, 19)\n(29, 20)\n(29, 21)\n(29, 22)\n(29, 23)\n(29, 24)\n(29, 25)\n(29, 26)\n(29, 27)\n(29, 28)\n(29, 29)\n(29, 30)\n(29, 31)\n(29, 32)\n(29, 33)\n(29, 34)\n(29, 35)\n(29, 36)\n(29, 37)\n(29, 38)\n(29, 39)\n(29, 40)\n(29, 41)\n(29, 42)\n(29, 43)\n(29, 44)\n(29, 45)\n(29, 46)\n(29, 47)\n(29, 48)\n(30, 0)\n(30, 1)\n(30, 2)\n(30, 3)\n(30, 4)\n(30, 5)\n(30, 6)\n(30, 7)\n(30, 8)\n(30, 9)\n(30, 10)\n(30, 11)\n(30, 12)\n(30, 13)\n(30, 14)\n(30, 15)\n(30, 16)\n(30, 17)\n(30, 18)\n(30, 19)\n(30, 20)\n(30, 21)\n(30, 22)\n(30, 23)\n(30, 24)\n(30, 25)\n(30, 26)\n(30, 27)\n(30, 28)\n(30, 29)\n(30, 30)\n(30, 31)\n(30, 32)\n(30, 33)\n(30, 34)\n(30, 35)\n(30, 36)\n(30, 37)\n(30, 38)\n(30, 39)\n(30, 40)\n(30, 41)\n(30, 42)\n(30, 43)\n(30, 44)\n(30, 45)\n(30, 46)\n(30, 47)\n(30, 48)\n(31, 0)\n(31, 1)\n(31, 2)\n(31, 3)\n(31, 4)\n(31, 5)\n(31, 6)\n(31, 7)\n(31, 8)\n(31, 9)\n(31, 10)\n(31, 11)\n(31, 12)\n(31, 13)\n(31, 14)\n(31, 15)\n(31, 16)\n(31, 17)\n(31, 18)\n(31, 19)\n(31, 20)\n(31, 21)\n(31, 22)\n(31, 23)\n(31, 24)\n(31, 25)\n(31, 26)\n(31, 27)\n(31, 28)\n(31, 29)\n(31, 30)\n(31, 31)\n(31, 32)\n(31, 33)\n(31, 34)\n(31, 35)\n(31, 36)\n(31, 37)\n(31, 38)\n(31, 39)\n(31, 40)\n(31, 41)\n(31, 42)\n(31, 43)\n(31, 44)\n(31, 45)\n(31, 46)\n(31, 47)\n(31, 48)\n(32, 0)\n(32, 1)\n(32, 2)\n(32, 3)\n(32, 4)\n(32, 5)\n(32, 6)\n(32, 7)\n(32, 8)\n(32, 9)\n(32, 10)\n(32, 11)\n(32, 12)\n(32, 13)\n(32, 14)\n(32, 15)\n(32, 16)\n(32, 17)\n(32, 18)\n(32, 19)\n(32, 20)\n(32, 21)\n(32, 22)\n(32, 23)\n(32, 24)\n(32, 25)\n(32, 26)\n(32, 27)\n(32, 28)\n(32, 29)\n(32, 30)\n(32, 31)\n(32, 32)\n(32, 33)\n(32, 34)\n(32, 35)\n(32, 36)\n(32, 37)\n(32, 38)\n(32, 39)\n(32, 40)\n(32, 41)\n(32, 42)\n(32, 43)\n(32, 44)\n(32, 45)\n(32, 46)\n(32, 47)\n(32, 48)\n(33, 0)\n(33, 1)\n(33, 2)\n(33, 3)\n(33, 4)\n(33, 5)\n(33, 6)\n(33, 7)\n(33, 8)\n(33, 9)\n(33, 10)\n(33, 11)\n(33, 12)\n(33, 13)\n(33, 14)\n(33, 15)\n(33, 16)\n(33, 17)\n(33, 18)\n(33, 19)\n(33, 20)\n(33, 21)\n(33, 22)\n(33, 23)\n(33, 24)\n(33, 25)\n(33, 26)\n(33, 27)\n(33, 28)\n(33, 29)\n(33, 30)\n(33, 31)\n(33, 32)\n(33, 33)\n(33, 34)\n(33, 35)\n(33, 36)\n(33, 37)\n(33, 38)\n(33, 39)\n(33, 40)\n(33, 41)\n(33, 42)\n(33, 43)\n(33, 44)\n(33, 45)\n(33, 46)\n(33, 47)\n(33, 48)\n(34, 0)\n(34, 1)\n(34, 2)\n(34, 3)\n(34, 4)\n(34, 5)\n(34, 6)\n(34, 7)\n(34, 8)\n(34, 9)\n(34, 10)\n(34, 11)\n(34, 12)\n(34, 13)\n(34, 14)\n(34, 15)\n(34, 16)\n(34, 17)\n(34, 18)\n(34, 19)\n(34, 20)\n(34, 21)\n(34, 22)\n(34, 23)\n(34, 24)\n(34, 25)\n(34, 26)\n(34, 27)\n(34, 28)\n(34, 29)\n(34, 30)\n(34, 31)\n(34, 32)\n(34, 33)\n(34, 34)\n(34, 35)\n(34, 36)\n(34, 37)\n(34, 38)\n(34, 39)\n(34, 40)\n(34, 41)\n(34, 42)\n(34, 43)\n(34, 44)\n(34, 45)\n(34, 46)\n(34, 47)\n(34, 48)\n(35, 0)\n(35, 1)\n(35, 2)\n(35, 3)\n(35, 4)\n(35, 5)\n(35, 6)\n(35, 7)\n(35, 8)\n(35, 9)\n(35, 10)\n(35, 11)\n(35, 12)\n(35, 13)\n(35, 14)\n(35, 15)\n(35, 16)\n(35, 17)\n(35, 18)\n(35, 19)\n(35, 20)\n(35, 21)\n(35, 22)\n(35, 23)\n(35, 24)\n(35, 25)\n(35, 26)\n(35, 27)\n(35, 28)\n(35, 29)\n(35, 30)\n(35, 31)\n(35, 32)\n(35, 33)\n(35, 34)\n(35, 35)\n(35, 36)\n(35, 37)\n(35, 38)\n(35, 39)\n(35, 40)\n(35, 41)\n(35, 42)\n(35, 43)\n(35, 44)\n(35, 45)\n(35, 46)\n(35, 47)\n(35, 48)\n(36, 0)\n(36, 1)\n(36, 2)\n(36, 3)\n(36, 4)\n(36, 5)\n(36, 6)\n(36, 7)\n(36, 8)\n(36, 9)\n(36, 10)\n(36, 11)\n(36, 12)\n(36, 13)\n(36, 14)\n(36, 15)\n(36, 16)\n(36, 17)\n(36, 18)\n(36, 19)\n(36, 20)\n(36, 21)\n(36, 22)\n(36, 23)\n(36, 24)\n(36, 25)\n(36, 26)\n(36, 27)\n(36, 28)\n(36, 29)\n(36, 30)\n(36, 31)\n(36, 32)\n(36, 33)\n(36, 34)\n(36, 35)\n(36, 36)\n(36, 37)\n(36, 38)\n(36, 39)\n(36, 40)\n(36, 41)\n(36, 42)\n(36, 43)\n(36, 44)\n(36, 45)\n(36, 46)\n(36, 47)\n(36, 48)\n(37, 0)\n(37, 1)\n(37, 2)\n(37, 3)\n(37, 4)\n(37, 5)\n(37, 6)\n(37, 7)\n(37, 8)\n(37, 9)\n(37, 10)\n(37, 11)\n(37, 12)\n(37, 13)\n(37, 14)\n(37, 15)\n(37, 16)\n(37, 17)\n(37, 18)\n(37, 19)\n(37, 20)\n(37, 21)\n(37, 22)\n(37, 23)\n(37, 24)\n(37, 25)\n(37, 26)\n(37, 27)\n(37, 28)\n(37, 29)\n(37, 30)\n(37, 31)\n(37, 32)\n(37, 33)\n(37, 34)\n(37, 35)\n(37, 36)\n(37, 37)\n(37, 38)\n(37, 39)\n(37, 40)\n(37, 41)\n(37, 42)\n(37, 43)\n(37, 44)\n(37, 45)\n(37, 46)\n(37, 47)\n(37, 48)\n(38, 0)\n(38, 1)\n(38, 2)\n(38, 3)\n(38, 4)\n(38, 5)\n(38, 6)\n(38, 7)\n(38, 8)\n(38, 9)\n(38, 10)\n(38, 11)\n(38, 12)\n(38, 13)\n(38, 14)\n(38, 15)\n(38, 16)\n(38, 17)\n(38, 18)\n(38, 19)\n(38, 20)\n(38, 21)\n(38, 22)\n(38, 23)\n(38, 24)\n(38, 25)\n(38, 26)\n(38, 27)\n(38, 28)\n(38, 29)\n(38, 30)\n(38, 31)\n(38, 32)\n(38, 33)\n(38, 34)\n(38, 35)\n(38, 36)\n(38, 37)\n(38, 38)\n(38, 39)\n(38, 40)\n(38, 41)\n(38, 42)\n(38, 43)\n(38, 44)\n(38, 45)\n(38, 46)\n(38, 47)\n(38, 48)\n(39, 0)\n(39, 1)\n(39, 2)\n(39, 3)\n(39, 4)\n(39, 5)\n(39, 6)\n(39, 7)\n(39, 8)\n(39, 9)\n(39, 10)\n(39, 11)\n(39, 12)\n(39, 13)\n(39, 14)\n(39, 15)\n(39, 16)\n(39, 17)\n(39, 18)\n(39, 19)\n(39, 20)\n(39, 21)\n(39, 22)\n(39, 23)\n(39, 24)\n(39, 25)\n(39, 26)\n(39, 27)\n(39, 28)\n(39, 29)\n(39, 30)\n(39, 31)\n(39, 32)\n(39, 33)\n(39, 34)\n(39, 35)\n(39, 36)\n(39, 37)\n(39, 38)\n(39, 39)\n(39, 40)\n(39, 41)\n(39, 42)\n(39, 43)\n(39, 44)\n(39, 45)\n(39, 46)\n(39, 47)\n(39, 48)\n   \u2705 Bias correction complete!\n   \ud83d\udce6 Collecting results from npy_stack...\n</pre> In\u00a0[\u00a0]: Copied! <pre># Step 3: Statistical downscaling\nprint(\"\\nStep 3: Statistical downscaling...\")\n\n# Check if grids are compatible for downscaling\nlat_factor = len(obs_fine_real.lat) / len(sim_fut_ba.lat)\nlon_factor = len(obs_fine_real.lon) / len(sim_fut_ba.lon)\n\nprint(f\"  Current downscaling factors: lat={lat_factor:.2f}x, lon={lon_factor:.2f}x\")\n\nif lat_factor != int(lat_factor) or lon_factor != int(lon_factor):\n    print(\"  \u26a0 Grids not compatible - creating compatible fine grid...\")\n    \n    # Create a compatible fine grid (integer multiples)\n    target_lat_factor = round(lat_factor)\n    target_lon_factor = round(lon_factor)\n    \n    # Create target coordinates\n    coarse_lat = sim_fut_ba.lat.values\n    coarse_lon = sim_fut_ba.lon.values\n    \n    # Generate fine grid with exact integer spacing\n    fine_lat = np.linspace(coarse_lat[0], coarse_lat[-1], len(coarse_lat) * target_lat_factor)\n    fine_lon = np.linspace(coarse_lon[0], coarse_lon[-1], len(coarse_lon) * target_lon_factor)\n    \n    # Create a dummy dataset with the target fine grid\n    target_fine_grid = xr.Dataset({\n        'dummy': (('lat', 'lon'), np.zeros((len(fine_lat), len(fine_lon))))\n    }, coords={'lat': fine_lat, 'lon': fine_lon})\n    \n    # Regrid obs_fine to the compatible grid\n    print(f\"  Regridding to {len(fine_lat)}\u00d7{len(fine_lon)} ({target_lat_factor}\u00d7{target_lon_factor} factors)...\")\n    \n    import xesmf as xe\n    regridder = xe.Regridder(obs_fine_real, target_fine_grid, 'bilinear')\n    obs_fine_compatible = regridder(obs_fine_real)\n    # regridder.clean_weight_file()\n    \n    # Ensure coordinates have proper metadata for iris\n    obs_fine_compatible['lat'].attrs.update({\n        'standard_name': 'latitude',\n        'long_name': 'latitude',\n        'units': 'degrees_north',\n        'axis': 'Y'\n    })\n    obs_fine_compatible['lon'].attrs.update({\n        'standard_name': 'longitude',\n        'long_name': 'longitude',\n        'units': 'degrees_east',\n        'axis': 'X'\n    })\n    obs_fine_compatible['time'].attrs.update({\n        'standard_name': 'time',\n        'long_name': 'time',\n        'axis': 'T'\n    })\n    \n    print(f\"  \u2713 Compatible fine grid created: {obs_fine_compatible['tas'].shape}\")\nelse:\n    obs_fine_compatible = obs_fine_real\n    print(\"  \u2713 Grids already compatible!\")\n\nsd = StatisticalDownscaling(\n    variable='tas',\n    n_iterations=20\n)\nresult = sd.downscale(\n    obs_fine=obs_fine_compatible,\n    sim_coarse=sim_fut_ba\n)\nprint(f\"  \u2713 Downscaled shape: {result['tas'].shape}\")\n\nprint(\"\\n\u2705 BCSD workflow completed successfully!\")\nprint(f\"Output dimensions: {result.dims}\")\nprint(f\"Output resolution: {(result.lat.values[1] - result.lat.values[0]):.3f}\u00b0\")\n    \n</pre>  # Step 3: Statistical downscaling print(\"\\nStep 3: Statistical downscaling...\")  # Check if grids are compatible for downscaling lat_factor = len(obs_fine_real.lat) / len(sim_fut_ba.lat) lon_factor = len(obs_fine_real.lon) / len(sim_fut_ba.lon)  print(f\"  Current downscaling factors: lat={lat_factor:.2f}x, lon={lon_factor:.2f}x\")  if lat_factor != int(lat_factor) or lon_factor != int(lon_factor):     print(\"  \u26a0 Grids not compatible - creating compatible fine grid...\")          # Create a compatible fine grid (integer multiples)     target_lat_factor = round(lat_factor)     target_lon_factor = round(lon_factor)          # Create target coordinates     coarse_lat = sim_fut_ba.lat.values     coarse_lon = sim_fut_ba.lon.values          # Generate fine grid with exact integer spacing     fine_lat = np.linspace(coarse_lat[0], coarse_lat[-1], len(coarse_lat) * target_lat_factor)     fine_lon = np.linspace(coarse_lon[0], coarse_lon[-1], len(coarse_lon) * target_lon_factor)          # Create a dummy dataset with the target fine grid     target_fine_grid = xr.Dataset({         'dummy': (('lat', 'lon'), np.zeros((len(fine_lat), len(fine_lon))))     }, coords={'lat': fine_lat, 'lon': fine_lon})          # Regrid obs_fine to the compatible grid     print(f\"  Regridding to {len(fine_lat)}\u00d7{len(fine_lon)} ({target_lat_factor}\u00d7{target_lon_factor} factors)...\")          import xesmf as xe     regridder = xe.Regridder(obs_fine_real, target_fine_grid, 'bilinear')     obs_fine_compatible = regridder(obs_fine_real)     # regridder.clean_weight_file()          # Ensure coordinates have proper metadata for iris     obs_fine_compatible['lat'].attrs.update({         'standard_name': 'latitude',         'long_name': 'latitude',         'units': 'degrees_north',         'axis': 'Y'     })     obs_fine_compatible['lon'].attrs.update({         'standard_name': 'longitude',         'long_name': 'longitude',         'units': 'degrees_east',         'axis': 'X'     })     obs_fine_compatible['time'].attrs.update({         'standard_name': 'time',         'long_name': 'time',         'axis': 'T'     })          print(f\"  \u2713 Compatible fine grid created: {obs_fine_compatible['tas'].shape}\") else:     obs_fine_compatible = obs_fine_real     print(\"  \u2713 Grids already compatible!\")  sd = StatisticalDownscaling(     variable='tas',     n_iterations=20 ) result = sd.downscale(     obs_fine=obs_fine_compatible,     sim_coarse=sim_fut_ba ) print(f\"  \u2713 Downscaled shape: {result['tas'].shape}\")  print(\"\\n\u2705 BCSD workflow completed successfully!\") print(f\"Output dimensions: {result.dims}\") print(f\"Output resolution: {(result.lat.values[1] - result.lat.values[0]):.3f}\u00b0\")       <pre>\nStep 3: Statistical downscaling...\n  Current downscaling factors: lat=9.25x, lon=9.18x\n  \u26a0 Grids not compatible - creating compatible fine grid...\n  Regridding to 360\u00d7441 (9\u00d79 factors)...\n  \u2713 Compatible fine grid created: (4018, 360, 441)\n\ud83d\udd27 StatisticalDownscaling initialized for tas\n   Iterations: 20\n\n\ud83d\udd04 Starting statistical downscaling for tas...\n   Converting xarray datasets to iris cubes...\n   Creating bilinearly interpolated intermediate data...\n   Created npy_stack directory: /beegfs/muduchuru/pkgs_fnl/climdata/docs/examples/sdba/tmp_bcsd_downscale/sim_fine.nc.npy_stack/\n   Running ISIMIP3BASD statistical downscaling...\n   (This may take a while for large datasets)\ndownscaling at coarse location ...\n(0, 0)\n(0, 1)\n(0, 2)\n(0, 3)\n(0, 4)\n(0, 5)\n(0, 6)\n(0, 7)\n(0, 8)\n(0, 9)\n(0, 10)\n(0, 11)\n(0, 12)\n(0, 13)\n(0, 14)\n(0, 15)\n(0, 16)\n(0, 17)\n(0, 18)\n(0, 19)\n(0, 20)\n(0, 21)\n(0, 22)\n(0, 23)\n(0, 24)\n(0, 25)\n(0, 26)\n(0, 27)\n(0, 28)\n(0, 29)\n(0, 30)\n(0, 31)\n(0, 32)\n(0, 33)\n(0, 34)\n(0, 35)\n(0, 36)\n(0, 37)\n(0, 38)\n(0, 39)\n(0, 40)\n(0, 41)\n(0, 42)\n(0, 43)\n(0, 44)\n(0, 45)\n(0, 46)\n(0, 47)\n(0, 48)\n(1, 0)\n(1, 1)\n(1, 2)\n(1, 3)\n(1, 4)\n(1, 5)\n(1, 6)\n(1, 7)\n(1, 8)\n(1, 9)\n(1, 10)\n(1, 11)\n(1, 12)\n(1, 13)\n(1, 14)\n(1, 15)\n(1, 16)\n(1, 17)\n(1, 18)\n(1, 19)\n(1, 20)\n(1, 21)\n(1, 22)\n(1, 23)\n(1, 24)\n(1, 25)\n(1, 26)\n(1, 27)\n(1, 28)\n(1, 29)\n(1, 30)\n(1, 31)\n(1, 32)\n(1, 33)\n(1, 34)\n(1, 35)\n(1, 36)\n(1, 37)\n(1, 38)\n(1, 39)\n(1, 40)\n(1, 41)\n(1, 42)\n(1, 43)\n(1, 44)\n(1, 45)\n(1, 46)\n(1, 47)\n(1, 48)\n(2, 0)\n(2, 1)\n(2, 2)\n(2, 3)\n(2, 4)\n(2, 5)\n(2, 6)\n(2, 7)\n(2, 8)\n(2, 9)\n(2, 10)\n(2, 11)\n(2, 12)\n(2, 13)\n(2, 14)\n(2, 15)\n(2, 16)\n(2, 17)\n(2, 18)\n(2, 19)\n(2, 20)\n(2, 21)\n(2, 22)\n(2, 23)\n(2, 24)\n(2, 25)\n(2, 26)\n(2, 27)\n(2, 28)\n(2, 29)\n(2, 30)\n(2, 31)\n(2, 32)\n(2, 33)\n(2, 34)\n(2, 35)\n(2, 36)\n(2, 37)\n(2, 38)\n(2, 39)\n(2, 40)\n(2, 41)\n(2, 42)\n(2, 43)\n(2, 44)\n(2, 45)\n(2, 46)\n(2, 47)\n(2, 48)\n(3, 0)\n(3, 1)\n(3, 2)\n(3, 3)\n(3, 4)\n(3, 5)\n(3, 6)\n(3, 7)\n(3, 8)\n(3, 9)\n(3, 10)\n(3, 11)\n(3, 12)\n(3, 13)\n(3, 14)\n(3, 15)\n(3, 16)\n(3, 17)\n(3, 18)\n(3, 19)\n(3, 20)\n(3, 21)\n(3, 22)\n(3, 23)\n(3, 24)\n(3, 25)\n(3, 26)\n(3, 27)\n(3, 28)\n(3, 29)\n(3, 30)\n(3, 31)\n(3, 32)\n(3, 33)\n(3, 34)\n(3, 35)\n(3, 36)\n(3, 37)\n(3, 38)\n(3, 39)\n(3, 40)\n(3, 41)\n(3, 42)\n(3, 43)\n(3, 44)\n(3, 45)\n(3, 46)\n(3, 47)\n(3, 48)\n(4, 0)\n(4, 1)\n(4, 2)\n(4, 3)\n(4, 4)\n(4, 5)\n(4, 6)\n(4, 7)\n(4, 8)\n(4, 9)\n(4, 10)\n(4, 11)\n(4, 12)\n(4, 13)\n(4, 14)\n(4, 15)\n(4, 16)\n(4, 17)\n(4, 18)\n(4, 19)\n(4, 20)\n(4, 21)\n(4, 22)\n(4, 23)\n(4, 24)\n(4, 25)\n(4, 26)\n(4, 27)\n(4, 28)\n(4, 29)\n(4, 30)\n(4, 31)\n(4, 32)\n(4, 33)\n(4, 34)\n(4, 35)\n(4, 36)\n(4, 37)\n(4, 38)\n(4, 39)\n(4, 40)\n(4, 41)\n(4, 42)\n(4, 43)\n(4, 44)\n(4, 45)\n(4, 46)\n(4, 47)\n(4, 48)\n(5, 0)\n(5, 1)\n(5, 2)\n(5, 3)\n(5, 4)\n(5, 5)\n(5, 6)\n(5, 7)\n(5, 8)\n(5, 9)\n(5, 10)\n(5, 11)\n(5, 12)\n(5, 13)\n(5, 14)\n(5, 15)\n(5, 16)\n(5, 17)\n(5, 18)\n(5, 19)\n(5, 20)\n(5, 21)\n(5, 22)\n(5, 23)\n(5, 24)\n(5, 25)\n(5, 26)\n(5, 27)\n(5, 28)\n(5, 29)\n(5, 30)\n(5, 31)\n(5, 32)\n(5, 33)\n(5, 34)\n(5, 35)\n(5, 36)\n(5, 37)\n(5, 38)\n(5, 39)\n(5, 40)\n(5, 41)\n(5, 42)\n(5, 43)\n(5, 44)\n(5, 45)\n(5, 46)\n(5, 47)\n(5, 48)\n(6, 0)\n(6, 1)\n(6, 2)\n(6, 3)\n(6, 4)\n(6, 5)\n(6, 6)\n(6, 7)\n(6, 8)\n(6, 9)\n(6, 10)\n(6, 11)\n(6, 12)\n(6, 13)\n(6, 14)\n(6, 15)\n(6, 16)\n(6, 17)\n(6, 18)\n(6, 19)\n(6, 20)\n(6, 21)\n(6, 22)\n(6, 23)\n(6, 24)\n(6, 25)\n(6, 26)\n(6, 27)\n(6, 28)\n(6, 29)\n(6, 30)\n(6, 31)\n(6, 32)\n(6, 33)\n(6, 34)\n(6, 35)\n(6, 36)\n(6, 37)\n(6, 38)\n(6, 39)\n(6, 40)\n(6, 41)\n(6, 42)\n(6, 43)\n(6, 44)\n(6, 45)\n(6, 46)\n(6, 47)\n(6, 48)\n(7, 0)\n(7, 1)\n(7, 2)\n(7, 3)\n(7, 4)\n(7, 5)\n(7, 6)\n(7, 7)\n(7, 8)\n(7, 9)\n(7, 10)\n(7, 11)\n(7, 12)\n(7, 13)\n(7, 14)\n(7, 15)\n(7, 16)\n(7, 17)\n(7, 18)\n(7, 19)\n(7, 20)\n(7, 21)\n(7, 22)\n(7, 23)\n(7, 24)\n(7, 25)\n(7, 26)\n(7, 27)\n(7, 28)\n(7, 29)\n(7, 30)\n(7, 31)\n(7, 32)\n(7, 33)\n(7, 34)\n(7, 35)\n(7, 36)\n(7, 37)\n(7, 38)\n(7, 39)\n(7, 40)\n(7, 41)\n(7, 42)\n(7, 43)\n(7, 44)\n(7, 45)\n(7, 46)\n(7, 47)\n(7, 48)\n(8, 0)\n(8, 1)\n(8, 2)\n(8, 3)\n(8, 4)\n(8, 5)\n(8, 6)\n(8, 7)\n(8, 8)\n(8, 9)\n(8, 10)\n(8, 11)\n(8, 12)\n(8, 13)\n(8, 14)\n(8, 15)\n(8, 16)\n(8, 17)\n(8, 18)\n(8, 19)\n(8, 20)\n(8, 21)\n(8, 22)\n(8, 23)\n(8, 24)\n(8, 25)\n(8, 26)\n(8, 27)\n(8, 28)\n(8, 29)\n(8, 30)\n(8, 31)\n(8, 32)\n(8, 33)\n(8, 34)\n(8, 35)\n(8, 36)\n(8, 37)\n(8, 38)\n(8, 39)\n(8, 40)\n(8, 41)\n(8, 42)\n(8, 43)\n(8, 44)\n(8, 45)\n(8, 46)\n(8, 47)\n(8, 48)\n(9, 0)\n(9, 1)\n(9, 2)\n(9, 3)\n(9, 4)\n(9, 5)\n(9, 6)\n(9, 7)\n(9, 8)\n(9, 9)\n(9, 10)\n(9, 11)\n(9, 12)\n(9, 13)\n(9, 14)\n(9, 15)\n(9, 16)\n(9, 17)\n(9, 18)\n(9, 19)\n(9, 20)\n(9, 21)\n(9, 22)\n(9, 23)\n(9, 24)\n(9, 25)\n(9, 26)\n(9, 27)\n(9, 28)\n(9, 29)\n(9, 30)\n(9, 31)\n(9, 32)\n(9, 33)\n(9, 34)\n(9, 35)\n(9, 36)\n(9, 37)\n(9, 38)\n(9, 39)\n(9, 40)\n(9, 41)\n(9, 42)\n(9, 43)\n(9, 44)\n(9, 45)\n(9, 46)\n(9, 47)\n(9, 48)\n(10, 0)\n(10, 1)\n(10, 2)\n(10, 3)\n(10, 4)\n(10, 5)\n(10, 6)\n(10, 7)\n(10, 8)\n(10, 9)\n(10, 10)\n(10, 11)\n(10, 12)\n(10, 13)\n(10, 14)\n(10, 15)\n(10, 16)\n(10, 17)\n(10, 18)\n(10, 19)\n(10, 20)\n(10, 21)\n(10, 22)\n(10, 23)\n(10, 24)\n(10, 25)\n(10, 26)\n(10, 27)\n(10, 28)\n(10, 29)\n(10, 30)\n(10, 31)\n(10, 32)\n(10, 33)\n(10, 34)\n(10, 35)\n(10, 36)\n(10, 37)\n(10, 38)\n(10, 39)\n(10, 40)\n(10, 41)\n(10, 42)\n(10, 43)\n(10, 44)\n(10, 45)\n(10, 46)\n(10, 47)\n(10, 48)\n(11, 0)\n(11, 1)\n(11, 2)\n(11, 3)\n(11, 4)\n(11, 5)\n(11, 6)\n(11, 7)\n(11, 8)\n(11, 9)\n(11, 10)\n(11, 11)\n(11, 12)\n(11, 13)\n(11, 14)\n(11, 15)\n(11, 16)\n(11, 17)\n(11, 18)\n(11, 19)\n(11, 20)\n(11, 21)\n(11, 22)\n(11, 23)\n(11, 24)\n(11, 25)\n(11, 26)\n(11, 27)\n(11, 28)\n(11, 29)\n(11, 30)\n(11, 31)\n(11, 32)\n(11, 33)\n(11, 34)\n(11, 35)\n(11, 36)\n(11, 37)\n(11, 38)\n(11, 39)\n(11, 40)\n(11, 41)\n(11, 42)\n(11, 43)\n(11, 44)\n(11, 45)\n(11, 46)\n(11, 47)\n(11, 48)\n(12, 0)\n(12, 1)\n(12, 2)\n(12, 3)\n(12, 4)\n(12, 5)\n(12, 6)\n(12, 7)\n(12, 8)\n(12, 9)\n(12, 10)\n(12, 11)\n(12, 12)\n(12, 13)\n(12, 14)\n(12, 15)\n(12, 16)\n(12, 17)\n(12, 18)\n(12, 19)\n(12, 20)\n(12, 21)\n(12, 22)\n(12, 23)\n(12, 24)\n(12, 25)\n(12, 26)\n(12, 27)\n(12, 28)\n(12, 29)\n(12, 30)\n(12, 31)\n(12, 32)\n(12, 33)\n(12, 34)\n(12, 35)\n(12, 36)\n(12, 37)\n(12, 38)\n(12, 39)\n(12, 40)\n(12, 41)\n(12, 42)\n(12, 43)\n(12, 44)\n(12, 45)\n(12, 46)\n(12, 47)\n(12, 48)\n(13, 0)\n(13, 1)\n(13, 2)\n(13, 3)\n(13, 4)\n(13, 5)\n(13, 6)\n(13, 7)\n(13, 8)\n(13, 9)\n(13, 10)\n(13, 11)\n(13, 12)\n(13, 13)\n(13, 14)\n(13, 15)\n(13, 16)\n(13, 17)\n(13, 18)\n(13, 19)\n(13, 20)\n(13, 21)\n(13, 22)\n(13, 23)\n(13, 24)\n(13, 25)\n(13, 26)\n(13, 27)\n(13, 28)\n(13, 29)\n(13, 30)\n(13, 31)\n(13, 32)\n(13, 33)\n(13, 34)\n(13, 35)\n(13, 36)\n(13, 37)\n(13, 38)\n(13, 39)\n(13, 40)\n(13, 41)\n(13, 42)\n(13, 43)\n(13, 44)\n(13, 45)\n(13, 46)\n(13, 47)\n(13, 48)\n(14, 0)\n(14, 1)\n(14, 2)\n(14, 3)\n(14, 4)\n(14, 5)\n(14, 6)\n(14, 7)\n(14, 8)\n(14, 9)\n(14, 10)\n(14, 11)\n(14, 12)\n(14, 13)\n(14, 14)\n(14, 15)\n(14, 16)\n(14, 17)\n(14, 18)\n(14, 19)\n(14, 20)\n(14, 21)\n(14, 22)\n(14, 23)\n(14, 24)\n(14, 25)\n(14, 26)\n(14, 27)\n(14, 28)\n(14, 29)\n(14, 30)\n(14, 31)\n(14, 32)\n(14, 33)\n(14, 34)\n(14, 35)\n(14, 36)\n(14, 37)\n(14, 38)\n(14, 39)\n(14, 40)\n(14, 41)\n(14, 42)\n(14, 43)\n(14, 44)\n(14, 45)\n(14, 46)\n(14, 47)\n(14, 48)\n(15, 0)\n(15, 1)\n(15, 2)\n(15, 3)\n(15, 4)\n(15, 5)\n(15, 6)\n(15, 7)\n(15, 8)\n(15, 9)\n(15, 10)\n(15, 11)\n(15, 12)\n(15, 13)\n(15, 14)\n(15, 15)\n(15, 16)\n(15, 17)\n(15, 18)\n(15, 19)\n(15, 20)\n(15, 21)\n(15, 22)\n(15, 23)\n(15, 24)\n(15, 25)\n(15, 26)\n(15, 27)\n(15, 28)\n(15, 29)\n(15, 30)\n(15, 31)\n(15, 32)\n(15, 33)\n(15, 34)\n(15, 35)\n(15, 36)\n(15, 37)\n(15, 38)\n(15, 39)\n(15, 40)\n(15, 41)\n(15, 42)\n(15, 43)\n(15, 44)\n(15, 45)\n(15, 46)\n(15, 47)\n(15, 48)\n(16, 0)\n(16, 1)\n(16, 2)\n(16, 3)\n(16, 4)\n(16, 5)\n(16, 6)\n(16, 7)\n(16, 8)\n(16, 9)\n(16, 10)\n(16, 11)\n(16, 12)\n(16, 13)\n(16, 14)\n(16, 15)\n(16, 16)\n(16, 17)\n(16, 18)\n(16, 19)\n(16, 20)\n(16, 21)\n(16, 22)\n(16, 23)\n(16, 24)\n(16, 25)\n(16, 26)\n(16, 27)\n(16, 28)\n(16, 29)\n(16, 30)\n(16, 31)\n(16, 32)\n(16, 33)\n(16, 34)\n(16, 35)\n(16, 36)\n(16, 37)\n(16, 38)\n(16, 39)\n(16, 40)\n(16, 41)\n(16, 42)\n(16, 43)\n(16, 44)\n(16, 45)\n(16, 46)\n(16, 47)\n(16, 48)\n(17, 0)\n(17, 1)\n(17, 2)\n(17, 3)\n(17, 4)\n(17, 5)\n(17, 6)\n(17, 7)\n(17, 8)\n(17, 9)\n(17, 10)\n(17, 11)\n(17, 12)\n(17, 13)\n(17, 14)\n(17, 15)\n(17, 16)\n(17, 17)\n(17, 18)\n(17, 19)\n(17, 20)\n(17, 21)\n(17, 22)\n(17, 23)\n(17, 24)\n(17, 25)\n(17, 26)\n(17, 27)\n(17, 28)\n(17, 29)\n(17, 30)\n(17, 31)\n(17, 32)\n(17, 33)\n(17, 34)\n(17, 35)\n(17, 36)\n(17, 37)\n(17, 38)\n(17, 39)\n(17, 40)\n(17, 41)\n(17, 42)\n(17, 43)\n(17, 44)\n(17, 45)\n(17, 46)\n(17, 47)\n(17, 48)\n(18, 0)\n(18, 1)\n(18, 2)\n(18, 3)\n(18, 4)\n(18, 5)\n(18, 6)\n(18, 7)\n(18, 8)\n(18, 9)\n(18, 10)\n(18, 11)\n(18, 12)\n(18, 13)\n(18, 14)\n(18, 15)\n(18, 16)\n(18, 17)\n(18, 18)\n(18, 19)\n(18, 20)\n(18, 21)\n(18, 22)\n(18, 23)\n(18, 24)\n(18, 25)\n(18, 26)\n(18, 27)\n(18, 28)\n(18, 29)\n(18, 30)\n(18, 31)\n(18, 32)\n(18, 33)\n(18, 34)\n(18, 35)\n(18, 36)\n(18, 37)\n(18, 38)\n(18, 39)\n(18, 40)\n(18, 41)\n(18, 42)\n(18, 43)\n(18, 44)\n(18, 45)\n(18, 46)\n(18, 47)\n(18, 48)\n(19, 0)\n(19, 1)\n(19, 2)\n(19, 3)\n(19, 4)\n(19, 5)\n(19, 6)\n(19, 7)\n(19, 8)\n(19, 9)\n(19, 10)\n(19, 11)\n(19, 12)\n(19, 13)\n(19, 14)\n(19, 15)\n(19, 16)\n(19, 17)\n(19, 18)\n(19, 19)\n(19, 20)\n(19, 21)\n(19, 22)\n(19, 23)\n(19, 24)\n(19, 25)\n(19, 26)\n(19, 27)\n(19, 28)\n(19, 29)\n(19, 30)\n(19, 31)\n(19, 32)\n(19, 33)\n(19, 34)\n(19, 35)\n(19, 36)\n(19, 37)\n(19, 38)\n(19, 39)\n(19, 40)\n(19, 41)\n(19, 42)\n(19, 43)\n(19, 44)\n(19, 45)\n(19, 46)\n(19, 47)\n(19, 48)\n(20, 0)\n(20, 1)\n(20, 2)\n(20, 3)\n(20, 4)\n(20, 5)\n(20, 6)\n(20, 7)\n(20, 8)\n(20, 9)\n(20, 10)\n(20, 11)\n(20, 12)\n(20, 13)\n(20, 14)\n(20, 15)\n(20, 16)\n(20, 17)\n(20, 18)\n(20, 19)\n(20, 20)\n(20, 21)\n(20, 22)\n(20, 23)\n(20, 24)\n(20, 25)\n(20, 26)\n(20, 27)\n(20, 28)\n(20, 29)\n(20, 30)\n(20, 31)\n(20, 32)\n(20, 33)\n(20, 34)\n(20, 35)\n(20, 36)\n(20, 37)\n(20, 38)\n(20, 39)\n(20, 40)\n(20, 41)\n(20, 42)\n(20, 43)\n(20, 44)\n(20, 45)\n(20, 46)\n(20, 47)\n(20, 48)\n(21, 0)\n(21, 1)\n(21, 2)\n(21, 3)\n(21, 4)\n(21, 5)\n(21, 6)\n(21, 7)\n(21, 8)\n(21, 9)\n(21, 10)\n(21, 11)\n(21, 12)\n(21, 13)\n(21, 14)\n(21, 15)\n(21, 16)\n(21, 17)\n(21, 18)\n(21, 19)\n(21, 20)\n(21, 21)\n(21, 22)\n(21, 23)\n(21, 24)\n(21, 25)\n(21, 26)\n(21, 27)\n(21, 28)\n(21, 29)\n(21, 30)\n(21, 31)\n(21, 32)\n(21, 33)\n(21, 34)\n(21, 35)\n(21, 36)\n(21, 37)\n(21, 38)\n(21, 39)\n(21, 40)\n(21, 41)\n(21, 42)\n(21, 43)\n(21, 44)\n(21, 45)\n(21, 46)\n(21, 47)\n(21, 48)\n(22, 0)\n(22, 1)\n(22, 2)\n(22, 3)\n(22, 4)\n(22, 5)\n(22, 6)\n(22, 7)\n(22, 8)\n(22, 9)\n(22, 10)\n(22, 11)\n(22, 12)\n(22, 13)\n(22, 14)\n(22, 15)\n(22, 16)\n(22, 17)\n(22, 18)\n(22, 19)\n(22, 20)\n(22, 21)\n(22, 22)\n(22, 23)\n(22, 24)\n(22, 25)\n(22, 26)\n(22, 27)\n(22, 28)\n(22, 29)\n(22, 30)\n(22, 31)\n(22, 32)\n(22, 33)\n(22, 34)\n(22, 35)\n(22, 36)\n(22, 37)\n(22, 38)\n(22, 39)\n(22, 40)\n(22, 41)\n(22, 42)\n(22, 43)\n(22, 44)\n(22, 45)\n(22, 46)\n(22, 47)\n(22, 48)\n(23, 0)\n(23, 1)\n(23, 2)\n(23, 3)\n(23, 4)\n(23, 5)\n(23, 6)\n(23, 7)\n(23, 8)\n(23, 9)\n(23, 10)\n(23, 11)\n(23, 12)\n(23, 13)\n(23, 14)\n(23, 15)\n(23, 16)\n(23, 17)\n(23, 18)\n(23, 19)\n(23, 20)\n(23, 21)\n(23, 22)\n(23, 23)\n(23, 24)\n(23, 25)\n(23, 26)\n(23, 27)\n(23, 28)\n(23, 29)\n(23, 30)\n(23, 31)\n(23, 32)\n(23, 33)\n(23, 34)\n(23, 35)\n(23, 36)\n(23, 37)\n(23, 38)\n(23, 39)\n(23, 40)\n(23, 41)\n(23, 42)\n(23, 43)\n(23, 44)\n(23, 45)\n(23, 46)\n(23, 47)\n(23, 48)\n(24, 0)\n(24, 1)\n(24, 2)\n(24, 3)\n(24, 4)\n(24, 5)\n(24, 6)\n(24, 7)\n(24, 8)\n(24, 9)\n(24, 10)\n(24, 11)\n(24, 12)\n(24, 13)\n(24, 14)\n(24, 15)\n(24, 16)\n(24, 17)\n(24, 18)\n(24, 19)\n(24, 20)\n(24, 21)\n(24, 22)\n(24, 23)\n(24, 24)\n(24, 25)\n(24, 26)\n(24, 27)\n(24, 28)\n(24, 29)\n(24, 30)\n(24, 31)\n(24, 32)\n(24, 33)\n(24, 34)\n(24, 35)\n(24, 36)\n(24, 37)\n(24, 38)\n(24, 39)\n(24, 40)\n(24, 41)\n(24, 42)\n(24, 43)\n(24, 44)\n(24, 45)\n(24, 46)\n(24, 47)\n(24, 48)\n(25, 0)\n(25, 1)\n(25, 2)\n(25, 3)\n(25, 4)\n(25, 5)\n(25, 6)\n(25, 7)\n(25, 8)\n(25, 9)\n(25, 10)\n(25, 11)\n(25, 12)\n(25, 13)\n(25, 14)\n(25, 15)\n(25, 16)\n(25, 17)\n(25, 18)\n(25, 19)\n(25, 20)\n(25, 21)\n(25, 22)\n(25, 23)\n(25, 24)\n(25, 25)\n(25, 26)\n(25, 27)\n(25, 28)\n(25, 29)\n(25, 30)\n(25, 31)\n(25, 32)\n(25, 33)\n(25, 34)\n(25, 35)\n(25, 36)\n(25, 37)\n(25, 38)\n(25, 39)\n(25, 40)\n(25, 41)\n(25, 42)\n(25, 43)\n(25, 44)\n(25, 45)\n(25, 46)\n(25, 47)\n(25, 48)\n(26, 0)\n(26, 1)\n(26, 2)\n(26, 3)\n(26, 4)\n(26, 5)\n(26, 6)\n(26, 7)\n(26, 8)\n(26, 9)\n(26, 10)\n(26, 11)\n(26, 12)\n(26, 13)\n(26, 14)\n(26, 15)\n(26, 16)\n(26, 17)\n(26, 18)\n(26, 19)\n(26, 20)\n(26, 21)\n(26, 22)\n(26, 23)\n(26, 24)\n(26, 25)\n(26, 26)\n(26, 27)\n(26, 28)\n(26, 29)\n(26, 30)\n(26, 31)\n(26, 32)\n(26, 33)\n(26, 34)\n(26, 35)\n(26, 36)\n(26, 37)\n(26, 38)\n(26, 39)\n(26, 40)\n(26, 41)\n(26, 42)\n(26, 43)\n(26, 44)\n(26, 45)\n(26, 46)\n(26, 47)\n(26, 48)\n(27, 0)\n(27, 1)\n(27, 2)\n(27, 3)\n(27, 4)\n(27, 5)\n(27, 6)\n(27, 7)\n(27, 8)\n(27, 9)\n(27, 10)\n(27, 11)\n(27, 12)\n(27, 13)\n(27, 14)\n(27, 15)\n(27, 16)\n(27, 17)\n(27, 18)\n(27, 19)\n(27, 20)\n(27, 21)\n(27, 22)\n(27, 23)\n(27, 24)\n(27, 25)\n(27, 26)\n(27, 27)\n(27, 28)\n(27, 29)\n(27, 30)\n(27, 31)\n(27, 32)\n(27, 33)\n(27, 34)\n(27, 35)\n(27, 36)\n(27, 37)\n(27, 38)\n(27, 39)\n(27, 40)\n(27, 41)\n(27, 42)\n(27, 43)\n(27, 44)\n(27, 45)\n(27, 46)\n(27, 47)\n(27, 48)\n(28, 0)\n(28, 1)\n(28, 2)\n(28, 3)\n(28, 4)\n(28, 5)\n(28, 6)\n(28, 7)\n(28, 8)\n(28, 9)\n(28, 10)\n(28, 11)\n(28, 12)\n(28, 13)\n(28, 14)\n(28, 15)\n(28, 16)\n(28, 17)\n(28, 18)\n(28, 19)\n(28, 20)\n(28, 21)\n(28, 22)\n(28, 23)\n(28, 24)\n(28, 25)\n(28, 26)\n(28, 27)\n(28, 28)\n(28, 29)\n(28, 30)\n(28, 31)\n(28, 32)\n(28, 33)\n(28, 34)\n(28, 35)\n(28, 36)\n(28, 37)\n(28, 38)\n(28, 39)\n(28, 40)\n(28, 41)\n(28, 42)\n(28, 43)\n(28, 44)\n(28, 45)\n(28, 46)\n(28, 47)\n(28, 48)\n(29, 0)\n(29, 1)\n(29, 2)\n(29, 3)\n(29, 4)\n(29, 5)\n(29, 6)\n(29, 7)\n(29, 8)\n(29, 9)\n(29, 10)\n(29, 11)\n(29, 12)\n(29, 13)\n(29, 14)\n(29, 15)\n(29, 16)\n(29, 17)\n(29, 18)\n(29, 19)\n(29, 20)\n(29, 21)\n(29, 22)\n(29, 23)\n(29, 24)\n(29, 25)\n(29, 26)\n(29, 27)\n(29, 28)\n(29, 29)\n(29, 30)\n(29, 31)\n(29, 32)\n(29, 33)\n(29, 34)\n(29, 35)\n(29, 36)\n(29, 37)\n(29, 38)\n(29, 39)\n(29, 40)\n(29, 41)\n(29, 42)\n(29, 43)\n(29, 44)\n(29, 45)\n(29, 46)\n(29, 47)\n(29, 48)\n(30, 0)\n(30, 1)\n(30, 2)\n(30, 3)\n(30, 4)\n(30, 5)\n(30, 6)\n(30, 7)\n(30, 8)\n(30, 9)\n(30, 10)\n(30, 11)\n(30, 12)\n(30, 13)\n(30, 14)\n(30, 15)\n(30, 16)\n(30, 17)\n(30, 18)\n(30, 19)\n(30, 20)\n(30, 21)\n(30, 22)\n(30, 23)\n(30, 24)\n(30, 25)\n(30, 26)\n(30, 27)\n(30, 28)\n(30, 29)\n(30, 30)\n(30, 31)\n(30, 32)\n(30, 33)\n(30, 34)\n(30, 35)\n(30, 36)\n(30, 37)\n(30, 38)\n(30, 39)\n(30, 40)\n(30, 41)\n(30, 42)\n(30, 43)\n(30, 44)\n(30, 45)\n(30, 46)\n(30, 47)\n(30, 48)\n(31, 0)\n(31, 1)\n(31, 2)\n(31, 3)\n(31, 4)\n(31, 5)\n(31, 6)\n(31, 7)\n(31, 8)\n(31, 9)\n(31, 10)\n(31, 11)\n(31, 12)\n(31, 13)\n(31, 14)\n(31, 15)\n(31, 16)\n(31, 17)\n(31, 18)\n(31, 19)\n(31, 20)\n(31, 21)\n(31, 22)\n(31, 23)\n(31, 24)\n(31, 25)\n(31, 26)\n(31, 27)\n(31, 28)\n(31, 29)\n(31, 30)\n(31, 31)\n(31, 32)\n(31, 33)\n(31, 34)\n(31, 35)\n(31, 36)\n(31, 37)\n(31, 38)\n(31, 39)\n(31, 40)\n(31, 41)\n(31, 42)\n(31, 43)\n(31, 44)\n(31, 45)\n(31, 46)\n(31, 47)\n(31, 48)\n(32, 0)\n(32, 1)\n(32, 2)\n(32, 3)\n(32, 4)\n(32, 5)\n(32, 6)\n(32, 7)\n(32, 8)\n(32, 9)\n(32, 10)\n(32, 11)\n(32, 12)\n(32, 13)\n(32, 14)\n(32, 15)\n(32, 16)\n(32, 17)\n(32, 18)\n(32, 19)\n(32, 20)\n(32, 21)\n(32, 22)\n(32, 23)\n(32, 24)\n(32, 25)\n(32, 26)\n(32, 27)\n(32, 28)\n(32, 29)\n(32, 30)\n(32, 31)\n(32, 32)\n(32, 33)\n(32, 34)\n(32, 35)\n(32, 36)\n(32, 37)\n(32, 38)\n(32, 39)\n(32, 40)\n(32, 41)\n(32, 42)\n(32, 43)\n(32, 44)\n(32, 45)\n(32, 46)\n(32, 47)\n(32, 48)\n(33, 0)\n(33, 1)\n(33, 2)\n(33, 3)\n(33, 4)\n(33, 5)\n(33, 6)\n(33, 7)\n(33, 8)\n(33, 9)\n(33, 10)\n(33, 11)\n(33, 12)\n(33, 13)\n(33, 14)\n(33, 15)\n(33, 16)\n(33, 17)\n(33, 18)\n(33, 19)\n(33, 20)\n(33, 21)\n(33, 22)\n(33, 23)\n(33, 24)\n(33, 25)\n(33, 26)\n(33, 27)\n(33, 28)\n(33, 29)\n(33, 30)\n(33, 31)\n(33, 32)\n(33, 33)\n(33, 34)\n(33, 35)\n(33, 36)\n(33, 37)\n(33, 38)\n(33, 39)\n(33, 40)\n(33, 41)\n(33, 42)\n(33, 43)\n(33, 44)\n(33, 45)\n(33, 46)\n(33, 47)\n(33, 48)\n(34, 0)\n(34, 1)\n(34, 2)\n(34, 3)\n(34, 4)\n(34, 5)\n(34, 6)\n(34, 7)\n(34, 8)\n(34, 9)\n(34, 10)\n(34, 11)\n(34, 12)\n(34, 13)\n(34, 14)\n(34, 15)\n(34, 16)\n(34, 17)\n(34, 18)\n(34, 19)\n(34, 20)\n(34, 21)\n(34, 22)\n(34, 23)\n(34, 24)\n(34, 25)\n(34, 26)\n(34, 27)\n(34, 28)\n(34, 29)\n(34, 30)\n(34, 31)\n(34, 32)\n(34, 33)\n(34, 34)\n(34, 35)\n(34, 36)\n(34, 37)\n(34, 38)\n(34, 39)\n(34, 40)\n(34, 41)\n(34, 42)\n(34, 43)\n(34, 44)\n(34, 45)\n(34, 46)\n(34, 47)\n(34, 48)\n(35, 0)\n(35, 1)\n(35, 2)\n(35, 3)\n(35, 4)\n(35, 5)\n(35, 6)\n(35, 7)\n(35, 8)\n(35, 9)\n(35, 10)\n(35, 11)\n(35, 12)\n(35, 13)\n(35, 14)\n(35, 15)\n(35, 16)\n(35, 17)\n(35, 18)\n(35, 19)\n(35, 20)\n(35, 21)\n(35, 22)\n(35, 23)\n(35, 24)\n(35, 25)\n(35, 26)\n(35, 27)\n(35, 28)\n(35, 29)\n(35, 30)\n(35, 31)\n(35, 32)\n(35, 33)\n(35, 34)\n(35, 35)\n(35, 36)\n(35, 37)\n(35, 38)\n(35, 39)\n(35, 40)\n(35, 41)\n(35, 42)\n(35, 43)\n(35, 44)\n(35, 45)\n(35, 46)\n(35, 47)\n(35, 48)\n(36, 0)\n(36, 1)\n(36, 2)\n(36, 3)\n(36, 4)\n(36, 5)\n(36, 6)\n(36, 7)\n(36, 8)\n(36, 9)\n(36, 10)\n(36, 11)\n(36, 12)\n(36, 13)\n(36, 14)\n(36, 15)\n(36, 16)\n(36, 17)\n(36, 18)\n(36, 19)\n(36, 20)\n(36, 21)\n(36, 22)\n(36, 23)\n(36, 24)\n(36, 25)\n(36, 26)\n(36, 27)\n(36, 28)\n(36, 29)\n(36, 30)\n(36, 31)\n(36, 32)\n(36, 33)\n(36, 34)\n(36, 35)\n(36, 36)\n(36, 37)\n(36, 38)\n(36, 39)\n(36, 40)\n(36, 41)\n(36, 42)\n(36, 43)\n(36, 44)\n(36, 45)\n(36, 46)\n(36, 47)\n(36, 48)\n(37, 0)\n(37, 1)\n(37, 2)\n(37, 3)\n(37, 4)\n(37, 5)\n(37, 6)\n(37, 7)\n(37, 8)\n(37, 9)\n(37, 10)\n(37, 11)\n(37, 12)\n(37, 13)\n(37, 14)\n(37, 15)\n(37, 16)\n(37, 17)\n(37, 18)\n(37, 19)\n(37, 20)\n(37, 21)\n(37, 22)\n(37, 23)\n(37, 24)\n(37, 25)\n(37, 26)\n(37, 27)\n(37, 28)\n(37, 29)\n(37, 30)\n(37, 31)\n(37, 32)\n(37, 33)\n(37, 34)\n(37, 35)\n(37, 36)\n(37, 37)\n(37, 38)\n(37, 39)\n(37, 40)\n(37, 41)\n(37, 42)\n(37, 43)\n(37, 44)\n(37, 45)\n(37, 46)\n(37, 47)\n(37, 48)\n(38, 0)\n(38, 1)\n(38, 2)\n(38, 3)\n(38, 4)\n(38, 5)\n(38, 6)\n(38, 7)\n(38, 8)\n(38, 9)\n(38, 10)\n(38, 11)\n(38, 12)\n(38, 13)\n(38, 14)\n(38, 15)\n(38, 16)\n(38, 17)\n(38, 18)\n(38, 19)\n(38, 20)\n(38, 21)\n(38, 22)\n(38, 23)\n(38, 24)\n(38, 25)\n(38, 26)\n(38, 27)\n(38, 28)\n(38, 29)\n(38, 30)\n(38, 31)\n(38, 32)\n(38, 33)\n(38, 34)\n(38, 35)\n(38, 36)\n(38, 37)\n(38, 38)\n(38, 39)\n(38, 40)\n(38, 41)\n(38, 42)\n(38, 43)\n(38, 44)\n(38, 45)\n(38, 46)\n(38, 47)\n(38, 48)\n(39, 0)\n(39, 1)\n(39, 2)\n(39, 3)\n(39, 4)\n(39, 5)\n(39, 6)\n(39, 7)\n(39, 8)\n(39, 9)\n(39, 10)\n(39, 11)\n(39, 12)\n(39, 13)\n(39, 14)\n(39, 15)\n(39, 16)\n(39, 17)\n(39, 18)\n(39, 19)\n(39, 20)\n(39, 21)\n(39, 22)\n(39, 23)\n(39, 24)\n(39, 25)\n(39, 26)\n(39, 27)\n(39, 28)\n(39, 29)\n(39, 30)\n(39, 31)\n(39, 32)\n(39, 33)\n(39, 34)\n(39, 35)\n(39, 36)\n(39, 37)\n(39, 38)\n(39, 39)\n(39, 40)\n(39, 41)\n(39, 42)\n(39, 43)\n(39, 44)\n(39, 45)\n(39, 46)\n(39, 47)\n(39, 48)\n   \u2705 Statistical downscaling complete!\n   \ud83d\udce6 Collecting results from npy_stack...\n</pre> <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 66\n     60     print(\"  \u2713 Grids already compatible!\")\n     62 sd = StatisticalDownscaling(\n     63     variable='tas',\n     64     n_iterations=20\n     65 )\n---&gt; 66 result = sd.downscale(\n     67     obs_fine=obs_fine_compatible,\n     68     sim_coarse=sim_fut_ba\n     69 )\n     70 print(f\"  \u2713 Downscaled shape: {result['tas'].shape}\")\n     72 print(\"\\n\u2705 BCSD workflow completed successfully!\")\n\nFile /beegfs/muduchuru/pkgs_fnl/climdata/climdata/sdba/bcsd.py:685, in StatisticalDownscaling.downscale(self, obs_fine, sim_coarse, output_path, **kwargs)\n    682 d = da.from_npy_stack(npy_stack_path, mmap_mode=None).reshape(sim_fine_cube.shape)\n    684 # Set the collected data\n--&gt; 685 sim_fine_cube.data = np.ma.masked_array(d.compute())\n    687 # Save the result cube to file\n    688 print(f\"   \ud83d\udcbe Saving result to: {sim_fine_path}\")\n\nFile ~/miniforge3/envs/sdba/lib/python3.10/site-packages/dask/base.py:376, in DaskMethodsMixin.compute(self, **kwargs)\n    352 def compute(self, **kwargs):\n    353     \"\"\"Compute this dask collection\n    354 \n    355     This turns a lazy Dask collection into its in-memory equivalent.\n   (...)\n    374     dask.compute\n    375     \"\"\"\n--&gt; 376     (result,) = compute(self, traverse=False, **kwargs)\n    377     return result\n\nFile ~/miniforge3/envs/sdba/lib/python3.10/site-packages/dask/base.py:664, in compute(traverse, optimize_graph, scheduler, get, *args, **kwargs)\n    661     postcomputes.append(x.__dask_postcompute__())\n    663 with shorten_traceback():\n--&gt; 664     results = schedule(dsk, keys, **kwargs)\n    666 return repack([f(r, *a) for r, (f, a) in zip(results, postcomputes)])\n\nFile ~/miniforge3/envs/sdba/lib/python3.10/site-packages/dask/utils.py:1241, in methodcaller.__call__(self, _methodcaller__obj, *args, **kwargs)\n   1240 def __call__(self, __obj, *args, **kwargs):\n-&gt; 1241     return getattr(__obj, self.method)(*args, **kwargs)\n\nValueError: cannot reshape array of size 13149 into shape (4018,1,1)</pre> In\u00a0[\u00a0]: Copied! <pre># Re-run downscaling\nprint(\"\\nStep 3: Statistical downscaling (with fix)...\")\n\nsd = StatisticalDownscaling(\n    variable='tas',\n    n_iterations=20\n)\nresult = sd.downscale(\n    obs_fine=obs_fine_compatible,\n    sim_coarse=sim_fut_ba\n)\nprint(f\"  \u2713 Downscaled shape: {result['tas'].shape}\")\n\nprint(\"\\n\u2705 BCSD workflow completed successfully!\")\nprint(f\"Output dimensions: {result.dims}\")\nprint(f\"Output resolution: {(result.lat.values[1] - result.lat.values[0]):.3f}\u00b0\")\n</pre> # Re-run downscaling print(\"\\nStep 3: Statistical downscaling (with fix)...\")  sd = StatisticalDownscaling(     variable='tas',     n_iterations=20 ) result = sd.downscale(     obs_fine=obs_fine_compatible,     sim_coarse=sim_fut_ba ) print(f\"  \u2713 Downscaled shape: {result['tas'].shape}\")  print(\"\\n\u2705 BCSD workflow completed successfully!\") print(f\"Output dimensions: {result.dims}\") print(f\"Output resolution: {(result.lat.values[1] - result.lat.values[0]):.3f}\u00b0\")"},{"location":"examples/sdba/sdba/#load-fine-resolution-reference-data-mswx","title":"Load Fine-Resolution Reference Data (MSWX)\u00b6","text":"<p>MSWX provides global weather data at 0.1\u00b0 resolution, blending multiple data sources.</p>"},{"location":"examples/sdba/sdba/#load-coarse-gcm-data-cmip6","title":"Load Coarse GCM Data (CMIP6)\u00b6","text":"<p>Load historical and future CMIP6 data for the same region.</p>"},{"location":"examples/sdba/sdba/#prepare-data-for-bcsd","title":"Prepare Data for BCSD\u00b6","text":"<p>Rename MSWX variable to match CMIP naming convention and ensure consistent coordinates.</p>"},{"location":"examples/sdba/sdba/#run-the-complete-workflow","title":"Run the Complete Workflow\u00b6","text":"<p>The workflow automatically:</p> <ol> <li>Regrids fine observations to coarse GCM grid</li> <li>Performs bias correction at coarse resolution</li> <li>Downscales corrected data to fine resolution</li> </ol>"}]}