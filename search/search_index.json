{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to climdata","text":""},{"location":"#climdata-quickstart-overview","title":"ClimData \u2014 Quickstart &amp; Overview","text":"<p>ClimData provides a unified interface for extracting climate data from multiple providers (MSWX, CMIP, POWER, DWD, HYRAS), computing extreme indices, and converting results to tabular form. The ClimData (or ClimateExtractor) class is central: it manages configuration, extraction, index computation, and common I/O.</p>"},{"location":"#key-features","title":"Key features","text":"<ul> <li>Provider-agnostic extraction (point / region / shapefile)</li> <li>Unit normalization via xclim</li> <li>Compute extreme indices using package indices</li> <li>Convert xarray Datasets \u2192 long-form pandas DataFrames</li> <li>Simple workflow runner for chained actions</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>1) Create and activate a conda environment: <pre><code># create\nconda create -n climdata python=3.11 -y\n\n# activate\nconda activate climdata\n</code></pre></p> <p>2) Install via pip (PyPI, if available) or from source: <pre><code># from PyPI\npip install climdata\n\n# or from local source (editable)\ngit clone &lt;repo-url&gt;\ncd climdata\npip install -e .\n</code></pre></p> <p>Install optional extras as needed (e.g., xclim, shapely, hydra, dask): <pre><code>pip install xarray xclim shapely hydra-core dask \"pandas&gt;=1.5\"\n</code></pre></p>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from climdata import ClimData  # or from climdata.utils.wrapper_workflow import ClimateExtractor\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n    \"data_dir=/path/to/data\",\n    \"index=tn10p\",\n]\n\n# initialize\nextractor = ClimData(overrides=overrides)\n\n# extract data (returns xarray.Dataset and updates internal state)\nds = extractor.extract()\n\n# compute index (uses cfg.index)\nds_index = extractor.calc_index(ds)\n\n# convert to long-form dataframe and save\ndf = extractor.to_dataframe(ds_index)\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"#workflow-runner","title":"Workflow runner","text":"<p>Use <code>run_workflow</code> for multi-step sequences: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n</code></pre> <code>WorkflowResult</code> contains produced dataset(s), dataframe(s), and filenames.</p>"},{"location":"#documentation-api","title":"Documentation &amp; API","text":"<ul> <li>See API docs under <code>docs/api/</code> for detailed descriptions of ClimData/ClimateExtractor methods.</li> <li>Examples and notebooks are under <code>examples/</code>.</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<ul> <li>Run tests and lint locally.</li> <li>Follow project coding and documentation conventions; submit PRs with tests.</li> </ul>"},{"location":"#license","title":"License","text":"<p>Refer to the repository LICENSE file for terms.</p>"},{"location":"#tip","title":"\u26a1\ufe0f Tip","text":"<ul> <li> <p>Make sure <code>yq</code> is installed:   <pre><code>brew install yq   # macOS\n# OR\npip install yq\n</code></pre></p> </li> <li> <p>To see available variables for a specific dataset (for example <code>mswx</code>), run:   <pre><code>python download_location.py --cfg job | yq '.mappings.mswx.variables | keys'\n</code></pre></p> </li> </ul>"},{"location":"#key-features_1","title":"\u2699\ufe0f Key Features","text":"<ul> <li>Supports multiple weather data providers</li> <li>Uses <code>xarray</code> for robust gridded data extraction</li> <li>Handles curvilinear and rectilinear grids</li> <li>Uses a Google Drive Service Account for secure downloads</li> <li>Easily reproducible runs using Hydra</li> </ul>"},{"location":"#google-drive-api-setup","title":"\ud83d\udce1 Google Drive API Setup","text":"<p>This project uses the Google Drive API with a Service Account to securely download weather data files from a shared Google Drive folder.</p> <p>Follow these steps to set it up correctly:</p>"},{"location":"#1-create-a-google-cloud-project","title":"\u2705 1. Create a Google Cloud Project","text":"<ul> <li>Go to Google Cloud Console.</li> <li>Click \u201cSelect Project\u201d \u2192 \u201cNew Project\u201d.</li> <li>Enter a project name (e.g. <code>WeatherDataDownloader</code>).</li> <li>Click \u201cCreate\u201d.</li> </ul>"},{"location":"#2-enable-the-google-drive-api","title":"\u2705 2. Enable the Google Drive API","text":"<ul> <li>In the left sidebar, go to APIs &amp; Services \u2192 Library.</li> <li>Search for \u201cGoogle Drive API\u201d.</li> <li>Click it, then click \u201cEnable\u201d.</li> </ul>"},{"location":"#3-create-a-service-account","title":"\u2705 3. Create a Service Account","text":"<ul> <li>Go to IAM &amp; Admin \u2192 Service Accounts.</li> <li>Click \u201cCreate Service Account\u201d.</li> <li>Enter a name (e.g. <code>weather-downloader-sa</code>).</li> <li>Click \u201cCreate and Continue\u201d. You can skip assigning roles for read-only Drive access.</li> <li>Click \u201cDone\u201d to finish.</li> </ul>"},{"location":"#4-create-and-download-a-json-key","title":"\u2705 4. Create and Download a JSON Key","text":"<ul> <li>After creating the Service Account, click on its email address to open its details.</li> <li>Go to the \u201cKeys\u201d tab.</li> <li>Click \u201cAdd Key\u201d \u2192 \u201cCreate new key\u201d \u2192 choose <code>JSON</code> \u2192 click \u201cCreate\u201d.</li> <li>A <code>.json</code> key file will download automatically. Store it securely!</li> </ul>"},{"location":"#5-store-the-json-key-securely","title":"\u2705 5. Store the JSON Key Securely","text":"<ul> <li>Place the downloaded <code>.json</code> key in the conf folder with the name service.json. </li> </ul>"},{"location":"#setup-instructions-from-era5-api","title":"Setup Instructions from ERA5 api","text":""},{"location":"#1-cds-api-key-setup","title":"1. CDS API Key Setup","text":"<ol> <li>Create a free account on the Copernicus Climate Data Store</li> <li>Once logged in, go to your user profile</li> <li>Click on the \"Show API key\" button</li> <li>Create the file <code>~/.cdsapirc</code> with the following content:</li> </ol> <pre><code>url: https://cds.climate.copernicus.eu/api/v2\nkey: &lt;your-api-key-here&gt;\n</code></pre> <ol> <li>Make sure the file has the correct permissions: <code>chmod 600 ~/.cdsapirc</code></li> </ol>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#climdata.utils.wrapper_workflow.ClimData","title":"<code> climdata.utils.wrapper_workflow.ClimData        </code>","text":"<p>Climate data extraction and extreme-index workflow manager.</p> <p>This class provides a high-level API for:   - loading/configuring dataset providers via Hydra config,   - uploading NetCDF/CSV content into xarray Datasets,   - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER),   - computing extreme indices using configured xclim indices,   - converting datasets to long-form DataFrames and saving results.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData--attributes","title":"Attributes","text":"<p>cfg : DictConfig     Hydra configuration object describing dataset, region/time/variables, outputs. current_ds : xr.Dataset     The most recently loaded or extracted dataset. current_df : pd.DataFrame     The most recently produced long-form DataFrame. filename_csv / filename_nc / filename_zarr : str     Generated output filename templates/paths.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData--example","title":"Example","text":"<p>extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe']) extractor.extract() idx_ds = extractor.calc_index() df = extractor.to_dataframe(idx_ds) extractor.to_csv(df)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>class ClimateExtractor:\n    \"\"\"\n    Climate data extraction and extreme-index workflow manager.\n\n    This class provides a high-level API for:\n      - loading/configuring dataset providers via Hydra config,\n      - uploading NetCDF/CSV content into xarray Datasets,\n      - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER),\n      - computing extreme indices using configured xclim indices,\n      - converting datasets to long-form DataFrames and saving results.\n\n    Attributes\n    ----------\n    cfg : DictConfig\n        Hydra configuration object describing dataset, region/time/variables, outputs.\n    current_ds : xr.Dataset\n        The most recently loaded or extracted dataset.\n    current_df : pd.DataFrame\n        The most recently produced long-form DataFrame.\n    filename_csv / filename_nc / filename_zarr : str\n        Generated output filename templates/paths.\n\n    Example\n    -------\n    extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe'])\n    extractor.extract()\n    idx_ds = extractor.calc_index()\n    df = extractor.to_dataframe(idx_ds)\n    extractor.to_csv(df)\n    \"\"\"\n\n    def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n        \"\"\"\n        Initialize the workflow manager and load configuration.\n\n        Parameters\n        ----------\n        cfg_name : str\n            Name of the Hydra configuration (default: \"config\").\n        conf_path : str, optional\n            Optional config path override (not commonly used).\n        overrides : list[str], optional\n            Hydra overrides to apply to the configuration.\n        \"\"\"\n        self.cfg_name = cfg_name\n        self.conf_path = conf_path\n        self.cfg: Optional[DictConfig] = None\n\n        # Stage datasets\n        self.ds = None\n        self.current_ds = None\n        self.index_ds = None\n        self.impute_ds = None\n        self.bias_corrected_ds = None\n\n        # Stage DataFrames\n        self.raw_df = None\n        self.current_df = None\n        self.index_df = None\n        self.impute_df = None\n        self.bias_corrected_df = None\n        self.df = None  # alias for current_df\n\n        # filenames\n        self.filename = None\n        self.filetype = None\n\n        # Automatically load config on init\n        self.load_config(overrides)\n        self.cfg = self.preprocess_aoi(self.cfg)\n    def _gen_fn(self, ds: xr.Dataset):\n        \"\"\"\n        Create filenames (csv, nc, zarr) using config templates and dataset metadata.\n        Automatically handles coordinate aliases such as lat/latitude, lon/longitude,\n        time/date.\n        \"\"\"\n\n        # ------------------------\n        # Helper: find coord alias\n        # ------------------------\n        def find_coord(ds, names):\n            \"\"\"Return ds coordinate if any alias in names exists.\"\"\"\n            for name in names:\n                if name in ds.coords:\n                    return ds[name]\n            return None\n\n        # ------------------------\n        # Extract coordinates\n        # ------------------------\n        lat = find_coord(ds, [\"lat\", \"latitude\"])\n        lon = find_coord(ds, [\"lon\", \"longitude\"])\n        time = find_coord(ds, [\"time\", \"date\"])\n\n        # ------------------------\n        # Provider\n        # ------------------------\n        provider = ds.attrs.get(\"source\", \"unknown\")\n\n        # ------------------------\n        # Parameter(s)\n        # ------------------------\n        vars_list = list(ds.data_vars)\n        parameter = vars_list[0] if len(vars_list) == 1 else \"_\".join(vars_list)\n\n        # ------------------------\n        # Latitude range\n        # ------------------------\n        if lat is None:\n            lat_str = \"unknown\"\n            lat_range = \"unknown\"\n        else:\n            # Flatten in case of 2D lat/lon grid\n            lat_vals = lat.values.reshape(-1)\n            lat_min = float(lat_vals.min())\n            lat_max = float(lat_vals.max())\n\n            if lat_min == lat_max:\n                lat_str = f\"{lat_min}\"\n                lat_range = f\"{lat_min}\"\n            else:\n                lat_str = f\"{lat_min}_{lat_max}\"\n                lat_range = f\"{lat_min}-{lat_max}\"\n\n        # ------------------------\n        # Longitude range\n        # ------------------------\n        if lon is None:\n            lon_str = \"unknown\"\n            lon_range = \"unknown\"\n        else:\n            lon_vals = lon.values.reshape(-1)\n            lon_min = float(lon_vals.min())\n            lon_max = float(lon_vals.max())\n\n            if lon_min == lon_max:\n                lon_str = f\"{lon_min}\"\n                lon_range = f\"{lon_min}\"\n            else:\n                lon_str = f\"{lon_min}_{lon_max}\"\n                lon_range = f\"{lon_min}-{lon_max}\"\n\n        # ------------------------\n        # Time range\n        # ------------------------\n        if time is None:\n            start = end = \"unknown\"\n        else:\n            tvals = pd.to_datetime(time.values)\n            start = tvals.min().strftime(\"%Y-%m-%d\")\n            end = tvals.max().strftime(\"%Y-%m-%d\")\n\n        # ------------------------\n        # Build filenames\n        # ------------------------\n        outdir = Path(self.cfg.output.out_dir)\n\n        def build(fn_template):\n            return fn_template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range,\n                lon_range=lon_range,\n            )\n\n        self.filename_csv = str(outdir / build(self.cfg.output.filename_csv))\n        self.filename_nc = str(outdir / build(self.cfg.output.filename_nc))\n        self.filename_zarr = str(outdir / build(self.cfg.output.filename_zarr))\n\n\n    def _gen_fn_cfg(self):\n        \"\"\"\n        Generate output filenames using ONLY cfg and extracted ds,\n        without relying on uploaded dataset metadata.\n        \"\"\"\n\n        cfg = self.cfg\n        out = cfg.output\n        provider = cfg.dataset.lower()\n        if self.current_ds:\n            if len(self.current_ds.data_vars) == 0:\n                parameter = \"unknown\"\n            elif len(self.current_ds.data_vars) == 1:\n                parameter = next(iter(self.current_ds.data_vars))\n            else:\n                parameter = \"_\".join(self.current_ds.data_vars)\n        else:\n            parameter = \"_\".join(self.cfg.variables)\n        # --------------------------------\n        # Determine lat/lon values\n        # --------------------------------\n        if cfg.lat is not None and cfg.lon is not None:\n            lat_range = lon_range = None   # single point\n            lat_str = str(cfg.lat)\n            lon_str = str(cfg.lon)\n        else:\n            b = cfg.bounds[cfg.region]\n            lat_min, lat_max = b[\"lat_min\"], b[\"lat_max\"]\n            lon_min, lon_max = b[\"lon_min\"], b[\"lon_max\"]\n\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # --------------------------------\n        # Time range from cfg\n        # --------------------------------\n        start = pd.to_datetime(cfg.time_range.start_date).strftime(\"%Y-%m-%d\")\n        end = pd.to_datetime(cfg.time_range.end_date).strftime(\"%Y-%m-%d\")\n\n        # --------------------------------\n        # Format filenames\n        # --------------------------------\n        def format_template(template):\n            return template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range or lat_str,\n                lon_range=lon_range or lon_str,\n            )\n\n        out_dir = Path('./')\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        self.filename_csv = str(out_dir / format_template(out.filename_csv))\n        self.filename_nc = str(out_dir / format_template(out.filename_nc))\n        self.filename_zarr = str(out_dir / format_template(out.filename_zarr))\n\n    # ----------------------------\n    # Hydra config\n    # ----------------------------\n    def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n        \"\"\"\n        Load and compose the Hydra configuration.\n\n        Parameters\n        ----------\n        overrides : list[str], optional\n            Hydra overrides to apply when composing the configuration.\n\n        Returns\n        -------\n        DictConfig\n            Composed Hydra configuration object and stored on `self.cfg`.\n        \"\"\"\n        overrides = overrides or []\n        conf_dir = _ensure_local_conf()\n        rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n        if not GlobalHydra.instance().is_initialized():\n            hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n        else:\n            hydra_ctx = None\n\n        if hydra_ctx:\n            with hydra_ctx:\n                self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        else:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        return self.cfg\n\n    # ----------------------------\n    # AOI preprocessing\n    # ----------------------------\n    def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n        \"\"\"\n        Process an 'aoi' specification in the configuration.\n\n        Supports GeoJSON strings or dictionaries for FeatureCollection, Feature,\n        or simple geometry objects (Point/Polygon). When a Point is provided,\n        `cfg.lat` and `cfg.lon` are set. When a Polygon is provided, `cfg.bounds`\n        is set and `cfg.region` is set to \"custom\".\n\n        Parameters\n        ----------\n        cfg : DictConfig\n            Configuration object with optional `aoi` entry.\n\n        Returns\n        -------\n        DictConfig\n            The modified configuration.\n        \"\"\"\n        if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n            return cfg\n\n        if isinstance(cfg.aoi, str):\n            try:\n                cfg.aoi = json.loads(cfg.aoi)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid AOI JSON string\")\n\n        aoi = cfg.aoi\n\n        if aoi.get(\"type\") == \"FeatureCollection\":\n            geom = shape(aoi[\"features\"][0][\"geometry\"])\n        elif aoi.get(\"type\") == \"Feature\":\n            geom = shape(aoi[\"geometry\"])\n        elif \"type\" in aoi:\n            geom = shape(aoi)\n        else:\n            raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n        if isinstance(geom, Point):\n            cfg.lat = geom.y\n            cfg.lon = geom.x\n            cfg.bounds = None\n        elif isinstance(geom, Polygon):\n            minx, miny, maxx, maxy = geom.bounds\n            cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                     \"lon_min\": minx, \"lon_max\": maxx}}\n            cfg.region = \"custom\"\n            cfg.lat = None\n            cfg.lon = None\n        else:\n            raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n        return cfg\n\n    # ----------------------------\n    # Upload NetCDF\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n        \"\"\"\n        Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n        Parameters\n        ----------\n        nc_file : str\n            Path to the NetCDF file to open.\n\n        Returns\n        -------\n        xr.Dataset\n            The loaded dataset (also sets `self.current_ds`).\n        \"\"\"\n        if not os.path.exists(nc_file):\n            raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n        ds = xr.open_dataset(nc_file)\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                                for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Upload CSV \u2192 xarray.Dataset\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n        \"\"\"\n        Load a long-form CSV into an xarray.Dataset.\n\n        CSV must contain `time` and `lat`/`latitude`, `lon`/`longitude`, `variable`, `value`.\n        Units may be supplied in a `units` column and an optional `source` column is recognised.\n\n        Parameters\n        ----------\n        csv_file : str\n            Path to the CSV file to load.\n\n        Returns\n        -------\n        xr.Dataset\n            The converted dataset (also sets `self.current_ds`).\n        \"\"\"\n        if not os.path.exists(csv_file):\n            raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n        df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n        lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n        lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n        if lat_col is None or lon_col is None:\n            raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n        id_vars = [\"time\", lat_col, lon_col]\n        df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n        ds = df_wide.set_index(id_vars).to_xarray()\n\n        # Attach units from CSV\n        for var in ds.data_vars:\n            units_series = df[df[\"variable\"] == var][\"units\"]\n            ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n        # Global source attribute\n        if \"source\" in df.columns:\n            source_series = df[\"source\"].dropna().unique()\n            if len(source_series) &gt; 0:\n                ds.attrs[\"source\"] = source_series[0]\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Extract data from datasets like CMIP, DWD, etc.\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def extract(self) -&gt; xr.Dataset:\n        \"\"\"\n        Extract data from the configured provider using `self.cfg`.\n\n        Uses provider-specific classes (e.g., `CMIP`, `DWD`, `MSWX`, `HYRAS`, `POWER`)\n        to fetch, load and extract datasets. When extraction completes, units are\n        converted to those declared in `cfg.varinfo`, the dataset is computed,\n        and filenames are generated from the configuration.\n\n        Returns\n        -------\n        xr.Dataset\n            The extracted and computed dataset (also sets `self.current_ds`).\n        \"\"\"\n        cfg = self.cfg\n        extract_kwargs = {}\n\n        if cfg.lat is not None and cfg.lon is not None:\n            extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n            if cfg.dataset == \"dwd\":\n                extract_kwargs[\"buffer_km\"] = 30\n        elif cfg.region is not None:\n            extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n        elif cfg.shapefile is not None:\n            extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n        ds = None\n        dataset_upper = cfg.dataset.upper()\n\n        if dataset_upper == \"MSWX\":\n            ds_vars = []\n            for var in cfg.variables:\n                mswx = climdata.MSWX(cfg)\n                mswx.extract(**extract_kwargs)\n                mswx.load(var)\n                ds_vars.append(mswx.dataset)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"CMIP\":\n            cmip = climdata.CMIP(cfg)\n            cmip.fetch()\n            cmip.load()\n            cmip.extract(**extract_kwargs)\n            ds = cmip.ds\n\n        elif dataset_upper == \"POWER\":\n            power = climdata.POWER(cfg)\n            power.fetch()\n            power.load()\n            ds = power.ds\n\n        elif dataset_upper == \"DWD\":\n            ds_vars = []\n            for var in cfg.variables:\n                dwd = climdata.DWD(cfg)\n                ds_var = dwd.extract(variable=var, **extract_kwargs)\n                ds_vars.append(ds_var)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"HYRAS\":\n            hyras = climdata.HYRAS(cfg)\n            ds_vars = []\n            for var in cfg.variables:\n                hyras.extract(**extract_kwargs)\n                ds_vars.append(hyras.load(var)[[var]])\n            ds = xr.merge(ds_vars, compat=\"override\")\n\n        for var in ds.data_vars:\n            ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n        ds = ds.compute()\n\n        self._gen_fn_cfg()\n\n        return ds\n    # ----------------------------\n    # Compute extreme index\n    # ----------------------------\n    @update_ds(attr_name='index_ds')\n    def calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate the configured extreme index using xclim indices.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to operate on. If None, `self.current_ds` is used.\n\n        Returns\n        -------\n        xr.Dataset\n            The computed index as an xarray Dataset (also sets `self.index_ds`).\n        \"\"\"\n        cfg = self.cfg\n\n        # Use provided ds or fallback\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.index is None:\n            print(\"No index selected.\")\n            return None\n\n        if \"time\" in ds.coords:\n            years = pd.to_datetime(ds.time.values).year\n            n_years = len(pd.unique(years))\n            if n_years &lt; 30:\n                warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n        indices = extreme_index(cfg, ds)\n        index_ds = indices.calculate(cfg.index).compute()\n        index_ds = index_ds.to_dataset(name=cfg.index)\n\n        self._gen_fn_cfg()\n\n        return index_ds\n    # ----------------------------\n    # Dataset \u2192 Long-form DataFrame\n    # ----------------------------\n    @update_df()\n    def to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert a dataset to a long-form pandas DataFrame.\n\n        The output contains columns: time, lat, lon (or latitude/longitude),\n        variable, value, units, source.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to convert. If None, uses `self.current_ds`.\n\n        Returns\n        -------\n        pd.DataFrame\n            Long-form DataFrame (also sets `self.current_df`).\n        \"\"\"\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        df = ds.to_dataframe().reset_index()\n\n        id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n        value_vars = [v for v in ds.data_vars if v in df.columns]\n\n        if not value_vars:\n            raise ValueError(\"No variables in dataset available to melt into long format\")\n\n        df_long = df.melt(\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"variable\",\n            value_name=\"value\"\n        )\n\n        df_long[\"units\"] = df_long[\"variable\"].apply(\n            lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n        )\n        df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n        self._gen_fn_cfg()\n        return df_long\n\n    # ----------------------------\n    # Save CSV\n    # ----------------------------\n    def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Save a DataFrame to CSV.\n\n        Parameters\n        ----------\n        df : pd.DataFrame, optional\n            DataFrame to save. Defaults to `self.current_df`.\n        filename : str, optional\n            Output filename. Defaults to `self.filename_csv`.\n\n        Returns\n        -------\n        str\n            The path of the written CSV file.\n        \"\"\"\n        df = df if df is not None else self.current_df\n\n        filename = filename or getattr(self, \"filename_csv\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_csv is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        df.to_csv(filename, index=False)\n        self.filename_csv = str(path)\n        self.current_filename = str(path)\n\n        print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n        return filename\n\n    def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Save an xarray Dataset to NetCDF.\n        - If ds is None: save current_ds.\n        - If filename is None: use self.filename_nc.\n        - Creates directories if needed.\n        - Updates self.filename_nc and self.current_filename.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to save. If None, uses `self.current_ds`.\n        filename : str, optional\n            Output filename. Defaults to `self.filename_nc`.\n\n        Returns\n        -------\n        str\n            The path of the written NetCDF file.\n        \"\"\"\n\n        # -------------------------------\n        # 1. Determine dataset to save\n        # -------------------------------\n        ds = ds or getattr(self, \"current_ds\", None)\n        if ds is None:\n            raise ValueError(\"No dataset available to save\")\n\n        # -------------------------------\n        # 2. Determine filename\n        # -------------------------------\n        filename = filename or getattr(self, \"filename_nc\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_nc is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # -------------------------------\n        # 3. Save to NetCDF\n        # -------------------------------\n        ds.to_netcdf(path)\n\n        # -------------------------------\n        # 4. Track filenames\n        # -------------------------------\n        self.filename_nc = str(path)\n        self.current_filename = str(path)\n\n        print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n        return str(path)\n\n    # ----------------------------\n    # Unified workflow\n    # ----------------------------\n    def run_workflow(self, overrides: Optional[List[str]] = None,\n                     actions: Optional[List[str]] = None,\n                     file: Optional[str] = None) -&gt; WorkflowResult:\n        \"\"\"\n        Execute a sequence of workflow actions.\n\n        Parameters\n        ----------\n        overrides : list[str], optional\n            Hydra overrides to apply (not all actions will use these).\n        actions : list[str], optional\n            Ordered list of actions to perform. Supported actions include:\n            'upload_netcdf', 'upload_csv', 'extract', 'calc_index',\n            'to_dataframe', 'to_csv', 'to_nc'.\n        file : str, optional\n            File path used for upload actions when required.\n\n        Returns\n        -------\n        WorkflowResult\n            Named result container with populated fields for dataframe/dataset/filenames.\n        \"\"\"\n        actions = actions or [\"extract\", \"compute_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n        result = WorkflowResult(cfg=self.cfg)\n        for action in actions:\n\n            if action == \"upload_netcdf\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                        \"but none was provided.\"\n                    )\n                    # Validate extension\n                valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_netcdf: '{file}'. \"\n                        f\"Expected one of: {valid_nc_ext}\"\n                    )\n                self.upload_netcdf(file)\n                result.dataset = self.current_ds\n\n            elif action == \"upload_csv\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_csv' requires argument 'csv_file', \"\n                        \"but none was provided.\"\n                    )\n\n                # Validate CSV extension\n                valid_csv_ext = (\".csv\", \".csv.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_csv: '{file}'. \"\n                        f\"Expected one of: {valid_csv_ext}\"\n                    )\n\n                self.upload_csv(file)\n                result.dataset = self.current_ds\n\n            elif action == \"extract\":\n                if self.cfg.dataset is None:\n                    raise ValueError(\n                        \"Action 'extract' cannot run because no dataset provider is set \"\n                        \"(cfg.dataset is None).\"\n                    )\n                self.extract()\n                result.dataset = self.current_ds\n\n            elif action == \"calc_index\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before computing an index.\"\n                    )\n                self.calc_index()\n                result.index_ds = self.current_ds\n\n            elif action == \"to_dataframe\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before converting to a DataFrame.\"\n                    )\n                self.to_dataframe()\n                result.dataframe = self.current_df\n\n            elif action == \"to_csv\":\n                if self.current_df is None:\n                    raise ValueError(\n                        \"Action 'to_csv' requires a DataFrame, but no DataFrame is available. \"\n                        \"Use 'to_dataframe' or upload a CSV before saving.\"\n                    )\n                result.filename = self.to_csv()\n\n            elif action == \"to_nc\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before saving to NetCDF.\"\n                    )\n                result.filename = self.to_nc()\n\n            else:\n                raise ValueError(f\"Unknown action '{action}'\")\n\n        return result\n\n    # ----------------------------\n    # Exploration helpers using cfg.dsinfo\n    # ----------------------------\n    def get_datasets(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of dataset provider names available in configuration.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n        return list(self.cfg.dsinfo.keys())\n\n    def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n        \"\"\"\n        Return the list of variables available for a dataset.\n\n        Parameters\n        ----------\n        dataset : str, optional\n            Dataset name to query. Defaults to `cfg.dataset`.\n\n        Returns\n        -------\n        List[str]\n            List of variable names.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n\n        dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n        if dataset_name is None:\n            raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n        dsinfo = self.cfg.dsinfo.get(dataset_name)\n        if not dsinfo or \"variables\" not in dsinfo:\n            raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n        return list(dsinfo[\"variables\"].keys())\n\n    def get_varinfo(self, var: str) -&gt; dict:\n        \"\"\"\n        Get metadata for a variable from varinfo.\n\n        Parameters\n        ----------\n        var : str\n            Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n        Returns\n        -------\n        dict\n            Metadata dictionary containing cf_name, long_name, units, etc.\n\n        Raises\n        ------\n        ValueError\n            If varinfo is not loaded or variable not found.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            raise ValueError(\"Configuration or varinfo not loaded\")\n\n        if var not in self.cfg.varinfo:\n            raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n        return self.cfg.varinfo[var]\n\n\n    def get_actions(self) -&gt; dict:\n        \"\"\"\n        Return a dictionary of workflow actions with their outputs and descriptions.\n        Supports actions.yaml in mapping style (key: action_name).\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n            raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n        actions_map = getattr(self.cfg, \"actionsinfo\")\n\n        # If 'actions' key exists, fallback to list style\n        if \"actions\" in actions_map:\n            actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                        for a in actions_map[\"actions\"]}\n\n        return actions_map\n    def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n        \"\"\"\n        Fetch climate extreme indices from `cfg.extinfo` that involve the given variables.\n\n        Parameters\n        ----------\n        variables : list[str]\n            Variables to filter indices by (if None, uses `cfg.variables`).\n        require_all : bool\n            If True, return indices that require all provided variables; otherwise\n            return indices if any variable matches.\n\n        Returns\n        -------\n        dict\n            Mapping index_name -&gt; index_definition.\n        \"\"\"\n        cfg = self.cfg\n        variables = variables or cfg.variables \n        if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n            raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n        indices_def = cfg.extinfo.get(\"indices\", {})\n        if not indices_def:\n            return {}\n\n        matched_indices = {}\n        for idx_name, idx_info in indices_def.items():\n            idx_vars = idx_info.get(\"variables\", [])\n            if require_all:\n                if all(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n            else:\n                if any(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n\n        return matched_indices\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.__init__","title":"<code>__init__(self, cfg_name='config', conf_path=None, overrides=None)</code>  <code>special</code>","text":"<p>Initialize the workflow manager and load configuration.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.__init__--parameters","title":"Parameters","text":"<p>cfg_name : str     Name of the Hydra configuration (default: \"config\"). conf_path : str, optional     Optional config path override (not commonly used). overrides : list[str], optional     Hydra overrides to apply to the configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n    \"\"\"\n    Initialize the workflow manager and load configuration.\n\n    Parameters\n    ----------\n    cfg_name : str\n        Name of the Hydra configuration (default: \"config\").\n    conf_path : str, optional\n        Optional config path override (not commonly used).\n    overrides : list[str], optional\n        Hydra overrides to apply to the configuration.\n    \"\"\"\n    self.cfg_name = cfg_name\n    self.conf_path = conf_path\n    self.cfg: Optional[DictConfig] = None\n\n    # Stage datasets\n    self.ds = None\n    self.current_ds = None\n    self.index_ds = None\n    self.impute_ds = None\n    self.bias_corrected_ds = None\n\n    # Stage DataFrames\n    self.raw_df = None\n    self.current_df = None\n    self.index_df = None\n    self.impute_df = None\n    self.bias_corrected_df = None\n    self.df = None  # alias for current_df\n\n    # filenames\n    self.filename = None\n    self.filetype = None\n\n    # Automatically load config on init\n    self.load_config(overrides)\n    self.cfg = self.preprocess_aoi(self.cfg)\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_actions","title":"<code>get_actions(self)</code>","text":"<p>Return a dictionary of workflow actions with their outputs and descriptions. Supports actions.yaml in mapping style (key: action_name).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_actions(self) -&gt; dict:\n    \"\"\"\n    Return a dictionary of workflow actions with their outputs and descriptions.\n    Supports actions.yaml in mapping style (key: action_name).\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n        raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n    actions_map = getattr(self.cfg, \"actionsinfo\")\n\n    # If 'actions' key exists, fallback to list style\n    if \"actions\" in actions_map:\n        actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                    for a in actions_map[\"actions\"]}\n\n    return actions_map\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_datasets","title":"<code>get_datasets(self)</code>","text":"<p>Return the list of dataset provider names available in configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_datasets(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of dataset provider names available in configuration.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n    return list(self.cfg.dsinfo.keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_indices","title":"<code>get_indices(self, variables, require_all=True)</code>","text":"<p>Fetch climate extreme indices from <code>cfg.extinfo</code> that involve the given variables.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_indices--parameters","title":"Parameters","text":"<p>variables : list[str]     Variables to filter indices by (if None, uses <code>cfg.variables</code>). require_all : bool     If True, return indices that require all provided variables; otherwise     return indices if any variable matches.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_indices--returns","title":"Returns","text":"<p>dict     Mapping index_name -&gt; index_definition.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n    \"\"\"\n    Fetch climate extreme indices from `cfg.extinfo` that involve the given variables.\n\n    Parameters\n    ----------\n    variables : list[str]\n        Variables to filter indices by (if None, uses `cfg.variables`).\n    require_all : bool\n        If True, return indices that require all provided variables; otherwise\n        return indices if any variable matches.\n\n    Returns\n    -------\n    dict\n        Mapping index_name -&gt; index_definition.\n    \"\"\"\n    cfg = self.cfg\n    variables = variables or cfg.variables \n    if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n        raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n    indices_def = cfg.extinfo.get(\"indices\", {})\n    if not indices_def:\n        return {}\n\n    matched_indices = {}\n    for idx_name, idx_info in indices_def.items():\n        idx_vars = idx_info.get(\"variables\", [])\n        if require_all:\n            if all(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n        else:\n            if any(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n\n    return matched_indices\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_variables","title":"<code>get_variables(self, dataset=None)</code>","text":"<p>Return the list of variables available for a dataset.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_variables--parameters","title":"Parameters","text":"<p>dataset : str, optional     Dataset name to query. Defaults to <code>cfg.dataset</code>.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_variables--returns","title":"Returns","text":"<p>List[str]     List of variable names.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    Return the list of variables available for a dataset.\n\n    Parameters\n    ----------\n    dataset : str, optional\n        Dataset name to query. Defaults to `cfg.dataset`.\n\n    Returns\n    -------\n    List[str]\n        List of variable names.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n\n    dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n    if dataset_name is None:\n        raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n    dsinfo = self.cfg.dsinfo.get(dataset_name)\n    if not dsinfo or \"variables\" not in dsinfo:\n        raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n    return list(dsinfo[\"variables\"].keys())\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo","title":"<code>get_varinfo(self, var)</code>","text":"<p>Get metadata for a variable from varinfo.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo--parameters","title":"Parameters","text":"<p>var : str     Name of the variable, e.g., 'tas', 'tasmax', 'pr'.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo--returns","title":"Returns","text":"<p>dict     Metadata dictionary containing cf_name, long_name, units, etc.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.get_varinfo--raises","title":"Raises","text":"<p>ValueError     If varinfo is not loaded or variable not found.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_varinfo(self, var: str) -&gt; dict:\n    \"\"\"\n    Get metadata for a variable from varinfo.\n\n    Parameters\n    ----------\n    var : str\n        Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n    Returns\n    -------\n    dict\n        Metadata dictionary containing cf_name, long_name, units, etc.\n\n    Raises\n    ------\n    ValueError\n        If varinfo is not loaded or variable not found.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        raise ValueError(\"Configuration or varinfo not loaded\")\n\n    if var not in self.cfg.varinfo:\n        raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n    return self.cfg.varinfo[var]\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.load_config","title":"<code>load_config(self, overrides=None)</code>","text":"<p>Load and compose the Hydra configuration.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.load_config--parameters","title":"Parameters","text":"<p>overrides : list[str], optional     Hydra overrides to apply when composing the configuration.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.load_config--returns","title":"Returns","text":"<p>DictConfig     Composed Hydra configuration object and stored on <code>self.cfg</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n    \"\"\"\n    Load and compose the Hydra configuration.\n\n    Parameters\n    ----------\n    overrides : list[str], optional\n        Hydra overrides to apply when composing the configuration.\n\n    Returns\n    -------\n    DictConfig\n        Composed Hydra configuration object and stored on `self.cfg`.\n    \"\"\"\n    overrides = overrides or []\n    conf_dir = _ensure_local_conf()\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    if not GlobalHydra.instance().is_initialized():\n        hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        hydra_ctx = None\n\n    if hydra_ctx:\n        with hydra_ctx:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    else:\n        self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    return self.cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.preprocess_aoi","title":"<code>preprocess_aoi(self, cfg)</code>","text":"<p>Process an 'aoi' specification in the configuration.</p> <p>Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon). When a Point is provided, <code>cfg.lat</code> and <code>cfg.lon</code> are set. When a Polygon is provided, <code>cfg.bounds</code> is set and <code>cfg.region</code> is set to \"custom\".</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.preprocess_aoi--parameters","title":"Parameters","text":"<p>cfg : DictConfig     Configuration object with optional <code>aoi</code> entry.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.preprocess_aoi--returns","title":"Returns","text":"<p>DictConfig     The modified configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"\n    Process an 'aoi' specification in the configuration.\n\n    Supports GeoJSON strings or dictionaries for FeatureCollection, Feature,\n    or simple geometry objects (Point/Polygon). When a Point is provided,\n    `cfg.lat` and `cfg.lon` are set. When a Polygon is provided, `cfg.bounds`\n    is set and `cfg.region` is set to \"custom\".\n\n    Parameters\n    ----------\n    cfg : DictConfig\n        Configuration object with optional `aoi` entry.\n\n    Returns\n    -------\n    DictConfig\n        The modified configuration.\n    \"\"\"\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid AOI JSON string\")\n\n    aoi = cfg.aoi\n\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n    if isinstance(geom, Point):\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                 \"lon_min\": minx, \"lon_max\": maxx}}\n        cfg.region = \"custom\"\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n    return cfg\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.run_workflow","title":"<code>run_workflow(self, overrides=None, actions=None, file=None)</code>","text":"<p>Execute a sequence of workflow actions.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.run_workflow--parameters","title":"Parameters","text":"<p>overrides : list[str], optional     Hydra overrides to apply (not all actions will use these). actions : list[str], optional     Ordered list of actions to perform. Supported actions include:     'upload_netcdf', 'upload_csv', 'extract', 'calc_index',     'to_dataframe', 'to_csv', 'to_nc'. file : str, optional     File path used for upload actions when required.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.run_workflow--returns","title":"Returns","text":"<p>WorkflowResult     Named result container with populated fields for dataframe/dataset/filenames.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def run_workflow(self, overrides: Optional[List[str]] = None,\n                 actions: Optional[List[str]] = None,\n                 file: Optional[str] = None) -&gt; WorkflowResult:\n    \"\"\"\n    Execute a sequence of workflow actions.\n\n    Parameters\n    ----------\n    overrides : list[str], optional\n        Hydra overrides to apply (not all actions will use these).\n    actions : list[str], optional\n        Ordered list of actions to perform. Supported actions include:\n        'upload_netcdf', 'upload_csv', 'extract', 'calc_index',\n        'to_dataframe', 'to_csv', 'to_nc'.\n    file : str, optional\n        File path used for upload actions when required.\n\n    Returns\n    -------\n    WorkflowResult\n        Named result container with populated fields for dataframe/dataset/filenames.\n    \"\"\"\n    actions = actions or [\"extract\", \"compute_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n    result = WorkflowResult(cfg=self.cfg)\n    for action in actions:\n\n        if action == \"upload_netcdf\":\n            if file is None:\n                raise ValueError(\n                    \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                    \"but none was provided.\"\n                )\n                # Validate extension\n            valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n            if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                raise ValueError(\n                    f\"Invalid file format for upload_netcdf: '{file}'. \"\n                    f\"Expected one of: {valid_nc_ext}\"\n                )\n            self.upload_netcdf(file)\n            result.dataset = self.current_ds\n\n        elif action == \"upload_csv\":\n            if file is None:\n                raise ValueError(\n                    \"Action 'upload_csv' requires argument 'csv_file', \"\n                    \"but none was provided.\"\n                )\n\n            # Validate CSV extension\n            valid_csv_ext = (\".csv\", \".csv.gz\")\n            if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                raise ValueError(\n                    f\"Invalid file format for upload_csv: '{file}'. \"\n                    f\"Expected one of: {valid_csv_ext}\"\n                )\n\n            self.upload_csv(file)\n            result.dataset = self.current_ds\n\n        elif action == \"extract\":\n            if self.cfg.dataset is None:\n                raise ValueError(\n                    \"Action 'extract' cannot run because no dataset provider is set \"\n                    \"(cfg.dataset is None).\"\n                )\n            self.extract()\n            result.dataset = self.current_ds\n\n        elif action == \"calc_index\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before computing an index.\"\n                )\n            self.calc_index()\n            result.index_ds = self.current_ds\n\n        elif action == \"to_dataframe\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before converting to a DataFrame.\"\n                )\n            self.to_dataframe()\n            result.dataframe = self.current_df\n\n        elif action == \"to_csv\":\n            if self.current_df is None:\n                raise ValueError(\n                    \"Action 'to_csv' requires a DataFrame, but no DataFrame is available. \"\n                    \"Use 'to_dataframe' or upload a CSV before saving.\"\n                )\n            result.filename = self.to_csv()\n\n        elif action == \"to_nc\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before saving to NetCDF.\"\n                )\n            result.filename = self.to_nc()\n\n        else:\n            raise ValueError(f\"Unknown action '{action}'\")\n\n    return result\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_csv","title":"<code>to_csv(self, df=None, filename=None)</code>","text":"<p>Save a DataFrame to CSV.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_csv--parameters","title":"Parameters","text":"<p>df : pd.DataFrame, optional     DataFrame to save. Defaults to <code>self.current_df</code>. filename : str, optional     Output filename. Defaults to <code>self.filename_csv</code>.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_csv--returns","title":"Returns","text":"<p>str     The path of the written CSV file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Save a DataFrame to CSV.\n\n    Parameters\n    ----------\n    df : pd.DataFrame, optional\n        DataFrame to save. Defaults to `self.current_df`.\n    filename : str, optional\n        Output filename. Defaults to `self.filename_csv`.\n\n    Returns\n    -------\n    str\n        The path of the written CSV file.\n    \"\"\"\n    df = df if df is not None else self.current_df\n\n    filename = filename or getattr(self, \"filename_csv\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_csv is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    df.to_csv(filename, index=False)\n    self.filename_csv = str(path)\n    self.current_filename = str(path)\n\n    print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n    return filename\n</code></pre>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_nc","title":"<code>to_nc(self, ds=None, filename=None)</code>","text":"<p>Save an xarray Dataset to NetCDF. - If ds is None: save current_ds. - If filename is None: use self.filename_nc. - Creates directories if needed. - Updates self.filename_nc and self.current_filename.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_nc--parameters","title":"Parameters","text":"<p>ds : xr.Dataset, optional     Dataset to save. If None, uses <code>self.current_ds</code>. filename : str, optional     Output filename. Defaults to <code>self.filename_nc</code>.</p>"},{"location":"api/#climdata.utils.wrapper_workflow.ClimData.to_nc--returns","title":"Returns","text":"<p>str     The path of the written NetCDF file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Save an xarray Dataset to NetCDF.\n    - If ds is None: save current_ds.\n    - If filename is None: use self.filename_nc.\n    - Creates directories if needed.\n    - Updates self.filename_nc and self.current_filename.\n\n    Parameters\n    ----------\n    ds : xr.Dataset, optional\n        Dataset to save. If None, uses `self.current_ds`.\n    filename : str, optional\n        Output filename. Defaults to `self.filename_nc`.\n\n    Returns\n    -------\n    str\n        The path of the written NetCDF file.\n    \"\"\"\n\n    # -------------------------------\n    # 1. Determine dataset to save\n    # -------------------------------\n    ds = ds or getattr(self, \"current_ds\", None)\n    if ds is None:\n        raise ValueError(\"No dataset available to save\")\n\n    # -------------------------------\n    # 2. Determine filename\n    # -------------------------------\n    filename = filename or getattr(self, \"filename_nc\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_nc is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------\n    # 3. Save to NetCDF\n    # -------------------------------\n    ds.to_netcdf(path)\n\n    # -------------------------------\n    # 4. Track filenames\n    # -------------------------------\n    self.filename_nc = str(path)\n    self.current_filename = str(path)\n\n    print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n    return str(path)\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"climdata/","title":"climdata module","text":""},{"location":"climdata/#climdata","title":"<code>climdata</code>  <code>special</code>","text":"<p>Top-level package for climdata.</p>"},{"location":"climdata/#climdata.utils","title":"<code>utils</code>  <code>special</code>","text":""},{"location":"climdata/#climdata.utils.config","title":"<code>config</code>","text":""},{"location":"climdata/#climdata.utils.config.load_config","title":"<code>load_config(config_name='config', overrides=None, verbose=False)</code>","text":"<p>Load Hydra config using ./conf in cwd.</p> Source code in <code>climdata/utils/config.py</code> <pre><code>def load_config(config_name=\"config\", overrides=None, verbose=False):\n    \"\"\"\n    Load Hydra config using ./conf in cwd.\n    \"\"\"\n    config_path = _ensure_local_conf()\n    print(config_path+config_name)\n    with initialize(config_path=config_path, version_base=None):\n        cfg = compose(config_name=config_name, overrides=overrides or [])\n        if verbose:\n            print(OmegaConf.to_yaml(cfg))\n        return cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download","title":"<code>utils_download</code>","text":""},{"location":"climdata/#climdata.utils.utils_download.download_drive_file","title":"<code>download_drive_file(file_id, local_path, service)</code>","text":"<p>Download a single file from Drive to a local path.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def download_drive_file(file_id, local_path, service):\n    \"\"\"\n    Download a single file from Drive to a local path.\n    \"\"\"\n    request = service.files().get_media(fileId=file_id)\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n\n    with io.FileIO(local_path, 'wb') as fh:\n        downloader = MediaIoBaseDownload(fh, request)\n\n        done = False\n        while not done:\n            status, done = downloader.next_chunk()\n            print(f\"   \u2192 Download {int(status.progress() * 100)}% complete\")\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.fetch_dwd","title":"<code>fetch_dwd(var_cfg, var)</code>","text":"<p>Download HYRAS data for one variable and a list of years.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def fetch_dwd(var_cfg,var):\n    \"\"\"Download HYRAS data for one variable and a list of years.\"\"\"\n    param_mapping = var_cfg.dsinfo\n    provider = var_cfg.dataset.lower()\n    parameter_key = var\n    # Validate provider and parameter\n\n    param_info = param_mapping[provider]['variables'][parameter_key]\n    base_url = param_info[\"base_url\"]\n    prefix = param_info[\"prefix\"]\n    version = param_info[\"version\"]\n\n    start_date = var_cfg.time_range.start_date\n    end_date = var_cfg.time_range.end_date\n\n    # Parse dates &amp; extract unique years\n    start_year = datetime.fromisoformat(start_date).year\n    end_year = datetime.fromisoformat(end_date).year\n    years = list(range(start_year, end_year + 1))\n\n    # output_file = cfg.output.filename\n    os.makedirs(parameter_key, exist_ok=True)\n\n    for year in years:\n        file_name = f\"{prefix}_{year}_{version}_de.nc\"\n        file_url = f\"{base_url}{file_name}\"\n        local_path = os.path.join(var_cfg.data_dir,provider,parameter_key.upper(), file_name)\n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        print(f\"\u2b07\ufe0f  Checking: {file_url}\")\n\n        if os.path.exists(local_path):\n            print(f\"\u2714\ufe0f  Exists locally: {local_path}\")\n            continue\n\n        # Check if file exists on server first (HEAD request)\n        head = requests.head(file_url)\n        if head.status_code != 200:\n            raise FileNotFoundError(f\"\u274c Not found on server: {file_url} (HTTP {head.status_code})\")\n\n        print(f\"\u2b07\ufe0f  Downloading: {file_url}\")\n        try:\n            response = requests.get(file_url, stream=True)\n            response.raise_for_status()\n            with open(local_path, \"wb\") as f:\n                for chunk in response.iter_content(chunk_size=8192):\n                    f.write(chunk)\n            print(f\"\u2705 Saved: {local_path}\")\n        except requests.HTTPError as e:\n            raise RuntimeError(f\"\u274c Failed download: {file_url} \u2014 {e}\")\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.find_nearest_xy","title":"<code>find_nearest_xy(ds, target_lat, target_lon)</code>","text":"<p>Given a dataset with curvilinear grid, find the nearest x,y index.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def find_nearest_xy(ds, target_lat, target_lon):\n    \"\"\"\n    Given a dataset with curvilinear grid, find the nearest x,y index.\n    \"\"\"\n    lat = ds['lat'].values  # shape (y,x) or (x,y)\n    lon = ds['lon'].values\n\n    # Flatten to 1D for k-d tree\n    lat_flat = lat.flatten()\n    lon_flat = lon.flatten()\n\n    tree = cKDTree(np.column_stack((lat_flat, lon_flat)))\n    _, idx = tree.query([target_lat, target_lon])\n    iy, ix = np.unravel_index(idx, lat.shape)\n\n    return iy, ix\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.get_output_filename","title":"<code>get_output_filename(cfg, output_type='nc', lat=None, lon=None, shp_name=None, param='surface')</code>","text":"<p>Generate output filename based on config, output type, and extraction mode. output_type: \"nc\", \"csv\", or \"zarr\"</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def get_output_filename(cfg, output_type=\"nc\", lat=None, lon=None, shp_name = None, param=\"surface\"):\n    \"\"\"\n    Generate output filename based on config, output type, and extraction mode.\n    output_type: \"nc\", \"csv\", or \"zarr\"\n    \"\"\"\n    if output_type == \"csv\":\n        template = cfg.output.filename_csv\n    elif output_type == \"zarr\":\n        template = cfg.output.filename_zarr\n    else:\n        template = cfg.output.filename_nc\n\n    # If lat/lon are provided, use point template\n    if lat is not None and lon is not None:\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat=f\"{lat}\",\n            lon=f\"{lon}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    elif shp_name is not None:\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat_range=f\"{shp_name}\",\n            lon_range=f\"{shp_name}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    else:\n        # Use region bounds\n        region_bounds = cfg.bounds[cfg.region]\n        filename = template.format(\n            provider=cfg.dataset,\n            parameter=param,\n            lat_range=f\"{region_bounds['lat_min']}-{region_bounds['lat_max']}\",\n            lon_range=f\"{region_bounds['lon_min']}-{region_bounds['lon_max']}\",\n            start=cfg.time_range.start_date.replace(\"-\", \"\"),\n            end=cfg.time_range.end_date.replace(\"-\", \"\"),\n        )\n    return filename\n</code></pre>"},{"location":"climdata/#climdata.utils.utils_download.list_drive_files","title":"<code>list_drive_files(folder_id, service)</code>","text":"<p>List all files in a Google Drive folder, handling pagination.</p> Source code in <code>climdata/utils/utils_download.py</code> <pre><code>def list_drive_files(folder_id, service):\n    \"\"\"\n    List all files in a Google Drive folder, handling pagination.\n    \"\"\"\n    files = []\n    page_token = None\n\n    while True:\n        results = service.files().list(\n            q=f\"'{folder_id}' in parents and trashed = false\",\n            fields=\"files(id, name), nextPageToken\",\n            pageToken=page_token\n        ).execute()\n\n        files.extend(results.get(\"files\", []))\n        page_token = results.get(\"nextPageToken\", None)\n\n        if not page_token:\n            break\n\n    return files\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper","title":"<code>wrapper</code>","text":""},{"location":"climdata/#climdata.utils.wrapper.extract_index","title":"<code>extract_index(csv_path, cfg_name='config', extra_overrides=None, save_to_file=True, output_path='output_index.csv')</code>","text":"<p>Load climate CSV \u2192 generate dataset + time_range overrides automatically \u2192 convert to xarray \u2192 compute extreme index.</p> Source code in <code>climdata/utils/wrapper.py</code> <pre><code>def extract_index(\n    csv_path: str,\n    cfg_name: str = \"config\",\n    extra_overrides: list = None,       # Only additional overrides (e.g., \"index=tx90p\")\n    save_to_file: bool = True,\n    output_path: str = \"output_index.csv\",\n):\n    \"\"\"\n    Load climate CSV \u2192 generate dataset + time_range overrides automatically \u2192\n    convert to xarray \u2192 compute extreme index.\n    \"\"\"\n\n    # ---- Load CSV ----\n    df = pd.read_csv(csv_path, parse_dates=[\"time\"])\n\n    # ---- Always extract dataset + time_range from CSV ----\n    dataset = df[\"source\"].unique()[0]\n    time_range_start = df[\"time\"].min().strftime(\"%Y-%m-%d\")\n    time_range_end   = df[\"time\"].max().strftime(\"%Y-%m-%d\")\n\n    auto_overrides = [\n        f\"dataset={dataset}\",\n        f\"time_range.start_date={time_range_start}\",\n        f\"time_range.end_date={time_range_end}\",\n    ]\n\n    # ---- Merge with any user-provided overrides ----\n    # (e.g., \"index=tn10p\")\n    if extra_overrides:\n        overrides = auto_overrides + extra_overrides\n    else:\n        overrides = auto_overrides + [\"index=tn10p\"]  # default index\n\n    print(\"Using Hydra overrides:\", overrides)\n\n    # 1. Ensure local configs are available\n    conf_dir = _ensure_local_conf()  # copies conf/ to cwd\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    # 2. Initialize Hydra only if not already initialized\n    if not GlobalHydra.instance().is_initialized():\n        hydra_context = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        # If already initialized, just set context to None for clarity\n        hydra_context = None\n\n    # Use compose within context manager if newly initialized\n    if hydra_context is not None:\n        with hydra_context:\n            cfg: DictConfig = compose(config_name=cfg_name, overrides=overrides)\n    else:\n        # Already initialized: compose directly\n        cfg: DictConfig = compose(config_name=cfg_name, overrides=overrides)\n    # ---- Convert CSV \u2192 xarray ----\n    df_pivot = df.pivot_table(\n        index=[\"time\", \"lat\", \"lon\"],\n        columns=\"variable\",\n        values=\"value\"\n    ).reset_index()\n\n    ds = df_pivot.set_index([\"time\", \"lat\", \"lon\"]).to_xarray()\n\n    # ---- Attach units ----\n    for var in df[\"variable\"].unique():\n        units = df[df[\"variable\"] == var][\"units\"].iloc[0]\n        ds[var].attrs[\"units\"] = units\n\n    print(ds)\n\n    # ---- Compute extreme index ----\n    indices = climdata.extreme_index(cfg, ds)\n    print(f\"Calculating index: {cfg.index}\")\n\n    result = indices.calculate(cfg.index).compute()\n\n    # ---- Save result ----\n    if save_to_file:\n        result.to_dataframe().reset_index().to_csv(output_path)\n        print(f\"Saved result \u2192 {output_path}\")\n\n    return result\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper.preprocess_aoi","title":"<code>preprocess_aoi(cfg)</code>","text":"<p>Normalize AOI in cfg: - Converts string AOI \u2192 dict - Extracts shapely geometry from FeatureCollection, Feature, or raw geometry - Detects type: point, bbox, or polygon - Updates cfg with:     cfg.aoi_type: \"point\" | \"bbox\" | \"polygon\"     cfg.lat, cfg.lon: for point     cfg.bounds: [minx, miny, maxx, maxy] for bbox/polygon     cfg.geometry: shapely geometry object</p> Source code in <code>climdata/utils/wrapper.py</code> <pre><code>def preprocess_aoi(cfg):\n    \"\"\"\n    Normalize AOI in cfg:\n    - Converts string AOI \u2192 dict\n    - Extracts shapely geometry from FeatureCollection, Feature, or raw geometry\n    - Detects type: point, bbox, or polygon\n    - Updates cfg with:\n        cfg.aoi_type: \"point\" | \"bbox\" | \"polygon\"\n        cfg.lat, cfg.lon: for point\n        cfg.bounds: [minx, miny, maxx, maxy] for bbox/polygon\n        cfg.geometry: shapely geometry object\n    \"\"\"\n    # -----------------------------\n    # 1) Load AOI value from string\n    # -----------------------------\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(f\"AOI string is not valid JSON: {cfg.aoi}\")\n\n    aoi = cfg.aoi\n\n    # -----------------------------\n    # 2) Extract shapely geometry\n    # -----------------------------\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        if not aoi.get(\"features\"):\n            raise ValueError(\"FeatureCollection contains no features\")\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi and \"coordinates\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n    # -----------------------------\n    # 3) Determine AOI type\n    # -----------------------------\n    if isinstance(geom, Point):\n        # cfg.aoi_type = \"point\"\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        # Check if axis-aligned bbox\n        coords = list(geom.exterior.coords)\n        is_bbox = len(coords) == 5 and len({c[0] for c in coords}) == 2 and len({c[1] for c in coords}) == 2\n\n        # if is_bbox:\n        #     cfg.aoi_type = \"bbox\"\n        # else:\n        #     cfg.aoi_type = \"polygon\"\n\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds['custom'] = {\n                \"lat_min\": miny,\n                \"lat_max\": maxy,\n                \"lon_min\": minx,\n                \"lon_max\": maxx\n            }\n        cfg.region='custom'\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unsupported geometry type: {geom.geom_type}\")\n\n    # -----------------------------\n    # 4) Store geometry itself\n    # -----------------------------\n    # cfg.shapefile = geom\n\n    return cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow","title":"<code>wrapper_workflow</code>","text":""},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor","title":"<code> ClimateExtractor        </code>","text":"<p>Climate data extraction and extreme-index workflow manager.</p> <p>This class provides a high-level API for:   - loading/configuring dataset providers via Hydra config,   - uploading NetCDF/CSV content into xarray Datasets,   - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER),   - computing extreme indices using configured xclim indices,   - converting datasets to long-form DataFrames and saving results.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor--attributes","title":"Attributes","text":"<p>cfg : DictConfig     Hydra configuration object describing dataset, region/time/variables, outputs. current_ds : xr.Dataset     The most recently loaded or extracted dataset. current_df : pd.DataFrame     The most recently produced long-form DataFrame. filename_csv / filename_nc / filename_zarr : str     Generated output filename templates/paths.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor--example","title":"Example","text":"<p>extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe']) extractor.extract() idx_ds = extractor.calc_index() df = extractor.to_dataframe(idx_ds) extractor.to_csv(df)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>class ClimateExtractor:\n    \"\"\"\n    Climate data extraction and extreme-index workflow manager.\n\n    This class provides a high-level API for:\n      - loading/configuring dataset providers via Hydra config,\n      - uploading NetCDF/CSV content into xarray Datasets,\n      - extracting data from supported providers (CMIP, DWD, MSWX, HYRAS, POWER),\n      - computing extreme indices using configured xclim indices,\n      - converting datasets to long-form DataFrames and saving results.\n\n    Attributes\n    ----------\n    cfg : DictConfig\n        Hydra configuration object describing dataset, region/time/variables, outputs.\n    current_ds : xr.Dataset\n        The most recently loaded or extracted dataset.\n    current_df : pd.DataFrame\n        The most recently produced long-form DataFrame.\n    filename_csv / filename_nc / filename_zarr : str\n        Generated output filename templates/paths.\n\n    Example\n    -------\n    extractor = ClimateExtractor(overrides=['dataset=cmip', 'region=europe'])\n    extractor.extract()\n    idx_ds = extractor.calc_index()\n    df = extractor.to_dataframe(idx_ds)\n    extractor.to_csv(df)\n    \"\"\"\n\n    def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n        \"\"\"\n        Initialize the workflow manager and load configuration.\n\n        Parameters\n        ----------\n        cfg_name : str\n            Name of the Hydra configuration (default: \"config\").\n        conf_path : str, optional\n            Optional config path override (not commonly used).\n        overrides : list[str], optional\n            Hydra overrides to apply to the configuration.\n        \"\"\"\n        self.cfg_name = cfg_name\n        self.conf_path = conf_path\n        self.cfg: Optional[DictConfig] = None\n\n        # Stage datasets\n        self.ds = None\n        self.current_ds = None\n        self.index_ds = None\n        self.impute_ds = None\n        self.bias_corrected_ds = None\n\n        # Stage DataFrames\n        self.raw_df = None\n        self.current_df = None\n        self.index_df = None\n        self.impute_df = None\n        self.bias_corrected_df = None\n        self.df = None  # alias for current_df\n\n        # filenames\n        self.filename = None\n        self.filetype = None\n\n        # Automatically load config on init\n        self.load_config(overrides)\n        self.cfg = self.preprocess_aoi(self.cfg)\n    def _gen_fn(self, ds: xr.Dataset):\n        \"\"\"\n        Create filenames (csv, nc, zarr) using config templates and dataset metadata.\n        Automatically handles coordinate aliases such as lat/latitude, lon/longitude,\n        time/date.\n        \"\"\"\n\n        # ------------------------\n        # Helper: find coord alias\n        # ------------------------\n        def find_coord(ds, names):\n            \"\"\"Return ds coordinate if any alias in names exists.\"\"\"\n            for name in names:\n                if name in ds.coords:\n                    return ds[name]\n            return None\n\n        # ------------------------\n        # Extract coordinates\n        # ------------------------\n        lat = find_coord(ds, [\"lat\", \"latitude\"])\n        lon = find_coord(ds, [\"lon\", \"longitude\"])\n        time = find_coord(ds, [\"time\", \"date\"])\n\n        # ------------------------\n        # Provider\n        # ------------------------\n        provider = ds.attrs.get(\"source\", \"unknown\")\n\n        # ------------------------\n        # Parameter(s)\n        # ------------------------\n        vars_list = list(ds.data_vars)\n        parameter = vars_list[0] if len(vars_list) == 1 else \"_\".join(vars_list)\n\n        # ------------------------\n        # Latitude range\n        # ------------------------\n        if lat is None:\n            lat_str = \"unknown\"\n            lat_range = \"unknown\"\n        else:\n            # Flatten in case of 2D lat/lon grid\n            lat_vals = lat.values.reshape(-1)\n            lat_min = float(lat_vals.min())\n            lat_max = float(lat_vals.max())\n\n            if lat_min == lat_max:\n                lat_str = f\"{lat_min}\"\n                lat_range = f\"{lat_min}\"\n            else:\n                lat_str = f\"{lat_min}_{lat_max}\"\n                lat_range = f\"{lat_min}-{lat_max}\"\n\n        # ------------------------\n        # Longitude range\n        # ------------------------\n        if lon is None:\n            lon_str = \"unknown\"\n            lon_range = \"unknown\"\n        else:\n            lon_vals = lon.values.reshape(-1)\n            lon_min = float(lon_vals.min())\n            lon_max = float(lon_vals.max())\n\n            if lon_min == lon_max:\n                lon_str = f\"{lon_min}\"\n                lon_range = f\"{lon_min}\"\n            else:\n                lon_str = f\"{lon_min}_{lon_max}\"\n                lon_range = f\"{lon_min}-{lon_max}\"\n\n        # ------------------------\n        # Time range\n        # ------------------------\n        if time is None:\n            start = end = \"unknown\"\n        else:\n            tvals = pd.to_datetime(time.values)\n            start = tvals.min().strftime(\"%Y-%m-%d\")\n            end = tvals.max().strftime(\"%Y-%m-%d\")\n\n        # ------------------------\n        # Build filenames\n        # ------------------------\n        outdir = Path(self.cfg.output.out_dir)\n\n        def build(fn_template):\n            return fn_template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range,\n                lon_range=lon_range,\n            )\n\n        self.filename_csv = str(outdir / build(self.cfg.output.filename_csv))\n        self.filename_nc = str(outdir / build(self.cfg.output.filename_nc))\n        self.filename_zarr = str(outdir / build(self.cfg.output.filename_zarr))\n\n\n    def _gen_fn_cfg(self):\n        \"\"\"\n        Generate output filenames using ONLY cfg and extracted ds,\n        without relying on uploaded dataset metadata.\n        \"\"\"\n\n        cfg = self.cfg\n        out = cfg.output\n        provider = cfg.dataset.lower()\n        if self.current_ds:\n            if len(self.current_ds.data_vars) == 0:\n                parameter = \"unknown\"\n            elif len(self.current_ds.data_vars) == 1:\n                parameter = next(iter(self.current_ds.data_vars))\n            else:\n                parameter = \"_\".join(self.current_ds.data_vars)\n        else:\n            parameter = \"_\".join(self.cfg.variables)\n        # --------------------------------\n        # Determine lat/lon values\n        # --------------------------------\n        if cfg.lat is not None and cfg.lon is not None:\n            lat_range = lon_range = None   # single point\n            lat_str = str(cfg.lat)\n            lon_str = str(cfg.lon)\n        else:\n            b = cfg.bounds[cfg.region]\n            lat_min, lat_max = b[\"lat_min\"], b[\"lat_max\"]\n            lon_min, lon_max = b[\"lon_min\"], b[\"lon_max\"]\n\n            lat_str = f\"{lat_min}_{lat_max}\"\n            lon_str = f\"{lon_min}_{lon_max}\"\n            lat_range = f\"{lat_min}-{lat_max}\"\n            lon_range = f\"{lon_min}-{lon_max}\"\n\n        # --------------------------------\n        # Time range from cfg\n        # --------------------------------\n        start = pd.to_datetime(cfg.time_range.start_date).strftime(\"%Y-%m-%d\")\n        end = pd.to_datetime(cfg.time_range.end_date).strftime(\"%Y-%m-%d\")\n\n        # --------------------------------\n        # Format filenames\n        # --------------------------------\n        def format_template(template):\n            return template.format(\n                provider=provider,\n                parameter=parameter,\n                lat=lat_str,\n                lon=lon_str,\n                start=start,\n                end=end,\n                lat_range=lat_range or lat_str,\n                lon_range=lon_range or lon_str,\n            )\n\n        out_dir = Path('./')\n        out_dir.mkdir(parents=True, exist_ok=True)\n\n        self.filename_csv = str(out_dir / format_template(out.filename_csv))\n        self.filename_nc = str(out_dir / format_template(out.filename_nc))\n        self.filename_zarr = str(out_dir / format_template(out.filename_zarr))\n\n    # ----------------------------\n    # Hydra config\n    # ----------------------------\n    def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n        \"\"\"\n        Load and compose the Hydra configuration.\n\n        Parameters\n        ----------\n        overrides : list[str], optional\n            Hydra overrides to apply when composing the configuration.\n\n        Returns\n        -------\n        DictConfig\n            Composed Hydra configuration object and stored on `self.cfg`.\n        \"\"\"\n        overrides = overrides or []\n        conf_dir = _ensure_local_conf()\n        rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n        if not GlobalHydra.instance().is_initialized():\n            hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n        else:\n            hydra_ctx = None\n\n        if hydra_ctx:\n            with hydra_ctx:\n                self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        else:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n        return self.cfg\n\n    # ----------------------------\n    # AOI preprocessing\n    # ----------------------------\n    def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n        \"\"\"\n        Process an 'aoi' specification in the configuration.\n\n        Supports GeoJSON strings or dictionaries for FeatureCollection, Feature,\n        or simple geometry objects (Point/Polygon). When a Point is provided,\n        `cfg.lat` and `cfg.lon` are set. When a Polygon is provided, `cfg.bounds`\n        is set and `cfg.region` is set to \"custom\".\n\n        Parameters\n        ----------\n        cfg : DictConfig\n            Configuration object with optional `aoi` entry.\n\n        Returns\n        -------\n        DictConfig\n            The modified configuration.\n        \"\"\"\n        if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n            return cfg\n\n        if isinstance(cfg.aoi, str):\n            try:\n                cfg.aoi = json.loads(cfg.aoi)\n            except json.JSONDecodeError:\n                raise ValueError(\"Invalid AOI JSON string\")\n\n        aoi = cfg.aoi\n\n        if aoi.get(\"type\") == \"FeatureCollection\":\n            geom = shape(aoi[\"features\"][0][\"geometry\"])\n        elif aoi.get(\"type\") == \"Feature\":\n            geom = shape(aoi[\"geometry\"])\n        elif \"type\" in aoi:\n            geom = shape(aoi)\n        else:\n            raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n        if isinstance(geom, Point):\n            cfg.lat = geom.y\n            cfg.lon = geom.x\n            cfg.bounds = None\n        elif isinstance(geom, Polygon):\n            minx, miny, maxx, maxy = geom.bounds\n            cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                     \"lon_min\": minx, \"lon_max\": maxx}}\n            cfg.region = \"custom\"\n            cfg.lat = None\n            cfg.lon = None\n        else:\n            raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n        return cfg\n\n    # ----------------------------\n    # Upload NetCDF\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_netcdf(self, nc_file: str) -&gt; xr.Dataset:\n        \"\"\"\n        Load a NetCDF file into an xarray.Dataset and update file metadata.\n\n        Parameters\n        ----------\n        nc_file : str\n            Path to the NetCDF file to open.\n\n        Returns\n        -------\n        xr.Dataset\n            The loaded dataset (also sets `self.current_ds`).\n        \"\"\"\n        if not os.path.exists(nc_file):\n            raise FileNotFoundError(f\"{nc_file} does not exist\")\n\n        ds = xr.open_dataset(nc_file)\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")}\n                                for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Upload CSV \u2192 xarray.Dataset\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def upload_csv(self, csv_file: str) -&gt; xr.Dataset:\n        \"\"\"\n        Load a long-form CSV into an xarray.Dataset.\n\n        CSV must contain `time` and `lat`/`latitude`, `lon`/`longitude`, `variable`, `value`.\n        Units may be supplied in a `units` column and an optional `source` column is recognised.\n\n        Parameters\n        ----------\n        csv_file : str\n            Path to the CSV file to load.\n\n        Returns\n        -------\n        xr.Dataset\n            The converted dataset (also sets `self.current_ds`).\n        \"\"\"\n        if not os.path.exists(csv_file):\n            raise FileNotFoundError(f\"{csv_file} does not exist\")\n\n        df = pd.read_csv(csv_file, parse_dates=[\"time\"])\n\n        lat_col = next((c for c in [\"lat\", \"latitude\"] if c in df.columns), None)\n        lon_col = next((c for c in [\"lon\", \"longitude\"] if c in df.columns), None)\n        if lat_col is None or lon_col is None:\n            raise ValueError(\"CSV must have 'lat'/'latitude' and 'lon'/'longitude' columns\")\n\n        id_vars = [\"time\", lat_col, lon_col]\n        df_wide = df.pivot_table(index=id_vars, columns=\"variable\", values=\"value\").reset_index()\n        ds = df_wide.set_index(id_vars).to_xarray()\n\n        # Attach units from CSV\n        for var in ds.data_vars:\n            units_series = df[df[\"variable\"] == var][\"units\"]\n            ds[var].attrs[\"units\"] = units_series.iloc[0] if not units_series.empty else \"unknown\"\n\n        # Global source attribute\n        if \"source\" in df.columns:\n            source_series = df[\"source\"].dropna().unique()\n            if len(source_series) &gt; 0:\n                ds.attrs[\"source\"] = source_series[0]\n\n        # Update cfg variables &amp; varinfo\n        if not hasattr(self.cfg, \"variables\") or not self.cfg.variables:\n            self.cfg.variables = list(ds.data_vars)\n        if not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            self.cfg.varinfo = {v: {\"units\": ds[v].attrs.get(\"units\", \"unknown\")} for v in ds.data_vars}\n        self._gen_fn(ds)\n        return ds\n\n    # ----------------------------\n    # Extract data from datasets like CMIP, DWD, etc.\n    # ----------------------------\n    @update_ds(attr_name='ds')\n    def extract(self) -&gt; xr.Dataset:\n        \"\"\"\n        Extract data from the configured provider using `self.cfg`.\n\n        Uses provider-specific classes (e.g., `CMIP`, `DWD`, `MSWX`, `HYRAS`, `POWER`)\n        to fetch, load and extract datasets. When extraction completes, units are\n        converted to those declared in `cfg.varinfo`, the dataset is computed,\n        and filenames are generated from the configuration.\n\n        Returns\n        -------\n        xr.Dataset\n            The extracted and computed dataset (also sets `self.current_ds`).\n        \"\"\"\n        cfg = self.cfg\n        extract_kwargs = {}\n\n        if cfg.lat is not None and cfg.lon is not None:\n            extract_kwargs[\"point\"] = (cfg.lon, cfg.lat)\n            if cfg.dataset == \"dwd\":\n                extract_kwargs[\"buffer_km\"] = 30\n        elif cfg.region is not None:\n            extract_kwargs[\"box\"] = cfg.bounds[cfg.region]\n        elif cfg.shapefile is not None:\n            extract_kwargs[\"shapefile\"] = cfg.shapefile\n\n        ds = None\n        dataset_upper = cfg.dataset.upper()\n\n        if dataset_upper == \"MSWX\":\n            ds_vars = []\n            for var in cfg.variables:\n                mswx = climdata.MSWX(cfg)\n                mswx.extract(**extract_kwargs)\n                mswx.load(var)\n                ds_vars.append(mswx.dataset)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"CMIP\":\n            cmip = climdata.CMIP(cfg)\n            cmip.fetch()\n            cmip.load()\n            cmip.extract(**extract_kwargs)\n            ds = cmip.ds\n\n        elif dataset_upper == \"POWER\":\n            power = climdata.POWER(cfg)\n            power.fetch()\n            power.load()\n            ds = power.ds\n\n        elif dataset_upper == \"DWD\":\n            ds_vars = []\n            for var in cfg.variables:\n                dwd = climdata.DWD(cfg)\n                ds_var = dwd.extract(variable=var, **extract_kwargs)\n                ds_vars.append(ds_var)\n            ds = xr.merge(ds_vars)\n\n        elif dataset_upper == \"HYRAS\":\n            hyras = climdata.HYRAS(cfg)\n            ds_vars = []\n            for var in cfg.variables:\n                hyras.extract(**extract_kwargs)\n                ds_vars.append(hyras.load(var)[[var]])\n            ds = xr.merge(ds_vars, compat=\"override\")\n\n        for var in ds.data_vars:\n            ds[var] = xclim.core.units.convert_units_to(ds[var], cfg.varinfo[var].units)\n\n        ds = ds.compute()\n\n        self._gen_fn_cfg()\n\n        return ds\n    # ----------------------------\n    # Compute extreme index\n    # ----------------------------\n    @update_ds(attr_name='index_ds')\n    def calc_index(self, ds: xr.Dataset = None) -&gt; xr.Dataset:\n        \"\"\"\n        Calculate the configured extreme index using xclim indices.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to operate on. If None, `self.current_ds` is used.\n\n        Returns\n        -------\n        xr.Dataset\n            The computed index as an xarray Dataset (also sets `self.index_ds`).\n        \"\"\"\n        cfg = self.cfg\n\n        # Use provided ds or fallback\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        if cfg.index is None:\n            print(\"No index selected.\")\n            return None\n\n        if \"time\" in ds.coords:\n            years = pd.to_datetime(ds.time.values).year\n            n_years = len(pd.unique(years))\n            if n_years &lt; 30:\n                warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n\n        indices = extreme_index(cfg, ds)\n        index_ds = indices.calculate(cfg.index).compute()\n        index_ds = index_ds.to_dataset(name=cfg.index)\n\n        self._gen_fn_cfg()\n\n        return index_ds\n    # ----------------------------\n    # Dataset \u2192 Long-form DataFrame\n    # ----------------------------\n    @update_df()\n    def to_dataframe(self, ds: xr.Dataset = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert a dataset to a long-form pandas DataFrame.\n\n        The output contains columns: time, lat, lon (or latitude/longitude),\n        variable, value, units, source.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to convert. If None, uses `self.current_ds`.\n\n        Returns\n        -------\n        pd.DataFrame\n            Long-form DataFrame (also sets `self.current_df`).\n        \"\"\"\n        ds = ds or self.current_ds\n        if ds is None:\n            raise ValueError(\"No dataset provided and no current_ds is available.\")\n\n        df = ds.to_dataframe().reset_index()\n\n        id_vars = [c for c in (\"time\", \"lat\", \"lon\", \"latitude\", \"longitude\") if c in df]\n        value_vars = [v for v in ds.data_vars if v in df.columns]\n\n        if not value_vars:\n            raise ValueError(\"No variables in dataset available to melt into long format\")\n\n        df_long = df.melt(\n            id_vars=id_vars,\n            value_vars=value_vars,\n            var_name=\"variable\",\n            value_name=\"value\"\n        )\n\n        df_long[\"units\"] = df_long[\"variable\"].apply(\n            lambda v: ds[v].attrs.get(\"units\", \"unknown\")\n        )\n        df_long[\"source\"] = getattr(self.cfg, \"dataset\", ds.attrs.get(\"source\", \"unknown\"))\n        self._gen_fn_cfg()\n        return df_long\n\n    # ----------------------------\n    # Save CSV\n    # ----------------------------\n    def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Save a DataFrame to CSV.\n\n        Parameters\n        ----------\n        df : pd.DataFrame, optional\n            DataFrame to save. Defaults to `self.current_df`.\n        filename : str, optional\n            Output filename. Defaults to `self.filename_csv`.\n\n        Returns\n        -------\n        str\n            The path of the written CSV file.\n        \"\"\"\n        df = df if df is not None else self.current_df\n\n        filename = filename or getattr(self, \"filename_csv\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_csv is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        df.to_csv(filename, index=False)\n        self.filename_csv = str(path)\n        self.current_filename = str(path)\n\n        print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n        return filename\n\n    def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n        \"\"\"\n        Save an xarray Dataset to NetCDF.\n        - If ds is None: save current_ds.\n        - If filename is None: use self.filename_nc.\n        - Creates directories if needed.\n        - Updates self.filename_nc and self.current_filename.\n\n        Parameters\n        ----------\n        ds : xr.Dataset, optional\n            Dataset to save. If None, uses `self.current_ds`.\n        filename : str, optional\n            Output filename. Defaults to `self.filename_nc`.\n\n        Returns\n        -------\n        str\n            The path of the written NetCDF file.\n        \"\"\"\n\n        # -------------------------------\n        # 1. Determine dataset to save\n        # -------------------------------\n        ds = ds or getattr(self, \"current_ds\", None)\n        if ds is None:\n            raise ValueError(\"No dataset available to save\")\n\n        # -------------------------------\n        # 2. Determine filename\n        # -------------------------------\n        filename = filename or getattr(self, \"filename_nc\", None)\n        if filename is None:\n            raise ValueError(\"No filename provided and filename_nc is not set\")\n\n        path = Path(filename)\n        path.parent.mkdir(parents=True, exist_ok=True)\n\n        # -------------------------------\n        # 3. Save to NetCDF\n        # -------------------------------\n        ds.to_netcdf(path)\n\n        # -------------------------------\n        # 4. Track filenames\n        # -------------------------------\n        self.filename_nc = str(path)\n        self.current_filename = str(path)\n\n        print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n        return str(path)\n\n    # ----------------------------\n    # Unified workflow\n    # ----------------------------\n    def run_workflow(self, overrides: Optional[List[str]] = None,\n                     actions: Optional[List[str]] = None,\n                     file: Optional[str] = None) -&gt; WorkflowResult:\n        \"\"\"\n        Execute a sequence of workflow actions.\n\n        Parameters\n        ----------\n        overrides : list[str], optional\n            Hydra overrides to apply (not all actions will use these).\n        actions : list[str], optional\n            Ordered list of actions to perform. Supported actions include:\n            'upload_netcdf', 'upload_csv', 'extract', 'calc_index',\n            'to_dataframe', 'to_csv', 'to_nc'.\n        file : str, optional\n            File path used for upload actions when required.\n\n        Returns\n        -------\n        WorkflowResult\n            Named result container with populated fields for dataframe/dataset/filenames.\n        \"\"\"\n        actions = actions or [\"extract\", \"compute_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n        result = WorkflowResult(cfg=self.cfg)\n        for action in actions:\n\n            if action == \"upload_netcdf\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                        \"but none was provided.\"\n                    )\n                    # Validate extension\n                valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_netcdf: '{file}'. \"\n                        f\"Expected one of: {valid_nc_ext}\"\n                    )\n                self.upload_netcdf(file)\n                result.dataset = self.current_ds\n\n            elif action == \"upload_csv\":\n                if file is None:\n                    raise ValueError(\n                        \"Action 'upload_csv' requires argument 'csv_file', \"\n                        \"but none was provided.\"\n                    )\n\n                # Validate CSV extension\n                valid_csv_ext = (\".csv\", \".csv.gz\")\n                if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                    raise ValueError(\n                        f\"Invalid file format for upload_csv: '{file}'. \"\n                        f\"Expected one of: {valid_csv_ext}\"\n                    )\n\n                self.upload_csv(file)\n                result.dataset = self.current_ds\n\n            elif action == \"extract\":\n                if self.cfg.dataset is None:\n                    raise ValueError(\n                        \"Action 'extract' cannot run because no dataset provider is set \"\n                        \"(cfg.dataset is None).\"\n                    )\n                self.extract()\n                result.dataset = self.current_ds\n\n            elif action == \"calc_index\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before computing an index.\"\n                    )\n                self.calc_index()\n                result.index_ds = self.current_ds\n\n            elif action == \"to_dataframe\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before converting to a DataFrame.\"\n                    )\n                self.to_dataframe()\n                result.dataframe = self.current_df\n\n            elif action == \"to_csv\":\n                if self.current_df is None:\n                    raise ValueError(\n                        \"Action 'to_csv' requires a DataFrame, but no DataFrame is available. \"\n                        \"Use 'to_dataframe' or upload a CSV before saving.\"\n                    )\n                result.filename = self.to_csv()\n\n            elif action == \"to_nc\":\n                if self.current_ds is None:\n                    raise ValueError(\n                        \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                        \"Upload or extract a dataset before saving to NetCDF.\"\n                    )\n                result.filename = self.to_nc()\n\n            else:\n                raise ValueError(f\"Unknown action '{action}'\")\n\n        return result\n\n    # ----------------------------\n    # Exploration helpers using cfg.dsinfo\n    # ----------------------------\n    def get_datasets(self) -&gt; List[str]:\n        \"\"\"\n        Return the list of dataset provider names available in configuration.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n        return list(self.cfg.dsinfo.keys())\n\n    def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n        \"\"\"\n        Return the list of variables available for a dataset.\n\n        Parameters\n        ----------\n        dataset : str, optional\n            Dataset name to query. Defaults to `cfg.dataset`.\n\n        Returns\n        -------\n        List[str]\n            List of variable names.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n            raise ValueError(\"Configuration or dsinfo not loaded\")\n\n        dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n        if dataset_name is None:\n            raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n        dsinfo = self.cfg.dsinfo.get(dataset_name)\n        if not dsinfo or \"variables\" not in dsinfo:\n            raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n        return list(dsinfo[\"variables\"].keys())\n\n    def get_varinfo(self, var: str) -&gt; dict:\n        \"\"\"\n        Get metadata for a variable from varinfo.\n\n        Parameters\n        ----------\n        var : str\n            Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n        Returns\n        -------\n        dict\n            Metadata dictionary containing cf_name, long_name, units, etc.\n\n        Raises\n        ------\n        ValueError\n            If varinfo is not loaded or variable not found.\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n            raise ValueError(\"Configuration or varinfo not loaded\")\n\n        if var not in self.cfg.varinfo:\n            raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n        return self.cfg.varinfo[var]\n\n\n    def get_actions(self) -&gt; dict:\n        \"\"\"\n        Return a dictionary of workflow actions with their outputs and descriptions.\n        Supports actions.yaml in mapping style (key: action_name).\n        \"\"\"\n        if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n            raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n        actions_map = getattr(self.cfg, \"actionsinfo\")\n\n        # If 'actions' key exists, fallback to list style\n        if \"actions\" in actions_map:\n            actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                        for a in actions_map[\"actions\"]}\n\n        return actions_map\n    def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n        \"\"\"\n        Fetch climate extreme indices from `cfg.extinfo` that involve the given variables.\n\n        Parameters\n        ----------\n        variables : list[str]\n            Variables to filter indices by (if None, uses `cfg.variables`).\n        require_all : bool\n            If True, return indices that require all provided variables; otherwise\n            return indices if any variable matches.\n\n        Returns\n        -------\n        dict\n            Mapping index_name -&gt; index_definition.\n        \"\"\"\n        cfg = self.cfg\n        variables = variables or cfg.variables \n        if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n            raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n        indices_def = cfg.extinfo.get(\"indices\", {})\n        if not indices_def:\n            return {}\n\n        matched_indices = {}\n        for idx_name, idx_info in indices_def.items():\n            idx_vars = idx_info.get(\"variables\", [])\n            if require_all:\n                if all(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n            else:\n                if any(var in variables for var in idx_vars):\n                    matched_indices[idx_name] = idx_info\n\n        return matched_indices\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.__init__","title":"<code>__init__(self, cfg_name='config', conf_path=None, overrides=None)</code>  <code>special</code>","text":"<p>Initialize the workflow manager and load configuration.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.__init__--parameters","title":"Parameters","text":"<p>cfg_name : str     Name of the Hydra configuration (default: \"config\"). conf_path : str, optional     Optional config path override (not commonly used). overrides : list[str], optional     Hydra overrides to apply to the configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def __init__(self, cfg_name=\"config\", conf_path=None, overrides: Optional[List[str]] = None):\n    \"\"\"\n    Initialize the workflow manager and load configuration.\n\n    Parameters\n    ----------\n    cfg_name : str\n        Name of the Hydra configuration (default: \"config\").\n    conf_path : str, optional\n        Optional config path override (not commonly used).\n    overrides : list[str], optional\n        Hydra overrides to apply to the configuration.\n    \"\"\"\n    self.cfg_name = cfg_name\n    self.conf_path = conf_path\n    self.cfg: Optional[DictConfig] = None\n\n    # Stage datasets\n    self.ds = None\n    self.current_ds = None\n    self.index_ds = None\n    self.impute_ds = None\n    self.bias_corrected_ds = None\n\n    # Stage DataFrames\n    self.raw_df = None\n    self.current_df = None\n    self.index_df = None\n    self.impute_df = None\n    self.bias_corrected_df = None\n    self.df = None  # alias for current_df\n\n    # filenames\n    self.filename = None\n    self.filetype = None\n\n    # Automatically load config on init\n    self.load_config(overrides)\n    self.cfg = self.preprocess_aoi(self.cfg)\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_actions","title":"<code>get_actions(self)</code>","text":"<p>Return a dictionary of workflow actions with their outputs and descriptions. Supports actions.yaml in mapping style (key: action_name).</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_actions(self) -&gt; dict:\n    \"\"\"\n    Return a dictionary of workflow actions with their outputs and descriptions.\n    Supports actions.yaml in mapping style (key: action_name).\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"actionsinfo\"):\n        raise ValueError(\"Configuration or actionsinfo not loaded\")\n\n    actions_map = getattr(self.cfg, \"actionsinfo\")\n\n    # If 'actions' key exists, fallback to list style\n    if \"actions\" in actions_map:\n        actions_map = {a[\"name\"]: {\"output\": a[\"output\"], \"description\": a[\"description\"]}\n                    for a in actions_map[\"actions\"]}\n\n    return actions_map\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_datasets","title":"<code>get_datasets(self)</code>","text":"<p>Return the list of dataset provider names available in configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_datasets(self) -&gt; List[str]:\n    \"\"\"\n    Return the list of dataset provider names available in configuration.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n    return list(self.cfg.dsinfo.keys())\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices","title":"<code>get_indices(self, variables, require_all=True)</code>","text":"<p>Fetch climate extreme indices from <code>cfg.extinfo</code> that involve the given variables.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices--parameters","title":"Parameters","text":"<p>variables : list[str]     Variables to filter indices by (if None, uses <code>cfg.variables</code>). require_all : bool     If True, return indices that require all provided variables; otherwise     return indices if any variable matches.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_indices--returns","title":"Returns","text":"<p>dict     Mapping index_name -&gt; index_definition.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_indices(self, variables: List[str], require_all: bool = True) -&gt; Dict[str, dict]:\n    \"\"\"\n    Fetch climate extreme indices from `cfg.extinfo` that involve the given variables.\n\n    Parameters\n    ----------\n    variables : list[str]\n        Variables to filter indices by (if None, uses `cfg.variables`).\n    require_all : bool\n        If True, return indices that require all provided variables; otherwise\n        return indices if any variable matches.\n\n    Returns\n    -------\n    dict\n        Mapping index_name -&gt; index_definition.\n    \"\"\"\n    cfg = self.cfg\n    variables = variables or cfg.variables \n    if not hasattr(cfg, \"extinfo\") or not cfg.extinfo:\n        raise ValueError(\"cfg.extinfo is not defined or empty\")\n\n    indices_def = cfg.extinfo.get(\"indices\", {})\n    if not indices_def:\n        return {}\n\n    matched_indices = {}\n    for idx_name, idx_info in indices_def.items():\n        idx_vars = idx_info.get(\"variables\", [])\n        if require_all:\n            if all(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n        else:\n            if any(var in variables for var in idx_vars):\n                matched_indices[idx_name] = idx_info\n\n    return matched_indices\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_variables","title":"<code>get_variables(self, dataset=None)</code>","text":"<p>Return the list of variables available for a dataset.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_variables--parameters","title":"Parameters","text":"<p>dataset : str, optional     Dataset name to query. Defaults to <code>cfg.dataset</code>.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_variables--returns","title":"Returns","text":"<p>List[str]     List of variable names.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_variables(self, dataset: Optional[str] = None) -&gt; List[str]:\n    \"\"\"\n    Return the list of variables available for a dataset.\n\n    Parameters\n    ----------\n    dataset : str, optional\n        Dataset name to query. Defaults to `cfg.dataset`.\n\n    Returns\n    -------\n    List[str]\n        List of variable names.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"dsinfo\"):\n        raise ValueError(\"Configuration or dsinfo not loaded\")\n\n    dataset_name = dataset or getattr(self.cfg, \"dataset\", None)\n    if dataset_name is None:\n        raise ValueError(\"Dataset not specified and cfg.dataset is None\")\n\n    dsinfo = self.cfg.dsinfo.get(dataset_name)\n    if not dsinfo or \"variables\" not in dsinfo:\n        raise ValueError(f\"No variable info available for dataset '{dataset_name}'\")\n\n    return list(dsinfo[\"variables\"].keys())\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo","title":"<code>get_varinfo(self, var)</code>","text":"<p>Get metadata for a variable from varinfo.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--parameters","title":"Parameters","text":"<p>var : str     Name of the variable, e.g., 'tas', 'tasmax', 'pr'.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--returns","title":"Returns","text":"<p>dict     Metadata dictionary containing cf_name, long_name, units, etc.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.get_varinfo--raises","title":"Raises","text":"<p>ValueError     If varinfo is not loaded or variable not found.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def get_varinfo(self, var: str) -&gt; dict:\n    \"\"\"\n    Get metadata for a variable from varinfo.\n\n    Parameters\n    ----------\n    var : str\n        Name of the variable, e.g., 'tas', 'tasmax', 'pr'.\n\n    Returns\n    -------\n    dict\n        Metadata dictionary containing cf_name, long_name, units, etc.\n\n    Raises\n    ------\n    ValueError\n        If varinfo is not loaded or variable not found.\n    \"\"\"\n    if not self.cfg or not hasattr(self.cfg, \"varinfo\") or not self.cfg.varinfo:\n        raise ValueError(\"Configuration or varinfo not loaded\")\n\n    if var not in self.cfg.varinfo:\n        raise ValueError(f\"Variable '{var}' not found in varinfo\")\n\n    return self.cfg.varinfo[var]\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.load_config","title":"<code>load_config(self, overrides=None)</code>","text":"<p>Load and compose the Hydra configuration.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.load_config--parameters","title":"Parameters","text":"<p>overrides : list[str], optional     Hydra overrides to apply when composing the configuration.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.load_config--returns","title":"Returns","text":"<p>DictConfig     Composed Hydra configuration object and stored on <code>self.cfg</code>.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def load_config(self, overrides: Optional[List[str]] = None) -&gt; DictConfig:\n    \"\"\"\n    Load and compose the Hydra configuration.\n\n    Parameters\n    ----------\n    overrides : list[str], optional\n        Hydra overrides to apply when composing the configuration.\n\n    Returns\n    -------\n    DictConfig\n        Composed Hydra configuration object and stored on `self.cfg`.\n    \"\"\"\n    overrides = overrides or []\n    conf_dir = _ensure_local_conf()\n    rel_conf_dir = os.path.relpath(conf_dir, os.path.dirname(__file__))\n\n    if not GlobalHydra.instance().is_initialized():\n        hydra_ctx = initialize(config_path=rel_conf_dir, version_base=None)\n    else:\n        hydra_ctx = None\n\n    if hydra_ctx:\n        with hydra_ctx:\n            self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    else:\n        self.cfg = compose(config_name=self.cfg_name, overrides=overrides)\n    return self.cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.preprocess_aoi","title":"<code>preprocess_aoi(self, cfg)</code>","text":"<p>Process an 'aoi' specification in the configuration.</p> <p>Supports GeoJSON strings or dictionaries for FeatureCollection, Feature, or simple geometry objects (Point/Polygon). When a Point is provided, <code>cfg.lat</code> and <code>cfg.lon</code> are set. When a Polygon is provided, <code>cfg.bounds</code> is set and <code>cfg.region</code> is set to \"custom\".</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.preprocess_aoi--parameters","title":"Parameters","text":"<p>cfg : DictConfig     Configuration object with optional <code>aoi</code> entry.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.preprocess_aoi--returns","title":"Returns","text":"<p>DictConfig     The modified configuration.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def preprocess_aoi(self, cfg: DictConfig) -&gt; DictConfig:\n    \"\"\"\n    Process an 'aoi' specification in the configuration.\n\n    Supports GeoJSON strings or dictionaries for FeatureCollection, Feature,\n    or simple geometry objects (Point/Polygon). When a Point is provided,\n    `cfg.lat` and `cfg.lon` are set. When a Polygon is provided, `cfg.bounds`\n    is set and `cfg.region` is set to \"custom\".\n\n    Parameters\n    ----------\n    cfg : DictConfig\n        Configuration object with optional `aoi` entry.\n\n    Returns\n    -------\n    DictConfig\n        The modified configuration.\n    \"\"\"\n    if not hasattr(cfg, \"aoi\") or cfg.aoi is None:\n        return cfg\n\n    if isinstance(cfg.aoi, str):\n        try:\n            cfg.aoi = json.loads(cfg.aoi)\n        except json.JSONDecodeError:\n            raise ValueError(\"Invalid AOI JSON string\")\n\n    aoi = cfg.aoi\n\n    if aoi.get(\"type\") == \"FeatureCollection\":\n        geom = shape(aoi[\"features\"][0][\"geometry\"])\n    elif aoi.get(\"type\") == \"Feature\":\n        geom = shape(aoi[\"geometry\"])\n    elif \"type\" in aoi:\n        geom = shape(aoi)\n    else:\n        raise ValueError(f\"Unsupported AOI format: {aoi}\")\n\n    if isinstance(geom, Point):\n        cfg.lat = geom.y\n        cfg.lon = geom.x\n        cfg.bounds = None\n    elif isinstance(geom, Polygon):\n        minx, miny, maxx, maxy = geom.bounds\n        cfg.bounds = {\"custom\": {\"lat_min\": miny, \"lat_max\": maxy,\n                                 \"lon_min\": minx, \"lon_max\": maxx}}\n        cfg.region = \"custom\"\n        cfg.lat = None\n        cfg.lon = None\n    else:\n        raise ValueError(f\"Unknown geometry type {geom.geom_type}\")\n\n    return cfg\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.run_workflow","title":"<code>run_workflow(self, overrides=None, actions=None, file=None)</code>","text":"<p>Execute a sequence of workflow actions.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.run_workflow--parameters","title":"Parameters","text":"<p>overrides : list[str], optional     Hydra overrides to apply (not all actions will use these). actions : list[str], optional     Ordered list of actions to perform. Supported actions include:     'upload_netcdf', 'upload_csv', 'extract', 'calc_index',     'to_dataframe', 'to_csv', 'to_nc'. file : str, optional     File path used for upload actions when required.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.run_workflow--returns","title":"Returns","text":"<p>WorkflowResult     Named result container with populated fields for dataframe/dataset/filenames.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def run_workflow(self, overrides: Optional[List[str]] = None,\n                 actions: Optional[List[str]] = None,\n                 file: Optional[str] = None) -&gt; WorkflowResult:\n    \"\"\"\n    Execute a sequence of workflow actions.\n\n    Parameters\n    ----------\n    overrides : list[str], optional\n        Hydra overrides to apply (not all actions will use these).\n    actions : list[str], optional\n        Ordered list of actions to perform. Supported actions include:\n        'upload_netcdf', 'upload_csv', 'extract', 'calc_index',\n        'to_dataframe', 'to_csv', 'to_nc'.\n    file : str, optional\n        File path used for upload actions when required.\n\n    Returns\n    -------\n    WorkflowResult\n        Named result container with populated fields for dataframe/dataset/filenames.\n    \"\"\"\n    actions = actions or [\"extract\", \"compute_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n    result = WorkflowResult(cfg=self.cfg)\n    for action in actions:\n\n        if action == \"upload_netcdf\":\n            if file is None:\n                raise ValueError(\n                    \"Action 'upload_netcdf' requires argument 'netcdf_file', \"\n                    \"but none was provided.\"\n                )\n                # Validate extension\n            valid_nc_ext = (\".nc\", \".nc4\", \".nc.gz\")\n            if not any(str(file).lower().endswith(ext) for ext in valid_nc_ext):\n                raise ValueError(\n                    f\"Invalid file format for upload_netcdf: '{file}'. \"\n                    f\"Expected one of: {valid_nc_ext}\"\n                )\n            self.upload_netcdf(file)\n            result.dataset = self.current_ds\n\n        elif action == \"upload_csv\":\n            if file is None:\n                raise ValueError(\n                    \"Action 'upload_csv' requires argument 'csv_file', \"\n                    \"but none was provided.\"\n                )\n\n            # Validate CSV extension\n            valid_csv_ext = (\".csv\", \".csv.gz\")\n            if not any(str(file).lower().endswith(ext) for ext in valid_csv_ext):\n                raise ValueError(\n                    f\"Invalid file format for upload_csv: '{file}'. \"\n                    f\"Expected one of: {valid_csv_ext}\"\n                )\n\n            self.upload_csv(file)\n            result.dataset = self.current_ds\n\n        elif action == \"extract\":\n            if self.cfg.dataset is None:\n                raise ValueError(\n                    \"Action 'extract' cannot run because no dataset provider is set \"\n                    \"(cfg.dataset is None).\"\n                )\n            self.extract()\n            result.dataset = self.current_ds\n\n        elif action == \"calc_index\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'calc_index' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before computing an index.\"\n                )\n            self.calc_index()\n            result.index_ds = self.current_ds\n\n        elif action == \"to_dataframe\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'to_dataframe' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before converting to a DataFrame.\"\n                )\n            self.to_dataframe()\n            result.dataframe = self.current_df\n\n        elif action == \"to_csv\":\n            if self.current_df is None:\n                raise ValueError(\n                    \"Action 'to_csv' requires a DataFrame, but no DataFrame is available. \"\n                    \"Use 'to_dataframe' or upload a CSV before saving.\"\n                )\n            result.filename = self.to_csv()\n\n        elif action == \"to_nc\":\n            if self.current_ds is None:\n                raise ValueError(\n                    \"Action 'to_nc' requires a dataset, but no dataset is available. \"\n                    \"Upload or extract a dataset before saving to NetCDF.\"\n                )\n            result.filename = self.to_nc()\n\n        else:\n            raise ValueError(f\"Unknown action '{action}'\")\n\n    return result\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_csv","title":"<code>to_csv(self, df=None, filename=None)</code>","text":"<p>Save a DataFrame to CSV.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_csv--parameters","title":"Parameters","text":"<p>df : pd.DataFrame, optional     DataFrame to save. Defaults to <code>self.current_df</code>. filename : str, optional     Output filename. Defaults to <code>self.filename_csv</code>.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_csv--returns","title":"Returns","text":"<p>str     The path of the written CSV file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_csv(self, df: Optional[pd.DataFrame] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Save a DataFrame to CSV.\n\n    Parameters\n    ----------\n    df : pd.DataFrame, optional\n        DataFrame to save. Defaults to `self.current_df`.\n    filename : str, optional\n        Output filename. Defaults to `self.filename_csv`.\n\n    Returns\n    -------\n    str\n        The path of the written CSV file.\n    \"\"\"\n    df = df if df is not None else self.current_df\n\n    filename = filename or getattr(self, \"filename_csv\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_csv is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    df.to_csv(filename, index=False)\n    self.filename_csv = str(path)\n    self.current_filename = str(path)\n\n    print(f\"DataFrame saved to CSV file: {self.current_filename}\")\n\n    return filename\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_nc","title":"<code>to_nc(self, ds=None, filename=None)</code>","text":"<p>Save an xarray Dataset to NetCDF. - If ds is None: save current_ds. - If filename is None: use self.filename_nc. - Creates directories if needed. - Updates self.filename_nc and self.current_filename.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_nc--parameters","title":"Parameters","text":"<p>ds : xr.Dataset, optional     Dataset to save. If None, uses <code>self.current_ds</code>. filename : str, optional     Output filename. Defaults to <code>self.filename_nc</code>.</p>"},{"location":"climdata/#climdata.utils.wrapper_workflow.ClimateExtractor.to_nc--returns","title":"Returns","text":"<p>str     The path of the written NetCDF file.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def to_nc(self, ds: Optional[xr.Dataset] = None, filename: Optional[str] = None) -&gt; str:\n    \"\"\"\n    Save an xarray Dataset to NetCDF.\n    - If ds is None: save current_ds.\n    - If filename is None: use self.filename_nc.\n    - Creates directories if needed.\n    - Updates self.filename_nc and self.current_filename.\n\n    Parameters\n    ----------\n    ds : xr.Dataset, optional\n        Dataset to save. If None, uses `self.current_ds`.\n    filename : str, optional\n        Output filename. Defaults to `self.filename_nc`.\n\n    Returns\n    -------\n    str\n        The path of the written NetCDF file.\n    \"\"\"\n\n    # -------------------------------\n    # 1. Determine dataset to save\n    # -------------------------------\n    ds = ds or getattr(self, \"current_ds\", None)\n    if ds is None:\n        raise ValueError(\"No dataset available to save\")\n\n    # -------------------------------\n    # 2. Determine filename\n    # -------------------------------\n    filename = filename or getattr(self, \"filename_nc\", None)\n    if filename is None:\n        raise ValueError(\"No filename provided and filename_nc is not set\")\n\n    path = Path(filename)\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    # -------------------------------\n    # 3. Save to NetCDF\n    # -------------------------------\n    ds.to_netcdf(path)\n\n    # -------------------------------\n    # 4. Track filenames\n    # -------------------------------\n    self.filename_nc = str(path)\n    self.current_filename = str(path)\n\n    print(f\"Dataset saved to NetCDF file: {self.current_filename}\")\n\n    return str(path)\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.WorkflowResult","title":"<code> WorkflowResult        </code>  <code>dataclass</code>","text":"<p>WorkflowResult(cfg: omegaconf.dictconfig.DictConfig, dataset: Optional[xarray.core.dataset.Dataset] = None, dataframe: Optional[pandas.core.frame.DataFrame] = None, filename: Optional[str] = None, index_ds: Optional[xarray.core.dataset.Dataset] = None, index_filename: Optional[str] = None)</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>@dataclass\nclass WorkflowResult:\n    cfg: DictConfig\n    dataset: Optional[xr.Dataset] = None\n    dataframe: Optional[pd.DataFrame] = None\n    filename: Optional[str] = None\n    index_ds: Optional[xr.Dataset] = None\n    index_filename: Optional[str] = None\n\n    def keys(self):\n        return [k for k, v in self.__dict__.items() if v is not None]\n</code></pre>"},{"location":"climdata/#climdata.utils.wrapper_workflow.update_df","title":"<code>update_df(attr_name=None)</code>","text":"<p>Decorator to update self.current_df with the result of a method, and optionally store it in a named attribute.</p> Source code in <code>climdata/utils/wrapper_workflow.py</code> <pre><code>def update_df(attr_name=None):\n    \"\"\"\n    Decorator to update self.current_df with the result of a method,\n    and optionally store it in a named attribute.\n    \"\"\"\n    def decorator(func):\n        def wrapper(self, *args, **kwargs):\n            df = func(self, *args, **kwargs)\n            if df is not None:\n                self.current_df = df\n                self.df = df  # keep alias for convenience\n                if attr_name:\n                    setattr(self, attr_name, df)\n            return df\n        return wrapper\n    return decorator\n</code></pre>"},{"location":"common/","title":"Common Concepts in climdata","text":"<p>This page describes common terminology, configuration patterns, and reusable components in the <code>climdata</code> package.</p>"},{"location":"common/#configuration-files","title":"Configuration Files","text":"<ul> <li>All configuration is managed via Hydra and YAML files in the <code>conf/</code> directory.</li> <li>See <code>config.yaml</code> for the main entry point.</li> </ul>"},{"location":"common/#standard-variable-names","title":"Standard Variable Names","text":"<ul> <li>Variables follow CF conventions (see <code>variables.yaml</code>).</li> <li>Example: <code>tas</code> for air temperature, <code>pr</code> for precipitation.</li> </ul>"},{"location":"common/#output-schema","title":"Output Schema","text":"<p>All outputs are standardized to the following columns:</p> Column Description latitude Latitude of observation/grid longitude Longitude of observation/grid time Timestamp source Data source/provider variable Variable name value Observed or modeled value units Units of measurement"},{"location":"common/#regions-and-bounds","title":"Regions and Bounds","text":"<ul> <li>Regions are defined in <code>config.yaml</code> under <code>bounds</code>.</li> <li>Example: <code>europe</code>, <code>global</code>.</li> </ul>"},{"location":"common/#usage-patterns","title":"Usage Patterns","text":"<ul> <li>Use <code>climdata.load_config()</code> to load configuration.</li> <li>Use <code>climdata.DWD(cfg)</code> or <code>climdata.MSWX(cfg)</code> for dataset access.</li> </ul> <p>Add more shared concepts as your documentation grows.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>climdata could always use more documentation, whether as part of the official climdata docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/Kaushikreddym/climdata/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up climdata for local development.</p> <ol> <li> <p>Fork the climdata repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/climdata.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv climdata\n$ cd climdata/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 climdata tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/Kaushikreddym/climdata/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>It's recommended to create and activate a conda environment first, then install via pip:</p> <pre><code># create &amp; activate conda environment (recommended)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install climdata from PyPI\npip install climdata\n</code></pre> <p>This is the preferred method to install climdata, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install climdata from sources, create/activate a conda environment and then install from the repository:</p> <pre><code># create &amp; activate conda environment (optional)\nconda create -n climdata python=3.11 -y\nconda activate climdata\n\n# install from GitHub (editable install if desired)\npip install git+https://github.com/Kaushikreddym/climdata\n# or for editable development install:\n# git clone https://github.com/Kaushikreddym/climdata\n# cd climdata\n# pip install -e .\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>Quick examples to get started with the ClimData workflow utilities.</p>"},{"location":"usage/#quickstart","title":"Quickstart","text":"<p>Install into a conda env (recommended) and then pip: <pre><code>conda create -n climdata python=3.11 -y\nconda activate climdata\npip install climdata\n# or from source:\n# pip install -e .\n</code></pre></p>"},{"location":"usage/#minimal-example","title":"Minimal example","text":"<pre><code>from climdata import ClimData\n\noverrides = [\n    \"dataset=mswx\",\n    \"lat=52.5\",\n    \"lon=13.4\",\n    \"time_range.start_date=2014-01-01\",\n    \"time_range.end_date=2014-12-31\",\n    \"variables=[tasmin,tasmax,pr]\",\n]\n\nextractor = ClimData(overrides=overrides)\n\n# Extract dataset (updates extractor.current_ds)\nds = extractor.extract()\n\n# Compute configured extreme index (updates extractor.index_ds)\nindex_ds = extractor.calc_index(ds)\n\n# Convert to long-form DataFrame (updates extractor.current_df)\ndf = extractor.to_dataframe(index_ds)\n\n# Save DataFrame to CSV\nextractor.to_csv(df, filename=\"index.csv\")\n</code></pre>"},{"location":"usage/#single-call-workflow","title":"Single-call workflow","text":"<p>Use the high-level runner to chain common steps: <pre><code>result = extractor.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\", \"to_csv\"])\n# result contains produced dataset/dataframe and filenames\nprint(result.dataframe.head())\nprint(\"Saved to:\", result.filename)\n</code></pre></p>"},{"location":"usage/#uploading-existing-files","title":"Uploading existing files","text":"<ul> <li>Load NetCDF: extractor.upload_netcdf(\"path/to/file.nc\")</li> <li>Load long-form CSV: extractor.upload_csv(\"path/to/file.csv\")</li> </ul>"},{"location":"usage/#introspection-helpers","title":"Introspection helpers","text":"<ul> <li>extractor.get_datasets()</li> <li>extractor.get_variables(dataset_name)</li> <li>extractor.get_varinfo(varname)</li> <li>extractor.get_actions()</li> </ul>"},{"location":"usage/#notes","title":"Notes","text":"<ul> <li>See <code>docs/index.md</code> for installation details and full examples.</li> <li>For provider-specific options (MSWX, CMIP, POWER, DWD, HYRAS) consult the configuration files under <code>conf/</code> and the API docs.</li> </ul>"},{"location":"examples/climdata_cli/","title":"Climdata cli","text":"In\u00a0[\u00a0]: Copied! <pre>import climdata\nimport xarray as xr\nimport xclim\nimport pandas as pd\nfrom climdata.utils.config import _ensure_local_conf\nfrom omegaconf import DictConfig\nimport hydra\nfrom hydra.core.global_hydra import GlobalHydra\nimport sys\n</pre> import climdata import xarray as xr import xclim import pandas as pd from climdata.utils.config import _ensure_local_conf from omegaconf import DictConfig import hydra from hydra.core.global_hydra import GlobalHydra import sys <p>Example usage:</p> <p>Run the CLI with overrides:</p> <p>python climdata_cli.py  dataset=mswx  lat=52.507  lon=13.137  time_range.start_date=2000-01-01  time_range.end_date=2000-12-31  dsinfo.mswx.params.google_service_account=/home/muduchuru/.climdata_conf/service.json  data_dir=/beegfs/muduchuru/data/  variables=['tas']</p> <p>All Hydra overrides follow the format key=value.</p> In\u00a0[\u00a0]: Copied! <pre>_ensure_local_conf()\n@hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None)\ndef main(cfg: DictConfig) -&gt; None:\n    overrides = sys.argv[1:]\n\n    # Extract data\n    cfg, filename, ds = climdata.extract_data(overrides=overrides)\n</pre> _ensure_local_conf() @hydra.main(config_path=\"./conf\", config_name=\"config\", version_base=None) def main(cfg: DictConfig) -&gt; None:     overrides = sys.argv[1:]      # Extract data     cfg, filename, ds = climdata.extract_data(overrides=overrides) In\u00a0[\u00a0]: Copied! <pre>if __name__ == \"__main__\":\n    main()\n</pre> if __name__ == \"__main__\":     main()"},{"location":"examples/climdata_cli/#uncomment-the-below-snippet-for-parallel-processing","title":"uncomment the below snippet for parallel processing\u00b6","text":"<p>import dask from dask.distributed import Client</p>"},{"location":"examples/climdata_cli/#configure-dask","title":"Configure Dask\u00b6","text":"<p>client = Client( n_workers=20,        # or match number of physical cores threads_per_worker=2, memory_limit=\"10GB\"  # per worker (8 * 10GB = 80GB total) ) from multiprocessing import freeze_support</p>"},{"location":"examples/extremes/","title":"Extremes","text":"In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) <pre>\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1989_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1989_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1990_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1990_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1991_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1991_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1992_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1992_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1993_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1993_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1994_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1994_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1995_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1995_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1996_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1996_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1997_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1997_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1998_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1998_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_1999_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_1999_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2000_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2000_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2001_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2001_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2002_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2002_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2003_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2003_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2004_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2004_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2005_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2005_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2006_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2006_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2007_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2007_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2008_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2008_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2009_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2009_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2010_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2010_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2011_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2011_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2012_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2012_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2013_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2013_v6-0_de.nc\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2014_v6-0_de.nc\n\u2714\ufe0f  Exists locally: /beegfs/muduchuru/data/hyras/TASMIN/tasmin_hyras_1_2014_v6-0_de.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\n</pre> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          13.246667038198012,\n          52.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ],\n    save_to_file=False\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           13.246667038198012,           52.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\", #optional              f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ],     save_to_file=False ) Out[\u00a0]: <pre>&lt;xarray.Dataset&gt; Size: 158kB\nDimensions:      (time: 9496, dayofyear: 366)\nCoordinates:\n  * time         (time) datetime64[ns] 76kB 1989-01-01 1989-01-02 ... 2014-12-31\n    lon          float64 8B 13.25\n    lat          float64 8B 52.9\n    x            float32 4B 4.54e+06\n    y            float32 4B 3.314e+06\n    percentiles  int64 8B 10\n  * dayofyear    (dayofyear) int64 3kB 1 2 3 4 5 6 7 ... 361 362 363 364 365 366\nData variables:\n    tasmin       (time) float64 76kB -0.5 3.2 -1.5 -3.1 ... -9.0 -7.9 -4.2 0.9\n    tasmin_per   (dayofyear) float64 3kB -11.02 -10.47 -10.85 ... -10.89 -11.02\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_1989_v6-0_de.nc</pre>xarray.Dataset<ul><li>Dimensions:<ul><li>time: 9496</li><li>dayofyear: 366</li></ul></li><li>Coordinates: (7)<ul><li>time(time)datetime64[ns]1989-01-01 ... 2014-12-31standard_name :timelong_name :Mid Of Twentyfour Hour Time Interval [UTC]bounds :time_bndsaxis :T<pre>array(['1989-01-01T00:00:00.000000000', '1989-01-02T00:00:00.000000000',\n       '1989-01-03T00:00:00.000000000', ..., '2014-12-29T00:00:00.000000000',\n       '2014-12-30T00:00:00.000000000', '2014-12-31T00:00:00.000000000'],\n      dtype='datetime64[ns]')</pre></li><li>lon()float6413.25standard_name :longitudelong_name :Longitude Of Cell Centerunits :degrees_east_CoordinateAxisType :Lon<pre>array(13.24777272)</pre></li><li>lat()float6452.9standard_name :latitudelong_name :Latitude Of Cell Centerunits :degrees_north_CoordinateAxisType :Lat<pre>array(52.89523559)</pre></li><li>x()float324.54e+06standard_name :projection_x_coordinatelong_name :X Coordinate Of Projection for Cell Centerunits :maxis :Xbounds :x_bnds<pre>array(4539500., dtype=float32)</pre></li><li>y()float323.314e+06standard_name :projection_y_coordinatelong_name :Y Coordinate Of Projection for Cell Centerunits :maxis :Ybounds :y_bnds<pre>array(3314500., dtype=float32)</pre></li><li>percentiles()int6410<pre>array(10)</pre></li><li>dayofyear(dayofyear)int641 2 3 4 5 6 ... 362 363 364 365 366<pre>array([  1,   2,   3, ..., 364, 365, 366])</pre></li></ul></li><li>Data variables: (2)<ul><li>tasmin(time)float64-0.5 3.2 -1.5 ... -7.9 -4.2 0.9standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]<pre>array([-0.50000001,  3.20000005, -1.50000002, ..., -7.90000012,\n       -4.20000006,  0.90000001])</pre></li><li>tasmin_per(dayofyear)float64-11.02 -10.47 ... -10.89 -11.02standard_name :air_temperaturelong_name :Daily Minimum Air Temperatureunits :degree_Celsiusgrid_mapping :crscell_methods :time: minimumunits_metadata :temperature: on-scaleCoordinateSystems :LatLonCoordinateSystem ProjectionCoordinateSystemesri_pe_string :PROJCS[\"ETRS_1989_LAEA\",GEOGCS[\"GCS_ETRS_1989\",DATUM[\"D_ETRS_1989\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Lambert_Azimuthal_Equal_Area\"],PARAMETER[\"False_Easting\",4321000.0],PARAMETER[\"False_Northing\",3210000.0],PARAMETER[\"Central_Meridian\",10.0],PARAMETER[\"Latitude_Of_Origin\",52.0],UNIT[\"Meter\",1.0]]climatology_bounds :['1989-01-01', '2014-12-31']window :5alpha :0.3333333333333333beta :0.3333333333333333history :[2025-11-27 15:54:07] per: percentile_doy(arr=tasmin, window=5, per=10, alpha=0.3333333333333333, beta=0.3333333333333333, copy=True) - xclim version: 0.56.0<pre>array([-11.01666683, -10.47482207, -10.8512513 , -10.99879468,\n       -11.00000016, -11.00000016, -10.85574445,  -9.87251156,\n        -9.50774443,  -8.09875811,  -6.32885854,  -5.71747954,\n        -5.82250237,  -5.7045115 ,  -6.25775352,  -6.490959  ,\n        -6.5000001 ,  -6.84641106,  -7.30381746,  -7.96494989,\n        -9.24452069, -10.47275815, -11.27005496, -12.8002285 ,\n       -13.1709317 , -12.97885864, -12.59801845, -10.42470335,\n        -9.61626498,  -9.56333348, -10.19662116, -10.68646591,\n       -10.72666683, -10.90858464, -11.56750702, -10.92812802,\n       -10.40264856, -10.6019545 , -10.11934262,  -8.50294077,\n        -8.5363015 ,  -8.56333346,  -8.56333346,  -7.57527865,\n        -6.84530604,  -6.38050238,  -6.15187224,  -6.12666676,\n        -6.61591791,  -6.20229233,  -6.12666676,  -6.35320557,\n        -7.24753435,  -7.28172614,  -7.73480377,  -8.27246588,\n        -8.01470332,  -7.30920559,  -6.61525124,  -5.34973524,\n        -5.12666674,  -5.02116902,  -5.738822  ,  -6.2512969 ,\n        -6.32666676,  -6.32666676,  -6.2229042 ,  -5.48697725,\n        -5.16392702,  -5.12666674,  -5.18593615,  -5.14093158,\n        -4.54869413,  -3.68133339,  -3.09871237,  -2.36369867,\n        -2.51671237,  -3.46794526,  -4.09315075,  -4.20000006,\n...\n        -0.5400822 ,  -0.77536987,  -1.10000002,  -1.14054796,\n        -1.42533335,  -1.80305026,  -1.30000002,  -1.30000002,\n        -1.30000002,  -1.30000002,  -1.37158906,  -1.69000003,\n        -1.69000003,  -1.6742192 ,  -1.61093153,  -1.63389044,\n        -1.50838358,  -1.60000002,  -1.61023747,  -1.63578998,\n        -1.47854797,  -1.36808221,  -1.05027399,  -0.68567124,\n        -0.24356165,  -0.51424658,  -0.74857535,  -1.68205482,\n        -1.88412788,  -2.51039273,  -3.06333338,  -3.04610963,\n        -2.92666671,  -2.98894982,  -3.50459366,  -3.99056627,\n        -4.28574436,  -5.06488592,  -5.91528776,  -6.10202749,\n        -5.76193616,  -4.61417358,  -4.49648409,  -4.4633334 ,\n        -4.46664847,  -4.53419185,  -4.9701188 ,  -5.88401835,\n        -6.35995443,  -5.31666675,  -5.31740647,  -5.32666675,\n        -5.36890419,  -5.99966219,  -6.87729691,  -8.05017364,\n        -8.42510515,  -8.55182661,  -8.283233  ,  -7.00666677,\n        -7.02032887,  -7.30000011,  -7.28794531,  -7.00244759,\n        -6.90117819,  -7.05763481,  -8.03612797,  -7.0433791 ,\n        -6.69986311,  -7.10490422,  -7.77000012,  -7.78200012,\n        -8.50945218,  -9.20581749, -10.63634719, -10.99941569,\n       -10.8936714 , -11.01666683])</pre></li></ul></li><li>Indexes: (2)<ul><li>timePandasIndex<pre>PandasIndex(DatetimeIndex(['1989-01-01', '1989-01-02', '1989-01-03', '1989-01-04',\n               '1989-01-05', '1989-01-06', '1989-01-07', '1989-01-08',\n               '1989-01-09', '1989-01-10',\n               ...\n               '2014-12-22', '2014-12-23', '2014-12-24', '2014-12-25',\n               '2014-12-26', '2014-12-27', '2014-12-28', '2014-12-29',\n               '2014-12-30', '2014-12-31'],\n              dtype='datetime64[ns]', name='time', length=9496, freq=None))</pre></li><li>dayofyearPandasIndex<pre>PandasIndex(Index([  1,   2,   3,   4,   5,   6,   7,   8,   9,  10,\n       ...\n       357, 358, 359, 360, 361, 362, 363, 364, 365, 366],\n      dtype='int64', name='dayofyear', length=366))</pre></li></ul></li><li>Attributes: (21)source :surface observationsinstitution :Deutscher Wetterdienst (DWD)Conventions :CF-1.11title :gridded_temperature_dataset_(HYRAS-DE TASMIN)realization :v6-0project_id :HYRASlevel_type :surfacefrequency :dayhorizontal_resolution :1_kmreferences :https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_minauthor :Climate Monitoring (KU21)contact :klimaanalyse@dwd.decreation_date :created at 2024-09-01 11:11:16variable_id :tasminunique_dataset_id :DWD_HYRAS_tasmin_v6-0_1989_day_0066D4B2A8license :The HYRAS data, produced by DWD, is licensed under a Creative Commons Attribution 4.0 International License (CC BY 4.0; (see https://creativecommons.org/licenses/).ConventionsURL :http://cfconventions.org/Data/cf-conventions/cf-conventions-1.11/cf-conventions.htmlrealm :atmosproduct :observationsinput_data_status :checkedfilename :tasmin_hyras_1_1989_v6-0_de.nc</li></ul> In\u00a0[\u00a0]: Copied! <pre>import json\nfrom climdata.extremes.indices import extreme_index \ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin','tas']\",\n            f\"time_range.start_date=2013-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json from climdata.extremes.indices import extreme_index  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   import climdata clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin','tas']\",             f\"time_range.start_date=2013-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_20130101_20141231.nc\n</pre> In\u00a0[6]: Copied! <pre>import json\nimport climdata\n# geojson = {\n#   \"type\": \"FeatureCollection\",\n#   \"features\": [\n#     {\n#       \"type\": \"Feature\",\n#       \"properties\": {},\n#       \"geometry\": {\n#         \"coordinates\": [\n#           13.246667038198012,\n#           52.891982026993958\n#         ],\n#         \"type\": \"Point\"\n#       }\n#     }\n#   ]\n# }\n\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"type\": \"Polygon\",\n        \"coordinates\": [\n          [\n            [8.0, 50.0],\n            [10.0, 50.0],\n            [10.0, 55.0],\n            [8.0, 55.0],\n            [8.0, 50.0]\n          ]\n        ]\n      }\n    }\n  ]\n}\n\n\nclim = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            f\"time_range.start_date=1989-01-01\",\n            f\"time_range.end_date=2014-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"index=tn10p\",\n            \"data_dir=/beegfs/muduchuru/data\", #optional \n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n,\nsave_to_file=True\n)\n</pre> import json import climdata # geojson = { #   \"type\": \"FeatureCollection\", #   \"features\": [ #     { #       \"type\": \"Feature\", #       \"properties\": {}, #       \"geometry\": { #         \"coordinates\": [ #           13.246667038198012, #           52.891982026993958 #         ], #         \"type\": \"Point\" #       } #     } #   ] # }  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"type\": \"Polygon\",         \"coordinates\": [           [             [8.0, 50.0],             [10.0, 50.0],             [10.0, 55.0],             [8.0, 55.0],             [8.0, 50.0]           ]         ]       }     }   ] }   clim = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             f\"time_range.start_date=1989-01-01\",             f\"time_range.end_date=2014-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"index=tn10p\",             \"data_dir=/beegfs/muduchuru/data\", #optional              \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] , save_to_file=True ) <pre>Saved NetCDF to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\nCalculating index: tn10p\n&lt;class 'xarray.core.dataset.Dataset'&gt;\nSaved index to: cmip_tn10p_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n\u2705 Saved output to cmip_surface_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\n</pre> In\u00a0[9]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\ndata = xr.open_dataset(\"cmip_tn10p_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\").isel(time=1)['tn10p']\n\nplt.pcolormesh(data.lon,data.lat,data)\n</pre> import xarray as xr import matplotlib.pyplot as plt data = xr.open_dataset(\"cmip_tn10p_LAT50.0-55.0_LON8.0-10.0_19890101_20141231.nc\").isel(time=1)['tn10p']  plt.pcolormesh(data.lon,data.lat,data) Out[9]: <pre>&lt;matplotlib.collections.QuadMesh at 0x145b62240fa0&gt;</pre>"},{"location":"examples/wrapper/","title":"Wrapper","text":"In\u00a0[1]: Copied! <pre>import json\n\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          24.246667038198012,\n          12.891982026993958\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\nimport climdata\nclim_ds = climdata.extract_data(\n    overrides=[\n            \"dataset=mswx\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n)\n</pre> import json  geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           24.246667038198012,           12.891982026993958         ],         \"type\": \"Point\"       }     }   ] }   import climdata clim_ds = climdata.extract_data(     overrides=[             \"dataset=mswx\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] ) <pre>\u2705 All 31 tasmin files already exist locally.\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to mswx_surface_LAT_12.891982026993958_LON_24.246667038198012_20201201_20201231.csv\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> Out[\u00a0]: <pre>[&lt;matplotlib.lines.Line2D at 0x14a5cdb04e50&gt;]</pre> In\u00a0[1]: Copied! <pre>import json\n\ngeojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}\n\nimport climdata\nmswx = climdata.extract_data(\n    overrides=[\n            \"dataset=cmip\",\n            \"variables=['tasmin']\",\n            \"data_dir=/beegfs/muduchuru/data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            f\"aoi='{json.dumps(geojson)}'\",\n            \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    ]\n)\n</pre> import json  geojson = {'type': 'FeatureCollection', 'features': [{'id': 'YQYMn0RtqfIYFlVSuBRNa78VOV5eGN6l', 'type': 'Feature', 'properties': {}, 'geometry': {'coordinates': [[[0.3489189054060944, 23.354454949438832], [7.903907963894596, 23.638032033731335], [7.669846441330236, 18.791303400710987], [-1.2471156321152819, 18.62954836205158], [0.3489189054060944, 23.354454949438832]]], 'type': 'Polygon'}}]}  import climdata mswx = climdata.extract_data(     overrides=[             \"dataset=cmip\",             \"variables=['tasmin']\",             \"data_dir=/beegfs/muduchuru/data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             f\"aoi='{json.dumps(geojson)}'\",             \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     ] ) <pre>Saved NetCDF to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n\u2139\ufe0f No index selected (cfg.index is None). Skipping index computation.\n\u2705 Saved output to cmip_surface_LAT18.62954836205158-23.638032033731335_LON-1.2471156321152819-7.903907963894596_20201201_20201231.nc\n</pre> In\u00a0[34]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name='tasmin'\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"mswx_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name='tasmin' data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  In\u00a0[22]: Copied! <pre>import climdata\nhyras = climdata.extract_data(\n    overrides=[\n            \"dataset=hyras\",\n            \"variables=['tasmin']\",\n            \"table_id=day\",\n            \"data_dir=./data\",\n            f\"time_range.start_date=2020-12-01\",\n            f\"time_range.end_date=2020-12-31\",\n            \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",\n            \"region=custom\",\n    ]\n)\n</pre> import climdata hyras = climdata.extract_data(     overrides=[             \"dataset=hyras\",             \"variables=['tasmin']\",             \"table_id=day\",             \"data_dir=./data\",             f\"time_range.start_date=2020-12-01\",             f\"time_range.end_date=2020-12-31\",             \"bounds.custom={lat_min:50,lat_max:51,lon_min:8,lon_max:10}\",             \"region=custom\",     ] ) <pre>../../examples/conf\n\u2b07\ufe0f  Checking: https://opendata.dwd.de/climate_environment/CDC/grids_germany/daily/hyras_de/air_temperature_min/tasmin_hyras_1_2020_v6-0_de.nc\n\u2714\ufe0f  Exists locally: ./data/hyras/TASMIN/tasmin_hyras_1_2020_v6-0_de.nc\n\ud83d\udce6 Extracted curvilinear box with shape: FrozenMappingWarningOnValuesAccess({'time': 366, 'bnds': 2, 'x': 145, 'y': 110})\n\u2705 Saved output to hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\n</pre> In\u00a0[\u00a0]: Copied! <pre>import xarray as xr\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\nfrom gisco_geodata import NUTS\n# Open the NetCDF file\nfile_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\"\nds = xr.open_dataset(file_path)\n# print(ds)  # check variable names and dimensions\n\n# Select variable and timestep\nvar_name = list(ds.data_vars)[0]  # replace with actual variable if needed\ndata = ds[var_name].isel(time=0)  # first timestep\n\n# Create plot with curvilinear coordinates\nfig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})\n\nlat = ds[\"lat\"].values\nlon = ds[\"lon\"].values\nlat_min, lat_max = lat.min(), lat.max()\nlon_min, lon_max = lon.min(), lon.max()\n\n# pcolormesh with 2D lat/lon coordinates\nim = ax.pcolormesh(\n    lon,  # 2D lon\n    lat,  # 2D lat\n    data,\n    cmap='viridis',\n    transform=ccrs.PlateCarree()\n)\nnuts = NUTS()\nnuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\")\nnuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree())\n# Add coastlines and borders\nax.coastlines(resolution='10m')\nax.add_feature(cfeature.BORDERS, linestyle=':')\nax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\")\nax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree())\n# Add colorbar\nfig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))\n\nplt.show()\n</pre> import xarray as xr import matplotlib.pyplot as plt import cartopy.crs as ccrs import cartopy.feature as cfeature from gisco_geodata import NUTS # Open the NetCDF file file_path = \"hyras_surface_LAT50-51_LON8-10_20201201_20201231.nc\" ds = xr.open_dataset(file_path) # print(ds)  # check variable names and dimensions  # Select variable and timestep var_name = list(ds.data_vars)[0]  # replace with actual variable if needed data = ds[var_name].isel(time=0)  # first timestep  # Create plot with curvilinear coordinates fig, ax = plt.subplots(figsize=(8,6), subplot_kw={'projection': ccrs.PlateCarree()})  lat = ds[\"lat\"].values lon = ds[\"lon\"].values lat_min, lat_max = lat.min(), lat.max() lon_min, lon_max = lon.min(), lon.max()  # pcolormesh with 2D lat/lon coordinates im = ax.pcolormesh(     lon,  # 2D lon     lat,  # 2D lat     data,     cmap='viridis',     transform=ccrs.PlateCarree() ) nuts = NUTS() nuts3 = nuts.get(nuts_level=\"LEVL_3\", spatial_type=\"RG\") nuts3.plot(ax=ax, edgecolor=\"black\", facecolor=\"none\", linewidth=0.5, transform=ccrs.PlateCarree()) # Add coastlines and borders ax.coastlines(resolution='10m') ax.add_feature(cfeature.BORDERS, linestyle=':') ax.set_title(f\"{var_name} on {str(data['time'].values)[:10]}\") ax.set_extent([lon_min, lon_max, lat_min, lat_max], crs=ccrs.PlateCarree()) # Add colorbar fig.colorbar(im, ax=ax, orientation='vertical', label=str(data.attrs.get('units', '')))  plt.show()  <pre>&lt;xarray.Dataset&gt; Size: 47MB\nDimensions:  (time: 366, y: 110, x: 145)\nCoordinates:\n  * time     (time) datetime64[ns] 3kB 2020-01-01T12:00:00 ... 2020-12-31T12:...\n    lon      (y, x) float64 128kB ...\n    lat      (y, x) float64 128kB ...\n  * x        (x) float32 580B 4.178e+06 4.178e+06 ... 4.32e+06 4.322e+06\n  * y        (y) float32 440B 2.99e+06 2.99e+06 ... 3.098e+06 3.098e+06\nData variables:\n    tasmin   (time, y, x) float64 47MB ...\nAttributes: (12/21)\n    source:                 surface observations\n    institution:            Deutscher Wetterdienst (DWD)\n    Conventions:            CF-1.11\n    title:                  gridded_temperature_dataset_(HYRAS-DE TASMIN)\n    realization:            v6-0\n    project_id:             HYRAS\n    ...                     ...\n    license:                The HYRAS data, produced by DWD, is licensed unde...\n    ConventionsURL:         http://cfconventions.org/Data/cf-conventions/cf-c...\n    realm:                  atmos\n    product:                observations\n    input_data_status:      checked\n    filename:               tasmin_hyras_1_2020_v6-0_de.nc\n</pre>"},{"location":"examples/wrapper_workflow/","title":"Workflow Notebook","text":"In\u00a0[\u00a0]: Copied! <pre>from climdata import ClimData\nimport pandas as pd\nimport xarray as xr\n</pre> from climdata import ClimData import pandas as pd import xarray as xr In\u00a0[2]: Copied! <pre>extractor = ClimData()\ndatasets = extractor.get_datasets()\nprint(datasets)\n</pre> extractor = ClimData() datasets = extractor.get_datasets() print(datasets) <pre>['dwd', 'mswx', 'hyras', 'cmip', 'power']\n</pre> In\u00a0[3]: Copied! <pre>variables = extractor.get_variables('mswx')\nprint(variables)\n</pre> variables = extractor.get_variables('mswx') print(variables) <pre>['tasmin', 'tasmax', 'tas', 'pr', 'rsds', 'hurs', 'sfcWind']\n</pre> In\u00a0[4]: Copied! <pre>varinfo = extractor.get_varinfo('tasmax')\nprint(varinfo)\n</pre> varinfo = extractor.get_varinfo('tasmax') print(varinfo) <pre>{'cf_name': 'air_temperature', 'long_name': 'Daily maximum near-surface air temperature', 'units': 'degC'}\n</pre> In\u00a0[5]: Copied! <pre>actions = extractor.get_actions()\nprint(actions.keys())\n</pre> actions = extractor.get_actions() print(actions.keys()) <pre>dict_keys(['extract', 'to_dataframe', 'to_csv', 'calc_index', 'to_nc', 'upload_netcdf', 'upload_csv'])\n</pre> In\u00a0[6]: Copied! <pre>indices = extractor.get_indices(['tasmin', 'tasmax']) # TODO\nprint(indices.keys())\n\n# impute_methods = extractor.get_impute_methods() # TODO\n# print(impute_methods.keys())\n</pre> indices = extractor.get_indices(['tasmin', 'tasmax']) # TODO print(indices.keys())  # impute_methods = extractor.get_impute_methods() # TODO # print(impute_methods.keys()) <pre>dict_keys(['heat_wave_index', 'heat_wave_frequency', 'heat_wave_max_length', 'heat_wave_total_length', 'hot_spell_frequency', 'hot_spell_max_length', 'hot_spell_total_length', 'hot_spell_max_magnitude', 'ice_days', 'isothermality', 'maximum_consecutive_frost_days', 'maximum_consecutive_frost_free_days', 'maximum_consecutive_tx_days'])\n</pre> In\u00a0[7]: Copied! <pre>import json\n\n# -----------------------------\n# Step 1: Define the area of interest (AOI)\n# -----------------------------\n# The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude].\ngeojson = {\n  \"type\": \"FeatureCollection\",\n  \"features\": [\n    {\n      \"type\": \"Feature\",\n      \"properties\": {},\n      \"geometry\": {\n        \"coordinates\": [\n          24.246667038198012,  # longitude\n          12.891982026993958   # latitude\n        ],\n        \"type\": \"Point\"\n      }\n    }\n  ]\n}\n\n\n# -----------------------------\n# Step 2: Define configuration overrides\n# -----------------------------\n# Overrides are strings used by Hydra to modify default configurations at runtime.\noverrides = [\n    \"dataset=mswx\",  # Choose the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above\n    f\"time_range.start_date=2014-12-01\",  # Start date for data extraction\n    f\"time_range.end_date=2014-12-31\",    # End date for data extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download\n    \"index=tn10p\",  # Climate extreme index to calculate\n]\n\n# -----------------------------\n# Step 3: Define the workflow sequence\n# -----------------------------\nseq = [\"extract\", \"to_nc\", \"calc_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]\n\n# -----------------------------\n# Step 4: Initialize the ClimData extractor\n# -----------------------------\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Step 5: Run the Multi-Step workflow\n# -----------------------------\nresult = extractor.run_workflow(\n    actions=seq,\n)\n</pre> import json  # ----------------------------- # Step 1: Define the area of interest (AOI) # ----------------------------- # The AOI is a single point. In GeoJSON format, the coordinates are [longitude, latitude]. geojson = {   \"type\": \"FeatureCollection\",   \"features\": [     {       \"type\": \"Feature\",       \"properties\": {},       \"geometry\": {         \"coordinates\": [           24.246667038198012,  # longitude           12.891982026993958   # latitude         ],         \"type\": \"Point\"       }     }   ] }   # ----------------------------- # Step 2: Define configuration overrides # ----------------------------- # Overrides are strings used by Hydra to modify default configurations at runtime. overrides = [     \"dataset=mswx\",  # Choose the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set the AOI as the point defined above     f\"time_range.start_date=2014-12-01\",  # Start date for data extraction     f\"time_range.end_date=2014-12-31\",    # End date for data extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temp and precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store raw/intermediate files     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",  # optional . required for MSWS data download     \"index=tn10p\",  # Climate extreme index to calculate ]  # ----------------------------- # Step 3: Define the workflow sequence # ----------------------------- seq = [\"extract\", \"to_nc\", \"calc_index\", \"to_dataframe\", \"to_csv\", \"to_nc\"]  # ----------------------------- # Step 4: Initialize the ClimData extractor # ----------------------------- extractor = ClimData(overrides=overrides)  # ----------------------------- # Step 5: Run the Multi-Step workflow # ----------------------------- result = extractor.run_workflow(     actions=seq, ) <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n\u2705 All 31 pr files already exist locally.\nDataset saved to NetCDF file: mswx_tasmin_tasmax_pr_LAT12.891982026993958_LON24.246667038198012_2014-12-01_2014-12-31.nc\n&lt;class 'xarray.core.dataset.Dataset'&gt;\nDataFrame saved to CSV file: mswx_tn10p_LAT_12.891982026993958_LON_24.246667038198012_2014-12-01_2014-12-31.csv\nDataset saved to NetCDF file: mswx_tn10p_LAT12.891982026993958_LON24.246667038198012_2014-12-01_2014-12-31.nc\n</pre> <pre>&lt;frozen importlib._bootstrap&gt;:241: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 96 from PyObject\n/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:466: UserWarning: Index tn10p usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n</pre> In\u00a0[8]: Copied! <pre>import json\n\n# -----------------------------\n# Define the area of interest (AOI)\n# -----------------------------\n# This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012\ngeojson = {\n    \"type\": \"FeatureCollection\",\n    \"features\": [\n        {\n            \"type\": \"Feature\",\n            \"properties\": {},\n            \"geometry\": {\n                \"coordinates\": [24.246667038198012, 12.891982026993958],\n                \"type\": \"Point\"\n            }\n        }\n    ]\n}\n\n# -----------------------------\n# Define configuration overrides\n# -----------------------------\n# These strings override the default hydra config at runtime\noverrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    # Optional Google service account if needed for MSWX access\n    # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",\n    \"index=tn10p\",  # Extreme climate index to calculate\n]\n\n# -----------------------------\n# Initialize the ClimData extractor\n# -----------------------------\n# This loads the configuration with overrides and prepares the object\nextractor = ClimData(overrides=overrides)\n\n# -----------------------------\n# Extract climate data\n# -----------------------------\n# Returns an xarray.Dataset for the selected variables, AOI, and time range\nds = extractor.extract()\n\n# -----------------------------\n# Compute the climate index\n# -----------------------------\n# Takes the extracted dataset and calculates the extreme index \"tn10p\"\n# Returns a new xarray.Dataset containing only the index\nds_index = extractor.calc_index(ds)\n\n# -----------------------------\n# Convert the index dataset to a long-form pandas DataFrame\n# -----------------------------\n# Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\")\ndf_index = extractor.to_dataframe(ds_index)\n\n# -----------------------------\n# Save the DataFrame to CSV\n# -----------------------------\n# This will write the index values to \"index.csv\" in the current working directory\nextractor.to_csv(df_index, filename=\"index.csv\")\n</pre> import json  # ----------------------------- # Define the area of interest (AOI) # ----------------------------- # This AOI is a single point with latitude 12.891982026993958 and longitude 24.246667038198012 geojson = {     \"type\": \"FeatureCollection\",     \"features\": [         {             \"type\": \"Feature\",             \"properties\": {},             \"geometry\": {                 \"coordinates\": [24.246667038198012, 12.891982026993958],                 \"type\": \"Point\"             }         }     ] }  # ----------------------------- # Define configuration overrides # ----------------------------- # These strings override the default hydra config at runtime overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"aoi='{json.dumps(geojson)}'\",  # Set AOI as the point defined above     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"variables=[tasmin,tasmax,pr]\",       # Variables to extract: min/max temperature &amp; precipitation     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     # Optional Google service account if needed for MSWX access     # \"dsinfo.mswx.params.google_service_account=./.climdata_conf/service.json\",     \"index=tn10p\",  # Extreme climate index to calculate ]  # ----------------------------- # Initialize the ClimData extractor # ----------------------------- # This loads the configuration with overrides and prepares the object extractor = ClimData(overrides=overrides)  # ----------------------------- # Extract climate data # ----------------------------- # Returns an xarray.Dataset for the selected variables, AOI, and time range ds = extractor.extract()  # ----------------------------- # Compute the climate index # ----------------------------- # Takes the extracted dataset and calculates the extreme index \"tn10p\" # Returns a new xarray.Dataset containing only the index ds_index = extractor.calc_index(ds)  # ----------------------------- # Convert the index dataset to a long-form pandas DataFrame # ----------------------------- # Each row corresponds to a time, lat, lon, and variable (here just \"tn10p\") df_index = extractor.to_dataframe(ds_index)  # ----------------------------- # Save the DataFrame to CSV # ----------------------------- # This will write the index values to \"index.csv\" in the current working directory extractor.to_csv(df_index, filename=\"index.csv\")  <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n\u2705 All 31 pr files already exist locally.\n&lt;class 'xarray.core.dataset.Dataset'&gt;\nDataFrame saved to CSV file: index.csv\n</pre> <pre>/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:466: UserWarning: Index tn10p usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n</pre> Out[8]: <pre>'index.csv'</pre> In\u00a0[9]: Copied! <pre>print(extractor.current_filename)\n# print(extractor_point.filename_nc)\n</pre> print(extractor.current_filename) # print(extractor_point.filename_nc) <pre>index.csv\n</pre> In\u00a0[10]: Copied! <pre>box_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    \"region=europe\", # Select the region\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n]\n\nextractor_box = ClimData(overrides=box_overrides)\nresult_box = extractor_box.run_workflow(actions=[\"extract\", \"to_dataframe\"])\n</pre> box_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     \"region=europe\", # Select the region     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files ]  extractor_box = ClimData(overrides=box_overrides) result_box = extractor_box.run_workflow(actions=[\"extract\", \"to_dataframe\"])  <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> In\u00a0[11]: Copied! <pre>result_box.dataframe\n</pre> result_box.dataframe Out[11]: time lat lon variable value units source 0 2014-12-01 34.049999 0.050000 tasmin 8.3750 \u00b0C mswx 1 2014-12-01 34.049999 0.150006 tasmin 7.7500 \u00b0C mswx 2 2014-12-01 34.049999 0.249997 tasmin 7.5000 \u00b0C mswx 3 2014-12-01 34.049999 0.350003 tasmin 7.3125 \u00b0C mswx 4 2014-12-01 34.049999 0.450009 tasmin 7.0625 \u00b0C mswx ... ... ... ... ... ... ... ... 10322995 2014-12-31 70.949997 44.549999 tasmax -3.8125 \u00b0C mswx 10322996 2014-12-31 70.949997 44.650005 tasmax -3.7500 \u00b0C mswx 10322997 2014-12-31 70.949997 44.749996 tasmax -3.8125 \u00b0C mswx 10322998 2014-12-31 70.949997 44.850002 tasmax -3.8125 \u00b0C mswx 10322999 2014-12-31 70.949997 44.950008 tasmax -3.7500 \u00b0C mswx <p>10323000 rows \u00d7 7 columns</p> In\u00a0[12]: Copied! <pre>lat_berlin, lon_berlin = [52.5,13.4]\nidx_overrides = [\n    \"dataset=mswx\",  # Select the MSWX dataset for extraction\n    f\"lat={lat_berlin}\", # Select the region\n    f\"lon={lon_berlin}\",\n    \"variables=[tasmin,tasmax]\",\n    f\"time_range.start_date=2014-12-01\",  # Start date of extraction\n    f\"time_range.end_date=2014-12-31\",    # End date of extraction\n    \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files\n    \"index=heat_wave_max_length\"\n]\n\n\nextractor_idx = ClimData(overrides=idx_overrides)\nresult_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\"])\nresult_idx.dataframe.head()\n</pre> lat_berlin, lon_berlin = [52.5,13.4] idx_overrides = [     \"dataset=mswx\",  # Select the MSWX dataset for extraction     f\"lat={lat_berlin}\", # Select the region     f\"lon={lon_berlin}\",     \"variables=[tasmin,tasmax]\",     f\"time_range.start_date=2014-12-01\",  # Start date of extraction     f\"time_range.end_date=2014-12-31\",    # End date of extraction     \"data_dir=/beegfs/muduchuru/data\",    # Local directory to store downloaded/intermediate files     \"index=heat_wave_max_length\" ]   extractor_idx = ClimData(overrides=idx_overrides) result_idx = extractor_idx.run_workflow(actions=[\"extract\", \"calc_index\", \"to_dataframe\"]) result_idx.dataframe.head() <pre>\u2705 All 31 tasmin files already exist locally.\n\u2705 All 31 tasmax files already exist locally.\n</pre> <pre>/beegfs/muduchuru/pkgs_fnl/climdata/climdata/utils/wrapper_workflow.py:466: UserWarning: Index heat_wave_max_length usually requires \u226530 years, got 1\n  warnings.warn(f\"Index {cfg.index} usually requires \u226530 years, got {n_years}\", UserWarning)\n</pre> Out[12]: time lat lon variable value units source 0 2014-01-01 52.549999 13.350003 heat_wave_max_length 0.0 d mswx In\u00a0[13]: Copied! <pre>try:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"calc_index\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"to_csv\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n\ntry:\n    bad_ex = ClimData()\n    bad_ex.run_workflow(actions=[\"upload_netcdf\"])\nexcept Exception as e:\n    print(\"Error:\", e)\n</pre> try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"calc_index\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"to_csv\"]) except Exception as e:     print(\"Error:\", e)  try:     bad_ex = ClimData()     bad_ex.run_workflow(actions=[\"upload_netcdf\"]) except Exception as e:     print(\"Error:\", e) <pre>Error: Action 'calc_index' requires a dataset, but no dataset is available. Upload or extract a dataset before computing an index.\nError: Action 'to_csv' requires a DataFrame, but no DataFrame is available. Use 'to_dataframe' or upload a CSV before saving.\nError: Action 'upload_netcdf' requires argument 'netcdf_file', but none was provided.\n</pre>"},{"location":"examples/wrapper_workflow/#climdata-tutorial","title":"ClimData Tutorial\u00b6","text":"<p>This notebook demonstrates usage of the <code>ClimData</code> class for climate data extraction, extreme index computation, and workflow management. Includes examples for point-based and box-based extraction, variable exploration, and error handling.</p>"},{"location":"examples/wrapper_workflow/#1-imports","title":"1\ufe0f\u20e3 Imports\u00b6","text":""},{"location":"examples/wrapper_workflow/#2-explore-available-datasets","title":"2\ufe0f\u20e3 Explore available datasets\u00b6","text":""},{"location":"examples/wrapper_workflow/#3-explore-variables-for-a-dataset","title":"3\ufe0f\u20e3 Explore variables for a dataset\u00b6","text":""},{"location":"examples/wrapper_workflow/#4-explore-metadata-for-a-variable","title":"4\ufe0f\u20e3 Explore metadata for a variable\u00b6","text":""},{"location":"examples/wrapper_workflow/#5-explore-available-workflow-actions","title":"5\ufe0f\u20e3 Explore available workflow actions\u00b6","text":""},{"location":"examples/wrapper_workflow/#6-point-extraction-workflow","title":"6\ufe0f\u20e3 Point extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#output-filenames","title":"Output filenames\u00b6","text":""},{"location":"examples/wrapper_workflow/#7-box-extraction-workflow","title":"7\ufe0f\u20e3 Box extraction workflow\u00b6","text":""},{"location":"examples/wrapper_workflow/#8-compute-extreme-index-only","title":"8\ufe0f\u20e3 Compute extreme index only\u00b6","text":""},{"location":"examples/wrapper_workflow/#9-error-examples","title":"9\ufe0f\u20e3 Error examples\u00b6","text":""}]}